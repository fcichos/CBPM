---
format:
  live-html:
    page-layout: full
    toc: true
    toc-location: right
pyodide:
  autorun: false
  packages:
    - matplotlib
    - numpy
    - scipy
---

## Introduction to Numerical Integration in Physics

Numerical integration stands as one of the fundamental computational tools in physics, enabling us to solve problems where analytical solutions are either impossible or impractical. In physics, we frequently encounter integrals when calculating quantities such as:

- Work done by a variable force
- Electric and magnetic fields from complex charge distributions
- Center of mass of irregularly shaped objects
- Probability distributions in quantum mechanics
- Energy levels in quantum wells with arbitrary potentials
- Heat flow in non-uniform materials

The need for numerical integration arises because many physical systems are described by functions that cannot be integrated analytically. For example, the potential energy of a complex molecular system, the trajectory of a spacecraft under multiple gravitational influences, or the behavior of quantum particles in complex potentials.

In this lecture, we'll explore three progressively more accurate numerical integration methods: the Box method, Trapezoid method, and Simpson's method. We'll analyze their accuracy, efficiency, and appropriate applications in physical problems.

```{pyodide}
#| edit: false
#| echo: false
#| execute: true

import numpy as np
import matplotlib.pyplot as plt
from scipy.optimize import curve_fit

# Set default plotting parameters
plt.rcParams.update({
    'font.size': 12,
    'lines.linewidth': 1,
    'lines.markersize': 5,
    'axes.labelsize': 11,
    'xtick.labelsize': 10,
    'ytick.labelsize': 10,
    'xtick.top': True,
    'xtick.direction': 'in',
    'ytick.right': True,
    'ytick.direction': 'in',
})

def get_size(w, h):
    return (w/2.54, h/2.54)
```

### Box Method (Rectangle Method)

#### Theory and Implementation

![Box Method Illustration](img/box.png){#fig-box}

The Box method (also known as the Rectangle method) represents the simplest approach for numerical integration. It approximates the function in each interval $\Delta x$ with a constant value taken at a specific point of the interval—typically the left endpoint, although midpoint or right endpoint variants exist.

Mathematically, the definite integral is approximated as:

\begin{equation}
\int_{a}^{b} f(x) dx \approx \sum_{i=1}^{N} f(x_{i}) \Delta x
\end{equation}

Where:

- $a$ and $b$ are the integration limits
- $N$ is the number of intervals
- $\Delta x = \frac{b-a}{N}$ is the width of each interval
- $x_i$ represents the left endpoint of each interval

The method gets its name from the visual representation of the approximation as a series of rectangular boxes.

#### Box Method Applications

::: {.panel-tabset}
### Physics Application: Work Calculation

Consider a particle moving along a straight line under a variable force $F(x) = kx^2$ where $k$ is some constant. The work done by this force when moving the particle from position $x=0$ to $x=L$ is given by:

\begin{equation}
W = \int_{0}^{L} F(x) dx = \int_{0}^{L} kx^2 dx
\end{equation}

We can approximate this integral using the Box method, particularly useful when the force follows a complex pattern measured at discrete points.

```{pyodide}
def f(x):
    """Example function to integrate: f(x) = x"""
    return x

def force(x, k=1.0):
    """Force function F(x) = kx^2 for work calculation"""
    return k * x**2

def int_box(f, a, b, N):
    """Box method integration"""
    if N < 2:
        raise ValueError("N must be at least 2")
    x = np.linspace(a, b, N)
    y = f(x)
    dx = (b-a)/(N-1)
    return np.sum(y[:-1] * dx)  # Sum using left endpoints

# Example: Calculate work done by a nonlinear force
L = 2.0  # meters
k = 0.5  # N/m^3
N_points = 100

work = int_box(lambda x: force(x, k), 0, L, N_points)
print(f"Work done by force F(x) = {k}x² from x=0 to x={L} m:")
print(f"Numerical result (Box method): {work:.6f} J")
print(f"Analytical result: {k*L**3/3:.6f} J")

# Demonstrate the method graphically
x_demo = np.linspace(0, 1, 6)
y_demo = x_demo  # Using f(x) = x for demonstration
dx_demo = x_demo[1] - x_demo[0]

plt.figure(figsize=get_size(15, 10))

# Plot the actual function
x_fine = np.linspace(0, 1, 1000)
y_fine = x_fine
plt.plot(x_fine, y_fine, 'b-', label='actual function f(x) = x')

# Plot the boxes
for i in range(len(x_demo)-1):
    plt.fill_between([x_demo[i], x_demo[i+1]], [y_demo[i], y_demo[i]], alpha=0.3, color='red')
    plt.plot([x_demo[i], x_demo[i]], [0, y_demo[i]], 'r--', alpha=0.5)

plt.plot(x_demo[:-1], y_demo[:-1], 'ro', label='Sample points')
plt.xlabel('x')
plt.ylabel('f(x)')
plt.legend()
plt.tight_layout()
plt.show()

```

### Convergence Analysis

```{pyodide}
# Convergence demonstration
N_values = np.arange(10, 10000, 100)
box_results = [int_box(f, 0, 1, N) for N in N_values]
exact_value = 0.5  # Exact integral of f(x) = x from 0 to 1

# Calculate absolute errors
box_errors = np.abs(np.array(box_results) - exact_value)

# Fit a power law to error vs N
def power_law(x, a, b):
    return a * x**b

popt, _ = curve_fit(power_law, N_values, box_errors)

plt.figure(figsize=get_size(15, 10))
plt.loglog(N_values, box_errors, 'o', label='Box Method Error')
plt.loglog(N_values, power_law(N_values, *popt), '--',
         label=f'Power Law Fit: error ∝ N^{popt[1]:.2f}')
plt.xlabel('number of points [N]')
plt.ylabel('absolute Error')
plt.legend()
plt.tight_layout()
plt.show()

print(f"Box Method convergence rate: approximately O(N^{popt[1]:.2f})")
```

:::

### Trapezoid Method

#### Theory and Implementation

![Trapezoid Method Illustration](img/trapez.png){#fig-trap}

The Trapezoid method improves upon the Box method by approximating the function with linear segments between consecutive points. Instead of using constant values within each interval, it connects adjacent points with straight lines, forming trapezoids.

The mathematical formula for the Trapezoid method is:

\begin{equation}
\int_{a}^{b} f(x) dx \approx \sum_{i=1}^{N-1} \frac{f(x_i) + f(x_{i+1})}{2} \Delta x
\end{equation}

Where:
- $\Delta x = \frac{b-a}{N-1}$ is the width of each interval
- $x_i$ are the sample points

This method is particularly effective for smoothly varying functions, which are common in physical systems.

#### Trapezoid Application

::: {.panel-tabset}
### Physics Application: Electric Potential from Charge Distribution

A practical application in electromagnetism involves calculating the electric potential at a point due to a non-uniform charge distribution along a line. For a linear charge density $\lambda(x)$ along the x-axis, the potential at point $(0,d)$ is given by:

\begin{equation}
V(0,d) = \frac{1}{4\pi\epsilon_0} \int_{a}^{b} \frac{\lambda(x)}{\sqrt{x^2 + d^2}} dx
\end{equation}

The Trapezoid method is well-suited for this calculation, especially when $\lambda(x)$ is provided as experimental data points.

```{pyodide}
def int_trap(f, a, b, N):
    """Trapezoid method integration"""
    if N < 2:
        raise ValueError("N must be at least 2")
    x = np.linspace(a, b, N)
    y = f(x)
    dx = (b-a)/(N-1)
    return np.sum((y[1:] + y[:-1]) * dx/2)

# Simple demonstration
def f_demo(x):
    return x**2  # Using f(x) = x² for demonstration

# Demonstrate the trapezoid method visually
x_demo = np.linspace(0, 1, 6)
y_demo = f_demo(x_demo)

plt.figure(figsize=get_size(15, 8))
# Plot the actual function
x_fine = np.linspace(0, 1, 1000)
y_fine = f_demo(x_fine)
plt.plot(x_fine, y_fine, 'b-', label='actual function f(x) = x²')

# Plot the trapezoids
for i in range(len(x_demo)-1):
    plt.fill_between([x_demo[i], x_demo[i+1]],
                    [y_demo[i], y_demo[i+1]],
                    alpha=0.3, color='green')
    plt.plot([x_demo[i], x_demo[i+1]], [y_demo[i], y_demo[i+1]], 'g-')

plt.plot(x_demo, y_demo, 'go', label='sample points')
plt.xlabel('x')
plt.ylabel('f(x)')
plt.tight_layout()
plt.legend()
plt.show()

```


### Convergence Analysis
```{pyodide}

# Convergence demonstration
N_values = np.arange(10, 1000, 10)
trap_results = [int_trap(f_demo, 0, 1, N) for N in N_values]
exact_value = 1/3  # Exact integral of x² from 0 to 1

# Calculate absolute errors
trap_errors = np.abs(np.array(trap_results) - exact_value)

# Fit a power law to error vs N
def power_law(x, a, b):
    return a * x**b

popt, _ = curve_fit(power_law, N_values, trap_errors)

plt.figure(figsize=get_size(12, 10))
plt.loglog(N_values, trap_errors, 'o', label='trapezoid method error')
plt.loglog(N_values, power_law(N_values, *popt), '--',
         label=f'power law fit: error ∝ N^{popt[1]:.2f}')
plt.xlabel('number of points [N]')
plt.ylabel('absolute error')
plt.legend()
plt.tight_layout()
plt.show()

print(f"Trapezoid Method convergence rate: approximately O(N^{popt[1]:.2f})")
```
:::


### Simpson's Method

#### Theory and Implementation

![Simpson's Method Illustration](img/simpson.png){#fig-simp}

Simpson's method represents a significant improvement in accuracy over the previous methods by approximating the function with parabolic segments rather than straight lines. This approach is particularly effective for functions with curvature, which are ubiquitous in physics problems.

The mathematical formulation of Simpson's rule is:

\begin{equation}
\int_{a}^{b} f(x) dx \approx \frac{\Delta x}{3} \sum_{i=0}^{(N-1)/2} \left(f(x_{2i}) + 4f(x_{2i+1}) + f(x_{2i+2})\right)
\end{equation}

Where:
- $N$ is the number of intervals (must be even)
- $\Delta x = \frac{b-a}{N}$ is the width of each interval

Simpson's rule is derived from fitting a quadratic polynomial through every three consecutive points and then integrating these polynomials.


#### Simpson's Nethod Applications

::: {.panel-tabset}
### Physics Application: Quantum Mechanical Probability

A fundamental application in quantum mechanics involves calculating the probability of finding a particle in a region of space. For a wavefunction $\psi(x)$, the probability of finding the particle between positions $a$ and $b$ is:

\begin{equation}
P(a \leq x \leq b) = \int_{a}^{b} |\psi(x)|^2 dx
\end{equation}

Simpson's method provides high accuracy for these calculations, especially important when dealing with oscillatory wavefunctions.

```{pyodide}
def int_simp(f, a, b, N):
    """Simpson's method integration"""
    if N % 2 == 0:
        N = N + 1  # Ensure N is odd for Simpson's rule

    if N < 3:
        raise ValueError("N must be at least 3 for Simpson's method")

    x = np.linspace(a, b, N)
    y = f(x)
    dx = (b-a)/(N-1)

    # Apply Simpson's formula
    return dx/3 * np.sum(y[0:-2:2] + 4*y[1:-1:2] + y[2::2])

# Demonstrate Simpson's method with a simple example
def f_demo(x):
    """Example function: sin(x)"""
    return np.sin(x)

# Visualize Simpson's method with a few segments
x_demo = np.linspace(0, np.pi, 7)  # 6 intervals
y_demo = f_demo(x_demo)

plt.figure(figsize=get_size(15, 8))
# Plot the actual function
x_fine = np.linspace(0, np.pi, 1000)
y_fine = f_demo(x_fine)
plt.plot(x_fine, y_fine, 'b-', label='f(x) = sin(x)')

# Plot the parabolic segments
for i in range(0, len(x_demo)-2, 2):
    x_segment = np.linspace(x_demo[i], x_demo[i+2], 50)

    # Fit a quadratic polynomial through three points
    x_points = x_demo[i:i+3]
    y_points = y_demo[i:i+3]
    coeffs = np.polyfit(x_points, y_points, 2)
    y_fit = np.polyval(coeffs, x_segment)

    plt.plot(x_segment, y_fit, 'r-', alpha=0.7)
    plt.fill_between(x_segment, 0, y_fit, alpha=0.2, color='purple')

plt.plot(x_demo, y_demo, 'go', label='Sample points')
plt.xlabel('x')
plt.ylabel('f(x)')
plt.title("Simpson's Method Visualization")
plt.grid(True)
plt.legend()
plt.show()
```


### Convergence Analysis

```{pyodide}
# Convergence demonstration with a simple test function
f_test = lambda x: x**4  # We can integrate x⁴ exactly
exact_value = 1/5  # Exact integral of x⁴ from 0 to 1

# Compare accuracy with different numbers of points
N_values = np.arange(3, 101, 2)  # Odd numbers for Simpson's rule
simp_errors = [abs(int_simp(f_test, 0, 1, n) - exact_value) for n in N_values]

# Fit a power law to error vs N
popt, _ = curve_fit(power_law, N_values, simp_errors)

plt.figure(figsize=get_size(12, 10))
plt.loglog(N_values, simp_errors, 'o', label="Simpson's method error")
plt.loglog(N_values, power_law(N_values, *popt), '--',
         label=f'Power law fit: error ∝ N^{popt[1]:.2f}')
plt.xlabel('number of points [N]')
plt.ylabel('absolute error')
plt.legend()
plt.tight_layout()
plt.show()

print(f"Simpson's Method convergence rate: approximately O(N^{popt[1]:.2f})")

```
:::

::: {.callout-note collapse=true}
### Simpson's Rule for Numerical Integration

Simpson's Rule is a method for numerical integration that approximates the definite integral of a function by using quadratic polynomials.


1) For an integral $\int_a^b f(x)dx$, Simpson's Rule fits a quadratic function through three points:

   - $f(a)$
   - $f(\frac{a+b}{2})$
   - $f(b)$

2) Let's define:

   - $h = \frac{b-a}{2}$
   - $x_0 = a$
   - $x_1 = \frac{a+b}{2}$
   - $x_2 = b$

3) The quadratic approximation has the form:
   $$P(x) = Ax^2 + Bx + C$$

4) This polynomial must satisfy:
   $$f(x_0) = Ax_0^2 + Bx_0 + C$$
   $$f(x_1) = Ax_1^2 + Bx_1 + C$$
   $$f(x_2) = Ax_2^2 + Bx_2 + C$$

5) Using Lagrange interpolation:
   $$P(x) = f(x_0)L_0(x) + f(x_1)L_1(x) + f(x_2)L_2(x)$$

   where $L_0$, $L_1$, $L_2$ are the Lagrange basis functions.

#### Final Formula

The integration of this polynomial leads to Simpson's Rule:

$$\int_a^b f(x)dx \approx \frac{h}{3}[f(a) + 4f(\frac{a+b}{2}) + f(b)]$$

### Error Term

The error in Simpson's Rule is proportional to:

$$-\frac{h^5}{90}f^{(4)}(\xi)$$

for some $\xi \in [a,b]$

#### Composite Simpson's Rule

For better accuracy, we can divide the interval into $n$ subintervals (where $n$ is even):

$$\int_a^b f(x)dx \approx \frac{h}{3}[f(x_0) + 4\sum_{i=1}^{n/2}f(x_{2i-1}) + 2\sum_{i=1}^{n/2-1}f(x_{2i}) + f(x_n)]$$

where $h = \frac{b-a}{n}$


The method is particularly effective for integrating functions that can be well-approximated by quadratic polynomials over small intervals.
:::


### When to Use Each Method

Choosing the appropriate numerical integration method for a physics problem requires consideration of several factors:

| Method | Error Order | Optimal For | Limitations | Example Physics Applications |
|--------|------------|-------------|-------------|-------------------------------|
| **Box** | $O(N^{-1})$ | - Simple, rapid calculations<br>- Step functions<br>- Real-time processing<br>- Discontinuous functions | - Low accuracy<br>- Requires many points for decent results | - Basic data analysis<br>- Signals with sharp transitions<br>- First approximations in mechanics |
| **Trapezoid** | $O(N^{-2})$ | - Smooth, continuous functions<br>- Moderate accuracy requirements<br>- Periodic functions | - Struggles with sharp peaks<br>- Not ideal for higher derivatives | - Electric and magnetic fields<br>- Orbital mechanics<br>- Path integrals<br>- Work/energy calculations |
| **Simpson** | $O(N^{-4})$ | - Functions with significant curvature<br>- High precision requirements<br>- Oscillatory integrands | - More computationally intensive<br>- Requires evenly spaced points | - Quantum mechanical probabilities<br>- Wave optics<br>- Statistical mechanics<br>- Thermal physics |

#### Additional Considerations

- **Adaptive methods** can be more efficient when dealing with functions that have varying behavior across the integration range
- **Improper integrals** (with infinite limits or singularities) often require specialized techniques beyond these basic methods
- **Higher-dimensional** integration problems (common in statistical and quantum mechanics) may benefit from Monte Carlo methods rather than these quadrature rules

## Error Analysis

The error behavior of numerical integration methods is crucial for understanding their applicability to physics problems:

- **Box Method**: Error $\propto$ $O(\Delta x)$ = $O(N^{-1})$ (linear convergence)
  - The error is proportional to the step size
  - The dominant error term comes from the first derivative of the function

- **Trapezoid Method**: Error $\propto$ $O(\Delta x^2)$ = $O(N^{-2})$ (quadratic convergence)
  - The error is proportional to the square of the step size
  - The dominant error term involves the second derivative of the function

- **Simpson's Method**: Error $\propto$ $O(\Delta x^4)$ = $O(N^{-4})$ (fourth-order convergence)
  - The error is proportional to the fourth power of the step size
  - The dominant error term involves the fourth derivative of the function

Consequently, if we double the number of points:

- Box method: error reduced by a factor of 2
- Trapezoid method: error reduced by a factor of 4
- Simpson's method: error reduced by a factor of 16

The practical impact of these convergence rates is substantial. For example, to achieve an error of $10^{-6}$ for a well-behaved function:

- Box method might require millions of points
- Trapezoid method might require thousands of points
- Simpson's method might require only hundreds of points

This explains why higher-order methods are generally preferred for physics applications requiring high precision, such as quantum mechanical calculations, gravitational wave analysis, or computational fluid dynamics.

### Conclusion

Numerical integration stands as a cornerstone of computational physics, bridging the gap between theoretical models and practical analysis of real-world systems. In this lecture, we've explored three fundamental methods—Box, Trapezoid, and Simpson's method—that provide different balances of simplicity, accuracy, and computational efficiency.

The key insights from our study include:

1. **Method selection matters**: The choice of integration technique can dramatically impact both accuracy and computational efficiency. For typical physics applications requiring high precision, Simpson's method often provides the optimal balance of accuracy and computation cost.

2. **Error scaling**: Understanding how errors scale with the number of sampling points is crucial for reliable scientific computation. The higher-order convergence of Simpson's method ($O(N^{-4})$) makes it particularly valuable for precision-critical applications in physics.

3. **Physical context**: The nature of the underlying physical system should guide your choice of numerical method. Smoothly varying functions benefit from higher-order methods, while functions with discontinuities may require adaptive or specialized approaches.

4. **Verification**: Always verify numerical results against analytical solutions when possible, or compare results from different numerical methods with increasing resolution to establish confidence in your calculations.

As you progress in your physics education, these numerical integration techniques will become essential tools in your computational toolkit, enabling you to tackle increasingly complex physical systems from quantum mechanics to astrophysics, fluid dynamics, and beyond.

In advanced courses, you'll explore additional techniques such as Gaussian quadrature, Romberg integration, and specialized methods for oscillatory, singular, or multi-dimensional integrals—each designed to address specific challenges encountered in modern physics.

Remember that numerical integration is not merely a computational technique but a powerful approach to understanding physical systems that resist analytical treatment, making it an indispensable skill for the modern physicist.


::: {.callout-note collapse=true}
## Advanced Topics in Numerical Integration

### Adaptive Integration Methods

The methods we've discussed so far use equal spacing between sampling points. However, most real-world physics problems involve functions that vary dramatically across the integration range. Adaptive methods adjust the point distribution to concentrate more points where the function changes rapidly.

```{pyodide}
# Simple demonstration of adaptive integration concept
def adaptive_demo(f, a, b, tol=1e-5, max_depth=10):
    """Simple demonstration of adaptive integration concept"""
    def recursive_integrate(a, b, depth=0):
        # Compute midpoint
        c = (a + b) / 2

        # Estimate integral using Simpson's rule on entire interval
        I_whole = (b-a)/6 * (f(a) + 4*f(c) + f(b))

        # Estimate integral using Simpson's rule on each half
        mid1 = (a + c) / 2
        mid2 = (c + b) / 2
        I_left = (c-a)/6 * (f(a) + 4*f(mid1) + f(c))
        I_right = (b-c)/6 * (f(c) + 4*f(mid2) + f(b))
        I_parts = I_left + I_right

        # Check error
        error = abs(I_whole - I_parts)

        # If error is small enough or max depth reached, return result
        if error < tol*(b-a) or depth >= max_depth:
            return I_parts, [(a, c, b, error)]

        # Otherwise, recursively integrate each half
        I_left_result, left_regions = recursive_integrate(a, c, depth+1)
        I_right_result, right_regions = recursive_integrate(c, b, depth+1)

        return I_left_result + I_right_result, left_regions + right_regions

    integral, regions = recursive_integrate(a, b)
    return integral, regions

# Define a challenging function with a sharp peak
def challenging_func(x):
    """Function with a sharp peak at x=0.7"""
    return 1 / (0.01 + (x - 0.7)**2)

# Calculate integral using adaptive and non-adaptive methods
a, b = 0, 1
n_points = 101  # Use a fixed number of points for non-adaptive methods

# Calculate using non-adaptive methods
box_result = int_box(challenging_func, a, b, n_points)
trap_result = int_trap(challenging_func, a, b, n_points)
simp_result = int_simp(challenging_func, a, b, n_points)

# Calculate using adaptive method
adaptive_result, regions = adaptive_demo(challenging_func, a, b)

# Calculate a reference solution using a very high number of points
reference = int_simp(challenging_func, a, b, 10001)

# Visualize the function and integration points
x_fine = np.linspace(a, b, 1000)
y_fine = [challenging_func(x) for x in x_fine]

plt.figure(figsize=get_size(15, 10))

# Plot the function
plt.plot(x_fine, y_fine, 'k-', label='f(x)')

# Plot the adaptive integration regions
for region in regions:
    a_r, m_r, b_r, error = region
    plt.plot([a_r, m_r, b_r], [challenging_func(a_r), challenging_func(m_r), challenging_func(b_r)],
             'ro-', alpha=0.5)

plt.xlabel('x')
plt.ylabel('f(x)')


# Adjust y-axis to show the function's behavior more clearly
plt.ylim(0, 40)

plt.legend()
plt.show()

# Show results
print("Results for integrating a function with a sharp peak:")
print(f"Box Method (n={n_points}): {box_result:.8f}, Error: {abs(box_result-reference):.8f}")
print(f"Trapezoid Method (n={n_points}): {trap_result:.8f}, Error: {abs(trap_result-reference):.8f}")
print(f"Simpson's Method (n={n_points}): {simp_result:.8f}, Error: {abs(simp_result-reference):.8f}")
print(f"Adaptive Method: {adaptive_result:.8f}, Error: {abs(adaptive_result-reference):.8f}")
print(f"Reference value: {reference:.8f}")
print(f"Number of adaptive regions: {len(regions)}")
```

### Multi-dimensional Integration

Many physics problems require integration over multiple dimensions, such as calculating mass moments of inertia, electric fields from volume charge distributions, or statistical mechanics partition functions.

For 2D integration, we can extend our 1D methods using the concept of iterated integrals:

\begin{equation}
\int_{a}^{b}\int_{c}^{d} f(x,y) dy dx \approx \sum_{i=1}^{N_x} \sum_{j=1}^{N_y} w_i w_j f(x_i, y_j)
\end{equation}

Where $w_i$ and $w_j$ are the weights for the respective 1D methods.

```{pyodide}
def int_2d_trap(f, x_range, y_range, nx, ny):
    """2D integration using the Trapezoid method"""
    x = np.linspace(x_range[0], x_range[1], nx)
    y = np.linspace(y_range[0], y_range[1], ny)
    dx = (x_range[1] - x_range[0]) / (nx - 1)
    dy = (y_range[1] - y_range[0]) / (ny - 1)

    result = 0
    for i in range(nx-1):
        for j in range(ny-1):
            # Average of function values at the four corners of each cell
            f_values = [
                f(x[i], y[j]),
                f(x[i+1], y[j]),
                f(x[i], y[j+1]),
                f(x[i+1], y[j+1])
            ]
            result += sum(f_values) / 4 * dx * dy

    return result

# Example: Electric potential from a square charge distribution
def potential_2d(x, y, z=1):
    """Electric potential at (x,y,z) from a square charge distribution in the x-y plane"""
    # Avoid division by zero
    denominator = (x**2 + y**2 + z**2)**0.5
    if denominator < 1e-10:
        return 0
    return 1 / denominator

# Calculate the electric potential at a point above a charged square
z = 1.0  # height above the plane
potential = int_2d_trap(lambda x, y: potential_2d(x, y, z), [-1, 1], [-1, 1], 51, 51)

# Visualize the potential in the plane above the charge
x_vals = np.linspace(-2, 2, 50)
y_vals = np.linspace(-2, 2, 50)
X, Y = np.meshgrid(x_vals, y_vals)
Z = np.zeros_like(X)

for i in range(len(x_vals)):
    for j in range(len(y_vals)):
        Z[j, i] = potential_2d(X[j, i], Y[j, i], z)

plt.figure(figsize=get_size(15, 10))

# Plot the potential as a contour
contour = plt.contourf(X, Y, Z, 50, cmap='viridis')
plt.colorbar(label='Electric Potential')

# Mark the square charge distribution
plt.plot([-1, 1, 1, -1, -1], [-1, -1, 1, 1, -1], 'r-', linewidth=2)

plt.xlabel('x')
plt.ylabel('y')
plt.axis('equal')

plt.show()

print(f"Electric potential at point (0,0,{z}): {potential:.6f}")
print(f"Analytical value at center: {np.log(1 + np.sqrt(2)):.6f}")
```

### Monte Carlo Integration

For higher-dimensional integrals and complex domains, Monte Carlo methods become increasingly efficient. These methods use random sampling to approximate integrals and are particularly valuable in quantum and statistical physics.

```{pyodide}
def monte_carlo_integrate(f, ranges, n_samples):
    """Monte Carlo integration in arbitrary dimensions"""
    # Generate random samples within the integration domain
    dim = len(ranges)
    samples = np.random.uniform(
        low=[r[0] for r in ranges],
        high=[r[1] for r in ranges],
        size=(n_samples, dim)
    )

    # Evaluate function at sample points
    f_values = np.array([f(*sample) for sample in samples])

    # Calculate volume of integration domain
    volume = np.prod([r[1] - r[0] for r in ranges])

    # Estimate integral and error
    integral = volume * np.mean(f_values)
    error = volume * np.std(f_values) / np.sqrt(n_samples)

    return integral, error

# Example: Calculate the volume of a n-dimensional hypersphere
def sphere_indicator(x, y, z):
    """Return 1 if point is inside unit sphere, 0 otherwise"""
    return 1 if x**2 + y**2 + z**2 <= 1 else 0

# Calculate the volume of a 3D sphere using Monte Carlo
n_samples = 100000
ranges = [[-1, 1], [-1, 1], [-1, 1]]  # Cube containing the sphere
volume, error = monte_carlo_integrate(sphere_indicator, ranges, n_samples)

# The exact volume of a unit sphere in 3D is (4/3)π
exact_volume = 4/3 * np.pi

# Visualize the convergence
sample_sizes = np.logspace(2, 5, 20, dtype=int)
volumes = []
errors = []

for n in sample_sizes:
    v, e = monte_carlo_integrate(sphere_indicator, ranges, n)
    volumes.append(v)
    errors.append(e)

plt.figure(figsize=get_size(15, 10))
plt.semilogx(sample_sizes, volumes, 'o-', label='Monte Carlo estimate')
plt.fill_between(
    sample_sizes,
    [v - e for v, e in zip(volumes, errors)],
    [v + e for v, e in zip(volumes, errors)],
    alpha=0.3
)
plt.axhline(exact_volume, color='r', linestyle='--', label='Exact value')
plt.xlabel('Number of samples')
plt.ylabel('Volume of unit sphere')
plt.legend()
plt.show()

print(f"Volume of unit sphere (Monte Carlo with {n_samples} samples): {volume:.6f} ± {error:.6f}")
print(f"Exact volume of unit sphere: {exact_volume:.6f}")
print(f"Relative error: {abs(volume - exact_volume)/exact_volume*100:.4f}%")
```

### Application to Real Physics Problems

Let's examine two common scenarios in physics that benefit from numerical integration:

1. **Non-uniform Magnetic Field**: When a charged particle moves through a non-uniform magnetic field, the work done can be calculated as:

   $$W = q\int_{\vec{r}_1}^{\vec{r}_2} \vec{v} \times \vec{B}(\vec{r}) \cdot d\vec{r}$$

2. **Quantum Tunneling**: The tunneling probability through a potential barrier is given by:

   $$T \approx \exp\left(-\frac{2}{\hbar}\int_{x_1}^{x_2} \sqrt{2m(V(x) - E)}\, dx\right)$$

In both cases, the integrals frequently cannot be solved analytically due to the complex spatial dependence of the fields or potentials, making numerical integration indispensable for modern physics.
:::
