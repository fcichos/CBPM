---
format:
  live-html:
    toc: true
    toc-location: right
pyodide:
  autorun: false
  packages:
    - matplotlib
    - numpy
    - scipy
---

# Solving Ordinary Differential Equations

## Introduction

In the previous lecture on numerical differentiation, we explored how to compute derivatives numerically using finite difference methods and matrix representations. We learned that:

1. Finite difference approximations allow us to estimate derivatives at discrete points
2. Differentiation can be represented as matrix operations
3. The accuracy of these approximations depends on the order of the method and step size

Building on this foundation, we can now tackle one of the most important applications in computational physics: solving ordinary differential equations (ODEs). Almost all dynamical systems in physics are described by differential equations, and learning how to solve them numerically is essential for modeling physical phenomena.

As second-semester physics students, you've likely encountered differential equations in various contexts:

- Newton's second law: $F = ma = m\frac{d^2x}{dt^2}$
- Simple harmonic motion: $\frac{d^2x}{dt^2} + \omega^2x = 0$
- RC circuits: $\frac{dQ}{dt} + \frac{1}{RC}Q = 0$
- Heat diffusion: $\frac{\partial T}{\partial t} = \alpha \nabla^2 T$

While analytical solutions exist for some simple cases, real-world physics problems often involve complex systems where analytical solutions are either impossible or impractical to obtain. This is where numerical methods become indispensable tools for the working physicist.

```{pyodide}
#| edit: false
#| echo: false
#| execute: true

import numpy as np
import io
import pandas as pd
import matplotlib.pyplot as plt
from scipy.sparse import diags
from scipy.integrate import solve_ivp

# default values for plotting
plt.rcParams.update({'font.size': 12,
                     'lines.linewidth': 1.5,
                     'lines.markersize': 5,
                     'axes.labelsize': 11,
                     'xtick.labelsize': 10,
                     'ytick.labelsize': 10,
                     'xtick.top': True,
                     'xtick.direction': 'in',
                     'ytick.right': True,
                     'ytick.direction': 'in',})

def get_size(w,h):
      return((w/2.54,h/2.54))

def set_plot_style():
    plt.style.use('seaborn-v0_8-whitegrid')
    plt.rcParams.update({
        'figure.figsize': (10, 6),
        'axes.grid': True,
        'grid.linestyle': '--',
        'grid.alpha': 0.7,
        'lines.linewidth': 2,
        'font.size': 12,
        'axes.labelsize': 14,
        'axes.titlesize': 16,
        'xtick.labelsize': 12,
        'ytick.labelsize': 12
    })
```

## Types of ODE Solution Methods

There are two main approaches to solving ODEs numerically:

1. **Implicit methods**: These methods solve for all time points simultaneously using matrix operations. They treat the problem as a large system of coupled algebraic equations. Just as you might solve a system of linear equations $Ax = b$ in linear algebra, implicit methods set up and solve a larger matrix equation that represents the entire evolution of the system.

2. **Explicit methods**: These methods march forward in time, computing the solution step by step. Starting from the initial conditions, they use the current state to calculate the next state, similar to how you might use the position and velocity at time $t$ to predict the position and velocity at time $t + \Delta t$.

We'll explore both approaches, highlighting their strengths and limitations. Each has its place in physics: implicit methods often handle "stiff" problems better (problems with vastly different timescales), while explicit methods are typically easier to implement and can handle nonlinear problems more naturally.

## The Harmonic Oscillator: A Prototypical ODE

::: {.callout-note}
## Physics Interlude: The Harmonic Oscillator

The harmonic oscillator is one of the most fundamental systems in physics, appearing in mechanics, electromagnetism, quantum mechanics, and many other fields. It serves as an excellent test case for ODE solvers due to its simplicity and known analytical solution.

The harmonic oscillator describes any system that experiences a restoring force proportional to displacement. Physically, this occurs in:

- A mass on a spring (Hooke's law: $F = -kx$)
- A pendulum for small angles (where $\sin\theta \approx \theta$)
- An LC circuit with inductors and capacitors
- Molecular vibrations in chemistry
- Phonons in solid state physics


::: {#fig-test}

![](/lectures/lecture06/img/spring.png){fig-align="center" width="300px"}

A mass suspended from a spring demonstrates a classic harmonic oscillator. When displaced from equilibrium, the spring exerts a restoring force proportional to the displacement, leading to oscillatory motion.
:::
The equation of motion for a classical harmonic oscillator is:

\begin{equation}
\frac{d^2x}{dt^2} + \omega^2 x = 0
\end{equation}

where $\omega = \sqrt{k/m}$ is the angular frequency, with $k$ being the spring constant and $m$ the mass. In terms of Newton's second law, this is:
\begin{equation}
m\frac{d^2x}{dt^2} = -kx
\end{equation}

This second-order differential equation requires two initial conditions:
- Initial position: $x(t=0) = x_0$
- Initial velocity: $\dot{x}(t=0) = v_0$

The analytical solution is:
\begin{equation}
x(t) = x_0 \cos(\omega t) + \frac{v_0}{\omega} \sin(\omega t)
\end{equation}

This represents sinusoidal oscillation with a constant amplitude and period $T = 2\pi/\omega$. The total energy of the system (kinetic + potential) remains constant:
\begin{equation}
E = \frac{1}{2}mv^2 + \frac{1}{2}kx^2
\end{equation}

This energy conservation will be an important test for our numerical methods.
:::

## Implicit Matrix Solution

Using the matrix representation of the second derivative operator from our previous lecture, we can transform the ODE into a linear system that can be solved in one step. This approach treats the entire time evolution as a single mathematical problem.

### Setting Up the System

From calculus, we know that the second derivative represents the rate of change of the rate of change. Physically, this corresponds to acceleration in mechanics. In numerical terms, we need to approximate this using finite differences.

Recall that we can represent the second derivative operator as a tridiagonal matrix:

$$D_2 = \frac{1}{\Delta t^2}
\begin{bmatrix}
-2 & 1  & 0 & 0 & \cdots & 0\\
 1 & -2 & 1 & 0 & \cdots & 0\\
 0 & 1  & -2 & 1 & \cdots & 0\\
 \vdots & \vdots & \vdots & \vdots & \ddots & \vdots\\
 0 & \cdots & 0 & 1 & -2 & 1\\
 0 & \cdots & 0 & 0 & 1 & -2\\
\end{bmatrix}$$

This matrix implements the central difference approximation for the second derivative:
\begin{equation}
\frac{d^2x}{dt^2} \approx \frac{x_{i+1} - 2x_i + x_{i-1}}{\Delta t^2}
\end{equation}

Each row in this matrix corresponds to the equation for a specific time point, linking it to its neighbors.

The harmonic oscillator term $\omega^2 x$ represents the restoring force. In a spring system, this is $F = -kx$ divided by mass, giving $a = -\frac{k}{m}x = -\omega^2 x$. This can be represented by a diagonal matrix:

$$V = \omega^2 I = \omega^2
\begin{bmatrix}
1 & 0 & 0 & \cdots & 0\\
0 & 1 & 0 & \cdots & 0\\
0 & 0 & 1 & \cdots & 0\\
\vdots & \vdots & \vdots & \ddots & \vdots\\
0 & 0 & 0 & \cdots & 1\\
\end{bmatrix}$$

Our equation $\frac{d^2x}{dt^2} + \omega^2 x = 0$ becomes the matrix equation $(D_2 + V)x = 0$, where $x$ is the vector containing the positions at all time points $(x_1, x_2, ..., x_n)$. This is a large system of linear equations that determine the entire trajectory at once.

### Incorporating Initial Conditions

To solve this system, we need to incorporate the initial conditions by modifying the first rows of our matrix. This is a crucial step because a second-order differential equation needs two initial conditions to define a unique solution. In physical terms, we need to know both the initial position (displacement) and the initial velocity of our oscillator. We incorporate these conditions directly into our matrix equation:

$$M =
\begin{bmatrix}
\color{red}{1} & \color{red}{0} & \color{red}{0} & \color{red}{0} & \color{red}{\cdots} & \color{red}{0} \\
\color{blue}{-1} & \color{blue}{1} & \color{blue}{0} & \color{blue}{0} & \color{blue}{\cdots} & \color{blue}{0} \\
\frac{1}{\Delta t^2} & -\frac{2}{\Delta t^2}+\omega^2 & \frac{1}{\Delta t^2} & 0 & \cdots & 0 \\
0 & \frac{1}{\Delta t^2} & -\frac{2}{\Delta t^2}+\omega^2 & \frac{1}{\Delta t^2} & \cdots & 0 \\
\vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\
0 & \cdots & 0 & \frac{1}{\Delta t^2} & -\frac{2}{\Delta t^2}+\omega^2 & \frac{1}{\Delta t^2} \\
\end{bmatrix}$$

The first row (in red) enforces $x(0) = x_0$ (initial position), and the second row (in blue) implements the initial velocity condition. The rest of the matrix represents the differential equation at each time step. By incorporating the initial conditions directly into the matrix, we ensure that our solution satisfies both the differential equation and the initial conditions.

```{pyodide}
#| autorun: false

# Define parameters
k = 15.5  # spring constant (N/m)
m = 0.2   # mass (kg)
x0 = 1    # initial elongation (m)
v0 = 0    # initial velocity (m/s)
omega = np.sqrt(k/m)  # angular frequency (rad/s)
                      # This comes from the physics: ω² = k/m

# Set up time domain
T = 2*np.pi/omega  # natural period of oscillation
L = 3*T            # solve for 3 complete periods
N = 500            # number of data points (time resolution)
t = np.linspace(0, L, N)  # time axis
dt = t[1] - t[0]  # time interval of each step

# Construct the matrix M that represents (D2 + V)
# This combines the second derivative operator and the potential
M = (diags([-2., 1., 1.], [-1,-2, 0], shape=(N, N))+diags([1], [-1], shape=(N, N))* omega**2*dt**2).todense()

# Incorporate initial conditions
M[0,0]=1  # First equation: x(0) = x0 (initial position)
M[1,0]=-1  # Second equation: velocity condition
M[1,1]=1   # These two lines encode v(0) = v0

# Create the right-hand side vector containing the initial conditions
b = np.zeros(N)  # initialize vector of zeros
b[0]=1  # Set initial position (x0 = 1)
b[1]=0  # Set initial velocity (v0 = 0)
b = b.transpose()

# Solve the linear system Mx = b
# This gives us the position at all time points at once
x = np.linalg.solve(M, b)  # this is the solution
```


```{pyodide}
#| autorun: false

# Plot the solution
plt.figure(figsize=get_size(12, 10))
plt.plot(t, x, label='Numerical Solution')
plt.plot(t, x0*np.cos(omega*t) + v0/omega*np.sin(omega*t), '--', label='Analytical Solution')
plt.xlabel('Time (s)')
plt.ylabel('Position x(t)')
plt.legend()
plt.show()
```

This matrix-based approach has several advantages:
- It solves for all time points simultaneously, giving the entire trajectory in one operation
- It can be very stable for certain types of problems, particularly "stiff" equations
- It handles boundary conditions naturally by incorporating them into the matrix structure
- It often provides good energy conservation for oscillatory systems

However, it also has limitations:
- It requires solving a large linear system, which becomes computationally intensive for long simulations
- It's not suitable for nonlinear problems without modification (e.g., a pendulum with large angles where $\sin\theta \neq \theta$)
- Memory requirements grow with the time span (an N×N matrix for N time steps)
- Implementing time-dependent forces is more complex than with explicit methods

From a physics perspective, this approach is similar to finding stationary states in quantum mechanics or solving boundary value problems in electrostatics—we're solving the entire system at once rather than stepping through time.

## Explicit Numerical Integration Methods

An alternative approach is to use explicit step-by-step integration methods. These methods convert the second-order ODE to a system of first-order ODEs and then advance the solution incrementally, similar to how we intuitively understand physical motion: position changes based on velocity, and velocity changes based on acceleration.

### Converting to a First-Order System

To convert our second-order ODE to a first-order system, we introduce a new variable $v = dx/dt$ (the velocity):

\begin{align}
\frac{dx}{dt} &= v \\
\frac{dv}{dt} &= -\omega^2 x
\end{align}

This transformation is common in physics. For example, in classical mechanics, we often convert Newton's second law (a second-order ODE) into phase space equations involving position and momentum. In the case of the harmonic oscillator:

1. The first equation simply states that the rate of change of position is the velocity
2. The second equation comes from $F = ma$ where $F = -kx$, giving $a = \frac{F}{m} = -\frac{k}{m}x = -\omega^2 x$

We can now represent the state of our system as a vector $\vec{y} = [x, v]^T$ and the derivative as $\dot{\vec{y}} = [v, -\omega^2 x]^T$. This representation is called the "phase space" description, and is fundamental in classical mechanics and dynamical systems theory.

::: {.panel-tabset}
### Euler Method

The simplest explicit integration method is the Euler method, derived from the first-order Taylor expansion:

\begin{equation}
\vec{y}(t + \Delta t) \approx \vec{y}(t) + \dot{\vec{y}}(t) \Delta t
\end{equation}

This is essentially a linear approximation—assuming the derivative stays constant over the small time step. Physically, it's like assuming constant velocity when updating position, and constant acceleration when updating velocity over each small time interval.

For the harmonic oscillator, this becomes:

\begin{align}
x_{n+1} &= x_n + v_n \Delta t \\
v_{n+1} &= v_n - \omega^2 x_n \Delta t
\end{align}

The physical interpretation is straightforward:
1. The new position equals the old position plus the displacement (velocity × time)
2. The new velocity equals the old velocity plus the acceleration (-ω²x) multiplied by the time step

In practice, the Euler method will cause the total energy of a harmonic oscillator to increase over time, which is physically incorrect. This is because the method doesn't account for the continuous change in acceleration during the time step.

```{pyodide}
#| autorun: false

def euler_method(f, y0, t_span, dt):
    """
    Implements the Euler method for solving ODEs.

    Parameters:
    f: Function that returns the derivative dy/dt = f(t, y)
    y0: Initial condition vector [position, velocity]
    t_span: (t_start, t_end)
    dt: Time step

    Returns:
    t: Array of time points
    y: Array of solution values
    """
    t_start, t_end = t_span
    n_steps = int((t_end - t_start) / dt) + 1
    t = np.linspace(t_start, t_end, n_steps)
    y = np.zeros((n_steps, len(y0)))
    y[0] = y0

    for i in range(1, n_steps):
        y[i] = y[i-1] + dt * f(t[i-1], y[i-1])

    return t, y
```

### Euler-Cromer Method

The Euler method often performs poorly for oscillatory systems, as it tends to artificially increase the energy over time. A simple but effective improvement is the Euler-Cromer method (also known as the symplectic Euler method), which uses the updated velocity for the position update:

\begin{align}
v_{n+1} &= v_n - \omega^2 x_n \Delta t \\
x_{n+1} &= x_n + v_{n+1} \Delta t
\end{align}

Notice the subtle but crucial difference: we use $v_{n+1}$ (the newly calculated velocity) to update the position, rather than $v_n$.

This small change dramatically improves energy conservation for oscillatory systems. From a physics perspective, this method better preserves the structure of Hamiltonian systems, which include the harmonic oscillator, planetary motion, and many other important physical systems.

The Euler-Cromer method is especially valuable in physics simulations where long-term energy conservation is important, such as:
- Orbital mechanics
- Molecular dynamics
- Plasma physics
- N-body simulations

While not perfect, this simple modification makes the method much more useful for real physical systems with oscillatory behavior.

```{pyodide}
#| autorun: false

def euler_cromer_method(f, y0, t_span, dt):
    """
    Implements the Euler-Cromer method for oscillatory systems.

    Parameters:
    f: Function that returns the derivative dy/dt = f(t, y)
    y0: Initial condition vector [position, velocity]
    t_span: (t_start, t_end)
    dt: Time step

    Returns:
    t: Array of time points
    y: Array of solution values
    """
    t_start, t_end = t_span
    n_steps = int((t_end - t_start) / dt) + 1
    t = np.linspace(t_start, t_end, n_steps)
    y = np.zeros((n_steps, len(y0)))
    y[0] = y0

    for i in range(1, n_steps):
        # Calculate derivatives
        derivatives = f(t[i-1], y[i-1])

        # Update velocity first
        y[i, 1] = y[i-1, 1] + dt * derivatives[1]

        # Then update position using the updated velocity
        y[i, 0] = y[i-1, 0] + dt * y[i, 1]

    return t, y
```

### Midpoint Method

For higher accuracy, we can use the midpoint method (also known as the second-order Runge-Kutta method or RK2). This evaluates the derivative at the midpoint of the interval, providing a better approximation of the average derivative over the time step.

\begin{align}
k_1 &= f(t_n, y_n) \\
k_2 &= f(t_n + \frac{\Delta t}{2}, y_n + \frac{\Delta t}{2}k_1) \\
y_{n+1} &= y_n + \Delta t \cdot k_2
\end{align}

The physical intuition behind this method is:
1. First, calculate the initial derivative $k_1$ (representing the initial rates of change)
2. Use this derivative to estimate the state at the middle of the time step
3. Calculate a new derivative $k_2$ at this midpoint
4. Use the midpoint derivative to advance the full step

This is analogous to finding the average velocity by looking at the velocity in the middle of a time interval, rather than just at the beginning. In physical problems with continuously varying forces, this provides a much better approximation than the Euler method.

The midpoint method achieves $O(\Delta t^2)$ accuracy, meaning the error decreases with the square of the step size. This makes it much more accurate than Euler's method for the same computational cost.

```{pyodide}
#| autorun: false

def midpoint_method(f, y0, t_span, dt):
    """
    Implements the midpoint method (RK2) for solving ODEs.

    Parameters:
    f: Function that returns the derivative dy/dt = f(t, y)
    y0: Initial condition vector [position, velocity]
    t_span: (t_start, t_end)
    dt: Time step

    Returns:
    t: Array of time points
    y: Array of solution values
    """
    t_start, t_end = t_span
    n_steps = int((t_end - t_start) / dt) + 1
    t = np.linspace(t_start, t_end, n_steps)
    y = np.zeros((n_steps, len(y0)))
    y[0] = y0

    for i in range(1, n_steps):
        k1 = f(t[i-1], y[i-1])
        k2 = f(t[i-1] + dt/2, y[i-1] + dt/2 * k1)
        y[i] = y[i-1] + dt * k2

    return t, y
```
:::

### Comparing the Methods

Let's compare these methods for the harmonic oscillator:

```{pyodide}
#| autorun: false

# Define the harmonic oscillator system
def harmonic_oscillator(t, y, omega=2.0):
    """
    Harmonic oscillator as a system of first-order ODEs.

    Parameters:
    t: time (not used explicitly, but required for compatibility with ODE solvers)
    y: state vector where y[0] is position x and y[1] is velocity v
    omega: angular frequency = sqrt(k/m) where k is spring constant and m is mass

    Returns:
    dydt: derivatives [dx/dt, dv/dt]

    Physics background:
    This implements the coupled differential equations:
    dx/dt = v
    dv/dt = -ω²x  (from F = ma where F = -kx)
    """
    dydt = np.zeros_like(y)
    dydt[0] = y[1]                # dx/dt = v
    dydt[1] = -omega**2 * y[0]    # dv/dt = -ω²x
    return dydt

# Analytical solution
def analytical_solution(t, x0, v0, omega):
    return x0 * np.cos(omega * t) + v0/omega * np.sin(omega * t)

# Parameters
omega = 2.0
x0 = 1.0
v0 = 0.0
y0 = np.array([x0, v0])
t_span = (0, 10)
dt = 0.1

# Solve using different methods
t_euler, y_euler = euler_method(lambda t, y: harmonic_oscillator(t, y, omega), y0, t_span, dt)
t_cromer, y_cromer = euler_cromer_method(lambda t, y: harmonic_oscillator(t, y, omega), y0, t_span, dt)
t_midpoint, y_midpoint = midpoint_method(lambda t, y: harmonic_oscillator(t, y, omega), y0, t_span, dt)

# Calculate analytical solution
y_analytical = analytical_solution(t_euler, x0, v0, omega)
```

```{pyodide}
#| autorun: false

# Plot comparisons
plt.figure(figsize=get_size(12,10))

# Position comparison
plt.plot(t_euler, y_euler[:, 0], label='Euler')
plt.plot(t_cromer, y_cromer[:, 0], label='Euler-Cromer')
plt.plot(t_midpoint, y_midpoint[:, 0], label='Midpoint')
plt.plot(t_euler, y_analytical, '--', label='Analytical')
plt.xlabel('Time (s)')
plt.ylabel('Position x(t)')

plt.legend()
plt.tight_layout()
plt.show()

```


```{pyodide}
#| autorun: false

# Energy calculation
def calculate_energy(y, omega):
    """
    Calculate the total energy of the harmonic oscillator.

    Parameters:
    y: state array with positions y[:,0] and velocities y[:,1]
    omega: angular frequency = sqrt(k/m)

    Returns:
    E: total energy at each time point

    Physics:
    E = KE + PE = (1/2)mv² + (1/2)kx²
      = (1/2)v² + (1/2)ω²x²  (assuming m=1)

    This energy should be conserved in a perfect harmonic oscillator system.
    The accuracy of energy conservation is a good measure of the
    quality of a numerical method.
    """
    kinetic = 0.5 * y[:, 1]**2       # KE = (1/2)mv² (assuming m=1)
    potential = 0.5 * omega**2 * y[:, 0]**2  # PE = (1/2)kx² = (1/2)ω²x²
    return kinetic + potential

# Energy comparison
E_euler = calculate_energy(y_euler, omega)
E_cromer = calculate_energy(y_cromer, omega)
E_midpoint = calculate_energy(y_midpoint, omega)
E_analytical = 0.5 * omega**2 * x0**2  # Constant for this initial condition

plt.figure(figsize=get_size(12,10))
plt.plot(t_euler, E_euler/E_analytical - 1, label='Euler')
plt.plot(t_cromer, E_cromer/E_analytical - 1, label='Euler-Cromer')
plt.plot(t_midpoint, E_midpoint/E_analytical - 1, label='Midpoint')
plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)
plt.xlabel('Time (s)')
plt.ylabel('Relative Energy Error')
plt.legend()


plt.tight_layout()
plt.show()
```

### Observations:

- **Euler Method**: Simple but least accurate. It systematically increases the total energy of the system, causing the amplitude to grow over time.
- **Euler-Cromer Method**: Much better energy conservation for oscillatory systems with minimal additional computation.
- **Midpoint Method**: Higher accuracy and better energy conservation.

The choice of method depends on the specific requirements of your problem:
- Use Euler for simplicity when accuracy is not critical
- Use Euler-Cromer for oscillatory systems when computational efficiency is important
- Use Midpoint or higher-order methods when accuracy is crucial

## Advanced Methods with SciPy

For practical applications, SciPy provides sophisticated ODE solvers with adaptive step size control, error estimation, and specialized algorithms for different types of problems. These methods represent the state-of-the-art in numerical integration and are what working physicists typically use for research and advanced applications.

### What Makes These Methods Advanced?

1. **Adaptive Step Size**: Unlike our fixed-step methods, these solvers can automatically adjust the step size based on the local error estimate, taking smaller steps where the solution changes rapidly and larger steps where it's smooth.

2. **Higher-Order Methods**: These solvers use higher-order approximations (4th, 5th, or even 8th order), achieving much higher accuracy with the same computational effort.

3. **Error Control**: They can maintain the error below a specified tolerance, giving you confidence in the accuracy of your results.

4. **Specialized Methods**: Different methods are optimized for different types of problems (stiff vs. non-stiff, conservative vs. dissipative).

### Using solve_ivp for the Harmonic Oscillator

```{pyodide}
#| autorun: false

from scipy.integrate import solve_ivp

# Define the ODE system
def SHO(t, y, omega=2.0):
    """
    Simple Harmonic Oscillator

    This function defines the right-hand side of the ODE system:
    dx/dt = v
    dv/dt = -ω²x
    """
    x, v = y   # Unpack the state vector: position and velocity
    dxdt = v   # Rate of change of position equals velocity
    dvdt = -omega**2 * x   # Rate of change of velocity equals acceleration
    return [dxdt, dvdt]

# Parameters
t_span = (0, 10)  # Time interval to solve over
y0 = [1.0, 0.0]   # Initial conditions: [position, velocity]
omega = 2.0       # Angular frequency (rad/s)

# Create evaluation points (for plotting)
t_eval = np.linspace(t_span[0], t_span[1], 200)

# Solve using different methods

# RK45: Explicit Runge-Kutta method of order 5(4)
# This is the default method, good for non-stiff problems
# It uses a 5th-order method with 4th-order error control
solution_RK45 = solve_ivp(
    lambda t, y: SHO(t, y, omega),
    t_span,
    y0,
    method='RK45',
    t_eval=t_eval
)

# BDF: Backward Differentiation Formula
# This is an implicit method designed for stiff problems
# (problems where certain components evolve on much faster timescales than others)
solution_BDF = solve_ivp(
    lambda t, y: SHO(t, y, omega),
    t_span,
    y0,
    method='BDF',
    t_eval=t_eval
)

# DOP853: Explicit Runge-Kutta method of order 8(5,3)
# This is a high-order method for high-precision requirements
# It uses an 8th-order formula for integration with 5th-order error estimation
solution_DOP853 = solve_ivp(
    lambda t, y: SHO(t, y, omega),
    t_span,
    y0,
    method='DOP853',  # Explicit Runge-Kutta method of order 8
    t_eval=t_eval
)

# Plot solutions
plt.figure(figsize=get_size(12, 10))

# Position comparison

plt.plot(solution_RK45.t, solution_RK45.y[0], label='RK45')
plt.plot(solution_BDF.t, solution_BDF.y[0], '--', label='BDF')
plt.plot(solution_DOP853.t, solution_DOP853.y[0], '-.', label='DOP853')
plt.plot(t_eval, analytical_solution(t_eval, y0[0], y0[1], omega), ':', label='Analytical')
plt.xlabel('Time (s)')
plt.ylabel('Position x(t)')
plt.legend()
plt.tight_layout()
plt.show()
```



```{pyodide}
# Phase space plot
plt.figure(figsize=get_size(12, 10))
plt.plot(solution_RK45.y[0], solution_RK45.y[1], label='Phase Space Trajectory')
plt.xlabel('Position x')
plt.ylabel('Velocity v')
plt.axis('equal')

plt.tight_layout()
plt.show()
```

## Advanced Example: Damped Driven Pendulum

Let's apply these methods to a more complex system: a damped driven pendulum. This system can exhibit chaotic behavior under certain conditions, making it a fascinating study in nonlinear dynamics.

::: {#fig-pendulum}

![](/lectures/lecture06/img/pendulum.png){fig-align="center" width="300px"}

The damped driven pendulum. External driving forces and damping (friction) create a system that can exhibit complex behaviors from regular oscillations to chaos.
:::


### Physical Context

The damped driven pendulum represents many real physical systems

- A physical pendulum with friction and external driving force
- An RLC circuit with nonlinear components
- Josephson junctions in superconductivity
- Certain types of mechanical and electrical oscillators

The equation of motion is:

\begin{equation}
\frac{d^2\theta}{dt^2} + b\frac{d\theta}{dt} + \omega_0^2\sin\theta = F_0\cos(\omega_d t)
\end{equation}

where

- $\theta$ is the angle from vertical (radians)
- $b$ is the damping coefficient (represents friction or resistance)
- $\omega_0 = \sqrt{g/L}$ is the natural frequency (where $g$ is gravity and $L$ is pendulum length)
- $F_0$ is the driving amplitude (strength of the external force)
- $\omega_d$ is the driving frequency (how rapidly the external force oscillates)

This equation differs from the harmonic oscillator in three crucial ways:

1. The $\sin\theta$ term (rather than just $\theta$) makes it nonlinear
2. The damping term $b\frac{d\theta}{dt}$ causes energy dissipation
3. The driving term $F_0\cos(\omega_d t)$ adds energy to the system

These additions make the system much more realistic and rich in behavior. Students could explore this system by varying parameters like the damping coefficient (b), driving amplitude (F0), and driving frequency (omega_d) to observe how the pendulum transitions between regular oscillations, period doubling, and chaos.

```{pyodide}
#| autorun: false

def damped_driven_pendulum(t, y, b=0.1, omega0=1.0, F0=0.5, omega_d=0.7):
    """
    Damped driven pendulum ODE system.

    Parameters:
    t: time (seconds)
    y: state vector where y[0] is theta (angle) and y[1] is omega (angular velocity)
    b: damping coefficient (1/second)
    omega0: natural frequency (rad/second) = sqrt(g/L)
    F0: driving amplitude (rad/second²)
    omega_d: driving frequency (rad/second)

    Returns:
    [dtheta_dt, domega_dt]: derivatives of state variables

    Physics:
    This system models a pendulum with:
    - Gravitational restoration torque: -omega0² * sin(theta)
    - Damping (friction) torque: -b * omega
    - External driving torque: F0 * cos(omega_d * t)

    The complete derivation comes from the torque equation:
    τ = I*α = -mgL*sin(θ) - b'*ω + τ_external

    Dividing by I (moment of inertia) gives this equation.
    """
    theta, omega = y  # Unpack the state vector
    dtheta_dt = omega  # Rate of change of angle equals angular velocity

    # Rate of change of angular velocity equals angular acceleration:
    # (Restoration) + (Damping) + (Driving)
    domega_dt = -omega0**2 * np.sin(theta) - b*omega + F0 * np.cos(omega_d * t)

    return [dtheta_dt, domega_dt]

# Parameters
t_span = (0, 150)     # Time span (seconds)
y0 = [0.1, 0.0]      # Initial conditions: [angle (rad), angular velocity (rad/s)]
b = 0.1              # Damping coefficient (friction) (1/s)
omega0 = 1.0         # Natural frequency = sqrt(g/L) (rad/s)
                     # (equivalent to a pendulum of length L = g/omega0² ≈ 9.81 m if omega0 = 1)
F0 = 0.1             # Driving amplitude (rad/s²)
omega_d = 0.7        # Driving frequency (rad/s)
                     # (driving period = 2π/omega_d ≈ 8.97 s)

# Solve the ODE system using solve_ivp
# We use RK45 which is well-suited for this type of problem
solution = solve_ivp(
    lambda t, y: damped_driven_pendulum(t, y, b, omega0, F0, omega_d),
    t_span,
    y0,
    method='RK45',
    t_eval=np.linspace(t_span[0], t_span[1], 1000),  # Points for evaluation
    rtol=1e-6,  # Relative tolerance - controls accuracy
    atol=1e-9   # Absolute tolerance - especially important for values near zero
)

# Plot solution
plt.figure(figsize=get_size(12, 10))

# Time series
plt.plot(solution.t, solution.y[0], label='Angle θ')
plt.plot(solution.t, solution.y[1], label='Angular Velocity ω')
plt.xlabel('Time (s)')
plt.ylabel('Value')
plt.legend()
plt.tight_layout()
plt.show()
```

```{pyodide}
#| autorun: false
# Phase space
plt.figure(figsize=get_size(12, 10))
plt.plot(solution.y[0] % (2*np.pi), solution.y[1])
plt.xlabel('Angle θ (mod 2π)')
plt.ylabel('Angular Velocity ω')


plt.tight_layout()
plt.show()
```

### Exploring Chaotic Behavior

Chaotic motion refers to a deterministic system whose behavior appears random and exhibits extreme sensitivity to initial conditions—the famous "butterfly effect." In the damped driven pendulum, chaos emerges when the system's nonlinear restoring force ($\sin\theta$ term) interacts with sufficient driving force. For this system, chaotic behavior typically appears when the driving amplitude $F_0$ exceeds a critical value (approximately 1.0 in our case) while maintaining moderate damping ($b \approx 0.1$) and a driving frequency $\omega_d$ that is not too far from the natural frequency $\omega_0$. In the chaotic regime, the pendulum's trajectory becomes unpredictable over long time scales, despite being governed by deterministic equations. The phase space transforms from closed orbits (regular motion) to a strange attractor with fractal structure, and the system never settles into a periodic oscillation.

::: {.callout-note collapse=true}
## Lyapunov Exponents: Quantifying Chaos

The Lyapunov exponent is a powerful mathematical tool for characterizing chaotic systems. It measures the rate at which nearby trajectories in phase space diverge over time. For a system with state vector $\vec{x}$, if two initial conditions differ by a small displacement $\delta\vec{x}_0$, then after time $t$, this separation grows approximately as:

$$|\delta\vec{x}(t)| \approx e^{\lambda t}|\delta\vec{x}_0|$$

where $\lambda$ is the Lyapunov exponent.

- A **positive** Lyapunov exponent ($\lambda > 0$) indicates chaos: nearby trajectories diverge exponentially, making long-term prediction impossible. The larger the exponent, the more chaotic the system.
- A **zero** Lyapunov exponent ($\lambda = 0$) suggests a stable limit cycle or quasiperiodic behavior.
- A **negative** Lyapunov exponent ($\lambda < 0$) indicates a stable fixed point, where trajectories converge.

For our damped driven pendulum, we can numerically estimate the Lyapunov exponent by comparing how two slightly different initial conditions evolve over time. For instance, we might begin with two pendulums at nearly the same angle—perhaps θ₁(0) = 0.1 and θ₂(0) = 0.1001—while keeping their initial angular velocities identical at ω₁(0) = ω₂(0) = 0. As we simulate both systems, we can track the separation between their trajectories in phase space. This separation represents the difference vector δx(t) = [θ₂(t) - θ₁(t), ω₂(t) - ω₁(t)]. Initially, this difference is very small, but in a chaotic system, it grows exponentially with time. By measuring this growth rate, we can compute the Lyapunov exponent as λ ≈ (1/t)ln(|δx(t)|/|δx(0)|). When examining our pendulum system, which has two dimensions (θ and ω), we actually have two Lyapunov exponents forming a spectrum. If either of these exponents is positive, we can conclusively determine that our system exhibits chaotic behavior, indicating fundamental unpredictability despite its deterministic nature.
:::

```{pyodide}
#| autorun: false

# Function to solve and plot for different driving amplitudes
def solve_for_driving_amplitude(F0):
    """
    Solves the pendulum equation for a given driving amplitude F0.

    Physics note:
    - For small F0, the system behaves like a damped oscillator with a periodic response
    - As F0 increases, the system can undergo period-doubling bifurcations
    - Above a critical value, the system becomes chaotic

    We skip the first 50 seconds (the "transient" behavior) to focus on the
    long-term behavior (the "attractor").
    """
    solution = solve_ivp(
        lambda t, y: damped_driven_pendulum(t, y, b=0.1, omega0=1.0, F0=F0, omega_d=0.7),
        (0, 100),  # Longer time span to observe chaos
        [0.1, 0.0],
        method='RK45',
        t_eval=np.linspace(50, 100, 1000)  # Only plot after transients die out
    )
    return solution

# Try different driving amplitudes to observe the transition to chaos
F0_values = [0.1, 1.0, 1.2]
plt.figure(figsize=get_size(15, 10))

behavior_types = ["Regular", "Period-Doubled", "Chaotic"]

for i, F0 in enumerate(F0_values):
    solution = solve_for_driving_amplitude(F0)

    # Time series plot
    plt.subplot(2, 3, i+1)
    plt.plot(solution.t, solution.y[0])
    plt.xlabel('Time (s)')
    plt.ylabel('Angle θ')

    # Phase space plot
    plt.subplot(2, 3, i+4)
    plt.plot(solution.y[0] % (2*np.pi), solution.y[1], '.', markersize=1)
    plt.xlabel('Angle θ (mod 2π)')
    plt.ylabel('Angular Velocity ω')

    # Add explanatory text for each type of behavior
    #if i == 0:
        #plt.text(1, 0, "Simple attractor\n(periodic motion)", fontsize=9)
    #elif i == 1:
        #plt.text(1, 0, "Period doubling\n(quasi-periodic)", fontsize=9)
    #else:
        #plt.text(1, 0, "Strange attractor\n(chaotic motion)", fontsize=9)

plt.tight_layout()
plt.show()
```

## Conclusion

In this lecture, we've built upon our knowledge of numerical differentiation to solve ordinary differential equations. We've explored:

1. **Implicit Matrix Methods**: Using matrices to solve the entire system at once
2. **Explicit Integration Methods**: Step-by-step methods like Euler, Euler-Cromer, and Midpoint
3. **Advanced SciPy Methods**: Leveraging powerful adaptive solvers for complex problems

Each approach has its strengths and is suited to different types of problems. The matrix method is excellent for linear systems, while explicit methods are more versatile for nonlinear problems. SciPy's solvers combine accuracy, stability, and efficiency for practical applications.

### Physical Relevance

The methods we've learned are essential tools for computational physics because they:
1. Allow us to study systems with no analytical solutions
2. Provide insights into nonlinear dynamics and chaos
3. Enable the modeling of realistic systems with friction, driving forces, and complex interactions
4. Connect mathematical formulations with observable physical phenomena

As a second-semester physics student, these numerical tools complement your analytical understanding of mechanics, electromagnetism, and other core physics subjects. When analytical methods reach their limits, these numerical approaches allow you to continue exploring and modeling physical reality.

## Exercises

Here are two simple exercises with solutions where you can train your programming skills. In these examples you should use the `odeint` method from SciPy's `integrate` module.

::: {.callout-note}
### Self-Exercise 1: Simple Harmonic Oscillator
Write a program to solve the equation of motion for a simple harmonic oscillator. This example demonstrates how to solve a second-order differential equation using scipy's `odeint`.

The equation of motion is: $\frac{d^2x}{dt^2} + \omega^2x = 0$

This represents an idealized spring-mass system or pendulum with small oscillations.


```{pyodide}
#| exercise: ex_1

import numpy as np
from scipy.integrate import odeint
import matplotlib.pyplot as plt

def oscillator(state, t, omega):
    # your code here
    # ypir code here

# Set parameters

# Solve ODE and plot solution
____
```

::: {.hint exercise="ex_1"}
Use `odeint(oscillator, initial_state, t, args=(omega,))` to solve the system. The solution will have two columns: position ([:,0]) and velocity ([:,1]). Create a plot showing position vs time using matplotlib. Remember to label your axes and add a title.
:::

::: {.solution exercise="ex_1"}
::: {.callout-note collapse="false"}
## Solution
```{pyodide}
import numpy as np
from scipy.integrate import odeint
import matplotlib.pyplot as plt

def oscillator(state, t, omega):
    x, v = state
    return [v, -omega**2 * x]

omega = 2.0
t = np.linspace(0, 10, 1000)
initial_state = [1, 0]

solution = odeint(oscillator, initial_state, t, args=(omega,))

plt.figure(figsize=(10, 4))
plt.plot(t, solution[:, 0])
plt.xlabel('Time (s)')
plt.ylabel('Position (m)')
plt.title('Simple Harmonic Motion')
plt.grid(True)
plt.show()
```
:::
:::
:::

::: {.callout-note}
### Self-Exercise 2: Damped Driven Harmonic Oscillator using Matrix Method
Model a damped harmonic oscillator with an external driving force using the implicit matrix method. This exercise demonstrates how to solve a second-order differential equation by constructing and solving a matrix system that represents the entire time evolution.

The equation of motion is: $m\frac{d^2x}{dt^2} + b\frac{dx}{dt} + kx = F_0 \cos(\omega t)$

Where $m$ is mass, $b$ is damping coefficient, $k$ is spring constant, $F_0$ is driving amplitude, and $\omega$ is driving frequency.

```{pyodide}
#| exercise: ex_2

import numpy as np
import matplotlib.pyplot as plt
from scipy.sparse import diags

# Define system parameters
m = 1.0      # mass (kg)
k = 16.0     # spring constant (N/m)
b = 0.5      # damping coefficient (kg/s)
F0 = 1.0     # driving force amplitude (N)
omega_d = 4.0  # driving frequency (rad/s)
omega0 = np.sqrt(k/m)  # natural frequency

# Setup time domain
T = 2*np.pi/omega0  # natural period
L = 10*T           # solve for 10 complete periods
N = 1000           # number of data points
t = np.linspace(0, L, N)  # time axis
dt = t[1] - t[0]   # time step

# Set initial conditions
x0 = 1.0    # initial position
v0 = 0.0    # initial velocity

# Step 1: Construct the matrix representing the left side of the equation
# This needs to include:
# - Second derivative operator (D2)
# - First derivative operator for damping (D1)
# - Potential term (diagonal with k/m)
# - Initial conditions

# Your code here

# Step 2: Define the right side of the equation with the driving force
# Your code here

# Step 3: Solve the system
# Your code here

# Step 4: Plot the solution
# Your code here
```

::: {.hint exercise="ex_2"}
First, construct a tridiagonal matrix for the second derivative operator $D_2$ with elements $\frac{1}{\Delta t^2}[1, -2, 1]$ along the diagonals.

For the first derivative (needed for damping), use a central difference approximation: $\frac{d x}{dt} \approx \frac{x_{i+1} - x_{i-1}}{2\Delta t}$, which gives a matrix $D_1$ with elements $\frac{1}{2\Delta t}[-1, 0, 1]$ along the diagonals.

The full matrix equation is $(D_2 + \frac{b}{m}D_1 + \frac{k}{m}I)x = \frac{F_0}{m}\cos(\omega t)$

To incorporate initial conditions, modify the first two rows of your matrix and the corresponding entries in your right-hand side vector:
- First row: $x_0 = x(0)$ (initial position)
- Second row: Approximation for initial velocity using forward difference

After solving, compare your solution with the analytical solution for the steady-state response of a driven harmonic oscillator.
:::

::: {.solution exercise="ex_2"}
::: {.callout-note collapse="false"}
## Solution
```{pyodide}
import numpy as np
import matplotlib.pyplot as plt
from scipy.sparse import diags

# Define system parameters
m = 1.0      # mass (kg)
k = 16.0     # spring constant (N/m)
b = 0.5      # damping coefficient (kg/s)
F0 = 1.0     # driving force amplitude (N)
omega_d = 4.0  # driving frequency (rad/s)
omega0 = np.sqrt(k/m)  # natural frequency

# Setup time domain
T = 2*np.pi/omega0  # natural period
L = 10*T           # solve for 10 complete periods
N = 1000           # number of data points
t = np.linspace(0, L, N)  # time axis
dt = t[1] - t[0]   # time step

# Set initial conditions
x0 = 1.0    # initial position
v0 = 0.0    # initial velocity

# Step 1: Construct the matrix for the left side of the equation
# Second derivative operator D2 (tridiagonal with [1, -2, 1]/dt²)
diag_vals = np.ones(N) * (-2.0/(dt**2))
upper_diag = np.ones(N-1) * (1.0/(dt**2))
lower_diag = np.ones(N-1) * (1.0/(dt**2))
D2 = diags([diag_vals, upper_diag, lower_diag], [0, 1, -1], shape=(N, N)).toarray()

# First derivative operator D1 for damping (tridiagonal with [-1, 0, 1]/(2dt))
diag_vals_d1 = np.zeros(N)
upper_diag_d1 = np.ones(N-1) * (1.0/(2*dt))
lower_diag_d1 = np.ones(N-1) * (-1.0/(2*dt))
D1 = diags([diag_vals_d1, upper_diag_d1, lower_diag_d1], [0, 1, -1], shape=(N, N)).toarray()

# Create the coefficient matrix: M = m*D2 + b*D1 + k*I
I = np.eye(N)  # Identity matrix
M = m*D2 + b*D1 + k*I

# Incorporate initial conditions (modify the first two rows)
M[0, :] = 0  # Clear first row for initial position
M[0, 0] = 1  # Set x_0 = x0

M[1, :] = 0  # Clear second row for initial velocity
M[1, 0] = -1  # Simple forward difference to encode initial velocity
M[1, 1] = 1
M[1, 2] = 0

# Step 2: Define the right side of the equation with the driving force
b_vector = np.zeros(N)
b_vector[2:] = F0 * np.cos(omega_d * t[2:])  # Driving force (skip first two points for initial conditions)

# Set initial conditions in the right-hand side vector
b_vector[0] = x0  # Initial position
b_vector[1] = v0 * dt  # Initial velocity (scaled by dt for the difference approximation)

# Step 3: Solve the system
x = np.linalg.solve(M, b_vector)

# Analytical steady-state solution for comparison
phase_angle = np.arctan2(b*omega_d, (k-m*omega_d**2))
amplitude = F0 / np.sqrt((k-m*omega_d**2)**2 + (b*omega_d)**2)
x_analytical = amplitude * np.cos(omega_d*t - phase_angle) + x0*np.exp(-b*t/(2*m))*np.cos(np.sqrt(k/m-(b/(2*m))**2)*t)

# Step 4: Plot the solution
plt.figure(figsize=get_size(8, 12))

# Plot numerical vs analytical solution
plt.subplot(2, 1, 1)
plt.plot(t, x, 'b-', label='Matrix Solution')
plt.plot(t, x_analytical, 'r--', label='Analytical Solution')
plt.xlabel('Time (s)')
plt.ylabel('Position (m)')



# Plot phase space trajectory
plt.subplot(2, 1, 2)
# Calculate velocity using central difference approximation
v = np.zeros_like(x)
v[1:-1] = (x[2:] - x[:-2]) / (2*dt)  # Central difference for internal points
v[0] = v0  # Use initial velocity
v[-1] = (x[-1] - x[-2]) / dt  # Forward difference for last point

plt.plot(x, v, 'g-')
plt.xlabel('Position (m)')
plt.ylabel('Velocity (m/s)')

plt.tight_layout()
plt.show()

```
:::
:::
:::



## Further Reading

### Numerical Methods
- "Numerical Recipes: The Art of Scientific Computing" by Press, Teukolsky, Vetterling, and Flannery - The standard reference for numerical methods in scientific computing
- "Computational Physics" by Mark Newman - Excellent introduction with Python examples
- SciPy documentation: [scipy.integrate.solve_ivp](https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.solve_ivp.html)

### Physics Applications
- "Computational Physics: Problem Solving with Python" by Landau, Páez, and Bordeianu - Connects computational methods with physics problems
- "An Introduction to Computer Simulation Methods" by Gould, Tobochnik, and Christian - Comprehensive coverage of simulation techniques in physics

### Dynamical Systems and Chaos
- "Differential Equations, Dynamical Systems, and an Introduction to Chaos" by Hirsch, Smale, and Devaney
- "Nonlinear Dynamics And Chaos" by Steven Strogatz - Accessible introduction to nonlinear dynamics
- "Chaos: Making a New Science" by James Gleick - Popular science book on the history and significance of chaos theory

### Online Resources
- [Scipy Lecture Notes](https://scipy-lectures.org/) - Tutorials on scientific computing with Python
- [Matplotlib Gallery](https://matplotlib.org/stable/gallery/index.html) - Examples of scientific visualization
