{
  "hash": "d046dbab03016a7047bc632d208f5922",
  "result": {
    "engine": "jupyter",
    "markdown": "---\nformat:\n  live-html:\n    toc: true\n    toc-location: right\npyodide:\n  autorun: false\n  packages:\n    - matplotlib\n    - numpy\n    - scipy\n---\n\n# Solving Ordinary Differential Equations\n\n## Introduction\n\nIn the previous lecture on numerical differentiation, we explored how to compute derivatives numerically using finite difference methods and matrix representations. We learned that:\n\n1. Finite difference approximations allow us to estimate derivatives at discrete points\n2. Differentiation can be represented as matrix operations\n3. The accuracy of these approximations depends on the order of the method and step size\n\nBuilding on this foundation, we can now tackle one of the most important applications in computational physics: solving ordinary differential equations (ODEs). Almost all dynamical systems in physics are described by differential equations, and learning how to solve them numerically is essential for modeling physical phenomena.\n\nAs second-semester physics students, you've likely encountered differential equations in various contexts:\n\n- Newton's second law: $F = ma = m\\frac{d^2x}{dt^2}$\n- Simple harmonic motion: $\\frac{d^2x}{dt^2} + \\omega^2x = 0$\n- RC circuits: $\\frac{dQ}{dt} + \\frac{1}{RC}Q = 0$\n- Heat diffusion: $\\frac{\\partial T}{\\partial t} = \\alpha \\nabla^2 T$\n\nWhile analytical solutions exist for some simple cases, real-world physics problems often involve complex systems where analytical solutions are either impossible or impractical to obtain. This is where numerical methods become indispensable tools for the working physicist.\n\n```{pyodide}\n#| edit: false\n#| echo: false\n#| execute: true\n\nimport numpy as np\nimport io\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.sparse import diags\nfrom scipy.integrate import solve_ivp\n\n# default values for plotting\nplt.rcParams.update({'font.size': 12,\n                     'lines.linewidth': 1.5,\n                     'lines.markersize': 5,\n                     'axes.labelsize': 11,\n                     'xtick.labelsize': 10,\n                     'ytick.labelsize': 10,\n                     'xtick.top': True,\n                     'xtick.direction': 'in',\n                     'ytick.right': True,\n                     'ytick.direction': 'in',})\n\ndef get_size(w,h):\n      return((w/2.54,h/2.54))\n\ndef set_plot_style():\n    plt.style.use('seaborn-v0_8-whitegrid')\n    plt.rcParams.update({\n        'figure.figsize': (10, 6),\n        'axes.grid': True,\n        'grid.linestyle': '--',\n        'grid.alpha': 0.7,\n        'lines.linewidth': 2,\n        'font.size': 12,\n        'axes.labelsize': 14,\n        'axes.titlesize': 16,\n        'xtick.labelsize': 12,\n        'ytick.labelsize': 12\n    })\n```\n\n## Types of ODE Solution Methods\n\nThere are two main approaches to solving ODEs numerically:\n\n1. **Implicit methods**: These methods solve for all time points simultaneously using matrix operations. They treat the problem as a large system of coupled algebraic equations. Just as you might solve a system of linear equations $Ax = b$ in linear algebra, implicit methods set up and solve a larger matrix equation that represents the entire evolution of the system.\n\n2. **Explicit methods**: These methods march forward in time, computing the solution step by step. Starting from the initial conditions, they use the current state to calculate the next state, similar to how you might use the position and velocity at time $t$ to predict the position and velocity at time $t + \\Delta t$.\n\nWe'll explore both approaches, highlighting their strengths and limitations. Each has its place in physics: implicit methods often handle \"stiff\" problems better (problems with vastly different timescales), while explicit methods are typically easier to implement and can handle nonlinear problems more naturally.\n\n## The Harmonic Oscillator: A Prototypical ODE\n\n::: {.callout-note}\n## Physics Interlude: The Harmonic Oscillator\n\nThe harmonic oscillator is one of the most fundamental systems in physics, appearing in mechanics, electromagnetism, quantum mechanics, and many other fields. It serves as an excellent test case for ODE solvers due to its simplicity and known analytical solution.\n\nThe harmonic oscillator describes any system that experiences a restoring force proportional to displacement. Physically, this occurs in:\n\n- A mass on a spring (Hooke's law: $F = -kx$)\n- A pendulum for small angles (where $\\sin\\theta \\approx \\theta$)\n- An LC circuit with inductors and capacitors\n- Molecular vibrations in chemistry\n- Phonons in solid state physics\n\n\n::: {#fig-test}\n\n![](/lectures/lecture06/img/spring.png){fig-align=\"center\" width=\"300px\"}\n\nA mass suspended from a spring demonstrates a classic harmonic oscillator. When displaced from equilibrium, the spring exerts a restoring force proportional to the displacement, leading to oscillatory motion.\n:::\nThe equation of motion for a classical harmonic oscillator is:\n\n\\begin{equation}\n\\frac{d^2x}{dt^2} + \\omega^2 x = 0\n\\end{equation}\n\nwhere $\\omega = \\sqrt{k/m}$ is the angular frequency, with $k$ being the spring constant and $m$ the mass. In terms of Newton's second law, this is:\n\\begin{equation}\nm\\frac{d^2x}{dt^2} = -kx\n\\end{equation}\n\nThis second-order differential equation requires two initial conditions:\n- Initial position: $x(t=0) = x_0$\n- Initial velocity: $\\dot{x}(t=0) = v_0$\n\nThe analytical solution is:\n\\begin{equation}\nx(t) = x_0 \\cos(\\omega t) + \\frac{v_0}{\\omega} \\sin(\\omega t)\n\\end{equation}\n\nThis represents sinusoidal oscillation with a constant amplitude and period $T = 2\\pi/\\omega$. The total energy of the system (kinetic + potential) remains constant:\n\\begin{equation}\nE = \\frac{1}{2}mv^2 + \\frac{1}{2}kx^2\n\\end{equation}\n\nThis energy conservation will be an important test for our numerical methods.\n:::\n\n## Implicit Matrix Solution\n\nUsing the matrix representation of the second derivative operator from our previous lecture, we can transform the ODE into a linear system that can be solved in one step. This approach treats the entire time evolution as a single mathematical problem.\n\n### Setting Up the System\n\nFrom calculus, we know that the second derivative represents the rate of change of the rate of change. Physically, this corresponds to acceleration in mechanics. In numerical terms, we need to approximate this using finite differences.\n\nRecall that we can represent the second derivative operator as a tridiagonal matrix:\n\n$$D_2 = \\frac{1}{\\Delta t^2}\n\\begin{bmatrix}\n-2 & 1  & 0 & 0 & \\cdots & 0\\\\\n 1 & -2 & 1 & 0 & \\cdots & 0\\\\\n 0 & 1  & -2 & 1 & \\cdots & 0\\\\\n \\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n 0 & \\cdots & 0 & 1 & -2 & 1\\\\\n 0 & \\cdots & 0 & 0 & 1 & -2\\\\\n\\end{bmatrix}$$\n\nThis matrix implements the central difference approximation for the second derivative:\n\\begin{equation}\n\\frac{d^2x}{dt^2} \\approx \\frac{x_{i+1} - 2x_i + x_{i-1}}{\\Delta t^2}\n\\end{equation}\n\nEach row in this matrix corresponds to the equation for a specific time point, linking it to its neighbors.\n\nThe harmonic oscillator term $\\omega^2 x$ represents the restoring force. In a spring system, this is $F = -kx$ divided by mass, giving $a = -\\frac{k}{m}x = -\\omega^2 x$. This can be represented by a diagonal matrix:\n\n$$V = \\omega^2 I = \\omega^2\n\\begin{bmatrix}\n1 & 0 & 0 & \\cdots & 0\\\\\n0 & 1 & 0 & \\cdots & 0\\\\\n0 & 0 & 1 & \\cdots & 0\\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n0 & 0 & 0 & \\cdots & 1\\\\\n\\end{bmatrix}$$\n\nOur equation $\\frac{d^2x}{dt^2} + \\omega^2 x = 0$ becomes the matrix equation $(D_2 + V)x = 0$, where $x$ is the vector containing the positions at all time points $(x_1, x_2, ..., x_n)$. This is a large system of linear equations that determine the entire trajectory at once.\n\n### Incorporating Initial Conditions\n\nTo solve this system, we need to incorporate the initial conditions by modifying the first rows of our matrix. This is a crucial step because a second-order differential equation needs two initial conditions to define a unique solution. In physical terms, we need to know both the initial position (displacement) and the initial velocity of our oscillator. We incorporate these conditions directly into our matrix equation:\n\n$$M =\n\\begin{bmatrix}\n\\color{red}{1} & \\color{red}{0} & \\color{red}{0} & \\color{red}{0} & \\color{red}{\\cdots} & \\color{red}{0} \\\\\n\\color{blue}{-1} & \\color{blue}{1} & \\color{blue}{0} & \\color{blue}{0} & \\color{blue}{\\cdots} & \\color{blue}{0} \\\\\n\\frac{1}{\\Delta t^2} & -\\frac{2}{\\Delta t^2}+\\omega^2 & \\frac{1}{\\Delta t^2} & 0 & \\cdots & 0 \\\\\n0 & \\frac{1}{\\Delta t^2} & -\\frac{2}{\\Delta t^2}+\\omega^2 & \\frac{1}{\\Delta t^2} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & \\cdots & 0 & \\frac{1}{\\Delta t^2} & -\\frac{2}{\\Delta t^2}+\\omega^2 & \\frac{1}{\\Delta t^2} \\\\\n\\end{bmatrix}$$\n\nThe first row (in red) enforces $x(0) = x_0$ (initial position), and the second row (in blue) implements the initial velocity condition. The rest of the matrix represents the differential equation at each time step. By incorporating the initial conditions directly into the matrix, we ensure that our solution satisfies both the differential equation and the initial conditions.\n\n```{pyodide}\n#| autorun: false\n\n# Define parameters\nk = 15.5  # spring constant (N/m)\nm = 0.2   # mass (kg)\nx0 = 1    # initial elongation (m)\nv0 = 0    # initial velocity (m/s)\nomega = np.sqrt(k/m)  # angular frequency (rad/s)\n                      # This comes from the physics: ω² = k/m\n\n# Set up time domain\nT = 2*np.pi/omega  # natural period of oscillation\nL = 3*T            # solve for 3 complete periods\nN = 500            # number of data points (time resolution)\nt = np.linspace(0, L, N)  # time axis\ndt = t[1] - t[0]  # time interval of each step\n\n# Construct the matrix M that represents (D2 + V)\n# This combines the second derivative operator and the potential\nM = (diags([-2., 1., 1.], [-1,-2, 0], shape=(N, N))+diags([1], [-1], shape=(N, N))* omega**2*dt**2).todense()\n\n# Incorporate initial conditions\nM[0,0]=1  # First equation: x(0) = x0 (initial position)\nM[1,0]=-1  # Second equation: velocity condition\nM[1,1]=1   # These two lines encode v(0) = v0\n\n# Create the right-hand side vector containing the initial conditions\nb = np.zeros(N)  # initialize vector of zeros\nb[0]=1  # Set initial position (x0 = 1)\nb[1]=0  # Set initial velocity (v0 = 0)\nb = b.transpose()\n\n# Solve the linear system Mx = b\n# This gives us the position at all time points at once\nx = np.linalg.solve(M, b)  # this is the solution\n```\n\n\n```{pyodide}\n#| autorun: false\n\n# Plot the solution\nplt.figure(figsize=get_size(12, 10))\nplt.plot(t, x, label='Numerical Solution')\nplt.plot(t, x0*np.cos(omega*t) + v0/omega*np.sin(omega*t), '--', label='Analytical Solution')\nplt.xlabel('Time (s)')\nplt.ylabel('Position x(t)')\nplt.legend()\nplt.show()\n```\n\nThis matrix-based approach has several advantages:\n- It solves for all time points simultaneously, giving the entire trajectory in one operation\n- It can be very stable for certain types of problems, particularly \"stiff\" equations\n- It handles boundary conditions naturally by incorporating them into the matrix structure\n- It often provides good energy conservation for oscillatory systems\n\nHowever, it also has limitations:\n- It requires solving a large linear system, which becomes computationally intensive for long simulations\n- It's not suitable for nonlinear problems without modification (e.g., a pendulum with large angles where $\\sin\\theta \\neq \\theta$)\n- Memory requirements grow with the time span (an N×N matrix for N time steps)\n- Implementing time-dependent forces is more complex than with explicit methods\n\nFrom a physics perspective, this approach is similar to finding stationary states in quantum mechanics or solving boundary value problems in electrostatics—we're solving the entire system at once rather than stepping through time.\n\n## Explicit Numerical Integration Methods\n\nAn alternative approach is to use explicit step-by-step integration methods. These methods convert the second-order ODE to a system of first-order ODEs and then advance the solution incrementally, similar to how we intuitively understand physical motion: position changes based on velocity, and velocity changes based on acceleration.\n\n### Converting to a First-Order System\n\nTo convert our second-order ODE to a first-order system, we introduce a new variable $v = dx/dt$ (the velocity):\n\n\\begin{align}\n\\frac{dx}{dt} &= v \\\\\n\\frac{dv}{dt} &= -\\omega^2 x\n\\end{align}\n\nThis transformation is common in physics. For example, in classical mechanics, we often convert Newton's second law (a second-order ODE) into phase space equations involving position and momentum. In the case of the harmonic oscillator:\n\n1. The first equation simply states that the rate of change of position is the velocity\n2. The second equation comes from $F = ma$ where $F = -kx$, giving $a = \\frac{F}{m} = -\\frac{k}{m}x = -\\omega^2 x$\n\nWe can now represent the state of our system as a vector $\\vec{y} = [x, v]^T$ and the derivative as $\\dot{\\vec{y}} = [v, -\\omega^2 x]^T$. This representation is called the \"phase space\" description, and is fundamental in classical mechanics and dynamical systems theory.\n\n::: {.panel-tabset}\n### Euler Method\n\nThe simplest explicit integration method is the Euler method, derived from the first-order Taylor expansion:\n\n\\begin{equation}\n\\vec{y}(t + \\Delta t) \\approx \\vec{y}(t) + \\dot{\\vec{y}}(t) \\Delta t\n\\end{equation}\n\nThis is essentially a linear approximation—assuming the derivative stays constant over the small time step. Physically, it's like assuming constant velocity when updating position, and constant acceleration when updating velocity over each small time interval.\n\nFor the harmonic oscillator, this becomes:\n\n\\begin{align}\nx_{n+1} &= x_n + v_n \\Delta t \\\\\nv_{n+1} &= v_n - \\omega^2 x_n \\Delta t\n\\end{align}\n\nThe physical interpretation is straightforward:\n1. The new position equals the old position plus the displacement (velocity × time)\n2. The new velocity equals the old velocity plus the acceleration (-ω²x) multiplied by the time step\n\nIn practice, the Euler method will cause the total energy of a harmonic oscillator to increase over time, which is physically incorrect. This is because the method doesn't account for the continuous change in acceleration during the time step.\n\n```{pyodide}\n#| autorun: false\n\ndef euler_method(f, y0, t_span, dt):\n    \"\"\"\n    Implements the Euler method for solving ODEs.\n\n    Parameters:\n    f: Function that returns the derivative dy/dt = f(t, y)\n    y0: Initial condition vector [position, velocity]\n    t_span: (t_start, t_end)\n    dt: Time step\n\n    Returns:\n    t: Array of time points\n    y: Array of solution values\n    \"\"\"\n    t_start, t_end = t_span\n    n_steps = int((t_end - t_start) / dt) + 1\n    t = np.linspace(t_start, t_end, n_steps)\n    y = np.zeros((n_steps, len(y0)))\n    y[0] = y0\n\n    for i in range(1, n_steps):\n        y[i] = y[i-1] + dt * f(t[i-1], y[i-1])\n\n    return t, y\n```\n\n### Euler-Cromer Method\n\nThe Euler method often performs poorly for oscillatory systems, as it tends to artificially increase the energy over time. A simple but effective improvement is the Euler-Cromer method (also known as the symplectic Euler method), which uses the updated velocity for the position update:\n\n\\begin{align}\nv_{n+1} &= v_n - \\omega^2 x_n \\Delta t \\\\\nx_{n+1} &= x_n + v_{n+1} \\Delta t\n\\end{align}\n\nNotice the subtle but crucial difference: we use $v_{n+1}$ (the newly calculated velocity) to update the position, rather than $v_n$.\n\nThis small change dramatically improves energy conservation for oscillatory systems. From a physics perspective, this method better preserves the structure of Hamiltonian systems, which include the harmonic oscillator, planetary motion, and many other important physical systems.\n\nThe Euler-Cromer method is especially valuable in physics simulations where long-term energy conservation is important, such as:\n- Orbital mechanics\n- Molecular dynamics\n- Plasma physics\n- N-body simulations\n\nWhile not perfect, this simple modification makes the method much more useful for real physical systems with oscillatory behavior.\n\n```{pyodide}\n#| autorun: false\n\ndef euler_cromer_method(f, y0, t_span, dt):\n    \"\"\"\n    Implements the Euler-Cromer method for oscillatory systems.\n\n    Parameters:\n    f: Function that returns the derivative dy/dt = f(t, y)\n    y0: Initial condition vector [position, velocity]\n    t_span: (t_start, t_end)\n    dt: Time step\n\n    Returns:\n    t: Array of time points\n    y: Array of solution values\n    \"\"\"\n    t_start, t_end = t_span\n    n_steps = int((t_end - t_start) / dt) + 1\n    t = np.linspace(t_start, t_end, n_steps)\n    y = np.zeros((n_steps, len(y0)))\n    y[0] = y0\n\n    for i in range(1, n_steps):\n        # Calculate derivatives\n        derivatives = f(t[i-1], y[i-1])\n\n        # Update velocity first\n        y[i, 1] = y[i-1, 1] + dt * derivatives[1]\n\n        # Then update position using the updated velocity\n        y[i, 0] = y[i-1, 0] + dt * y[i, 1]\n\n    return t, y\n```\n\n### Midpoint Method\n\nFor higher accuracy, we can use the midpoint method (also known as the second-order Runge-Kutta method or RK2). This evaluates the derivative at the midpoint of the interval, providing a better approximation of the average derivative over the time step.\n\n\\begin{align}\nk_1 &= f(t_n, y_n) \\\\\nk_2 &= f(t_n + \\frac{\\Delta t}{2}, y_n + \\frac{\\Delta t}{2}k_1) \\\\\ny_{n+1} &= y_n + \\Delta t \\cdot k_2\n\\end{align}\n\nThe physical intuition behind this method is:\n1. First, calculate the initial derivative $k_1$ (representing the initial rates of change)\n2. Use this derivative to estimate the state at the middle of the time step\n3. Calculate a new derivative $k_2$ at this midpoint\n4. Use the midpoint derivative to advance the full step\n\nThis is analogous to finding the average velocity by looking at the velocity in the middle of a time interval, rather than just at the beginning. In physical problems with continuously varying forces, this provides a much better approximation than the Euler method.\n\nThe midpoint method achieves $O(\\Delta t^2)$ accuracy, meaning the error decreases with the square of the step size. This makes it much more accurate than Euler's method for the same computational cost.\n\n```{pyodide}\n#| autorun: false\n\ndef midpoint_method(f, y0, t_span, dt):\n    \"\"\"\n    Implements the midpoint method (RK2) for solving ODEs.\n\n    Parameters:\n    f: Function that returns the derivative dy/dt = f(t, y)\n    y0: Initial condition vector [position, velocity]\n    t_span: (t_start, t_end)\n    dt: Time step\n\n    Returns:\n    t: Array of time points\n    y: Array of solution values\n    \"\"\"\n    t_start, t_end = t_span\n    n_steps = int((t_end - t_start) / dt) + 1\n    t = np.linspace(t_start, t_end, n_steps)\n    y = np.zeros((n_steps, len(y0)))\n    y[0] = y0\n\n    for i in range(1, n_steps):\n        k1 = f(t[i-1], y[i-1])\n        k2 = f(t[i-1] + dt/2, y[i-1] + dt/2 * k1)\n        y[i] = y[i-1] + dt * k2\n\n    return t, y\n```\n:::\n\n### Comparing the Methods\n\nLet's compare these methods for the harmonic oscillator:\n\n```{pyodide}\n#| autorun: false\n\n# Define the harmonic oscillator system\ndef harmonic_oscillator(t, y, omega=2.0):\n    \"\"\"\n    Harmonic oscillator as a system of first-order ODEs.\n\n    Parameters:\n    t: time (not used explicitly, but required for compatibility with ODE solvers)\n    y: state vector where y[0] is position x and y[1] is velocity v\n    omega: angular frequency = sqrt(k/m) where k is spring constant and m is mass\n\n    Returns:\n    dydt: derivatives [dx/dt, dv/dt]\n\n    Physics background:\n    This implements the coupled differential equations:\n    dx/dt = v\n    dv/dt = -ω²x  (from F = ma where F = -kx)\n    \"\"\"\n    dydt = np.zeros_like(y)\n    dydt[0] = y[1]                # dx/dt = v\n    dydt[1] = -omega**2 * y[0]    # dv/dt = -ω²x\n    return dydt\n\n# Analytical solution\ndef analytical_solution(t, x0, v0, omega):\n    return x0 * np.cos(omega * t) + v0/omega * np.sin(omega * t)\n\n# Parameters\nomega = 2.0\nx0 = 1.0\nv0 = 0.0\ny0 = np.array([x0, v0])\nt_span = (0, 10)\ndt = 0.1\n\n# Solve using different methods\nt_euler, y_euler = euler_method(lambda t, y: harmonic_oscillator(t, y, omega), y0, t_span, dt)\nt_cromer, y_cromer = euler_cromer_method(lambda t, y: harmonic_oscillator(t, y, omega), y0, t_span, dt)\nt_midpoint, y_midpoint = midpoint_method(lambda t, y: harmonic_oscillator(t, y, omega), y0, t_span, dt)\n\n# Calculate analytical solution\ny_analytical = analytical_solution(t_euler, x0, v0, omega)\n```\n\n```{pyodide}\n#| autorun: false\n\n# Plot comparisons\nplt.figure(figsize=get_size(12,10))\n\n# Position comparison\nplt.plot(t_euler, y_euler[:, 0], label='Euler')\nplt.plot(t_cromer, y_cromer[:, 0], label='Euler-Cromer')\nplt.plot(t_midpoint, y_midpoint[:, 0], label='Midpoint')\nplt.plot(t_euler, y_analytical, '--', label='Analytical')\nplt.xlabel('Time (s)')\nplt.ylabel('Position x(t)')\n\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n```\n\n\n```{pyodide}\n#| autorun: false\n\n# Energy calculation\ndef calculate_energy(y, omega):\n    \"\"\"\n    Calculate the total energy of the harmonic oscillator.\n\n    Parameters:\n    y: state array with positions y[:,0] and velocities y[:,1]\n    omega: angular frequency = sqrt(k/m)\n\n    Returns:\n    E: total energy at each time point\n\n    Physics:\n    E = KE + PE = (1/2)mv² + (1/2)kx²\n      = (1/2)v² + (1/2)ω²x²  (assuming m=1)\n\n    This energy should be conserved in a perfect harmonic oscillator system.\n    The accuracy of energy conservation is a good measure of the\n    quality of a numerical method.\n    \"\"\"\n    kinetic = 0.5 * y[:, 1]**2       # KE = (1/2)mv² (assuming m=1)\n    potential = 0.5 * omega**2 * y[:, 0]**2  # PE = (1/2)kx² = (1/2)ω²x²\n    return kinetic + potential\n\n# Energy comparison\nE_euler = calculate_energy(y_euler, omega)\nE_cromer = calculate_energy(y_cromer, omega)\nE_midpoint = calculate_energy(y_midpoint, omega)\nE_analytical = 0.5 * omega**2 * x0**2  # Constant for this initial condition\n\nplt.figure(figsize=get_size(12,10))\nplt.plot(t_euler, E_euler/E_analytical - 1, label='Euler')\nplt.plot(t_cromer, E_cromer/E_analytical - 1, label='Euler-Cromer')\nplt.plot(t_midpoint, E_midpoint/E_analytical - 1, label='Midpoint')\nplt.axhline(y=0, color='k', linestyle='--', alpha=0.3)\nplt.xlabel('Time (s)')\nplt.ylabel('Relative Energy Error')\nplt.legend()\n\n\nplt.tight_layout()\nplt.show()\n```\n\n### Observations:\n\n- **Euler Method**: Simple but least accurate. It systematically increases the total energy of the system, causing the amplitude to grow over time.\n- **Euler-Cromer Method**: Much better energy conservation for oscillatory systems with minimal additional computation.\n- **Midpoint Method**: Higher accuracy and better energy conservation.\n\nThe choice of method depends on the specific requirements of your problem:\n- Use Euler for simplicity when accuracy is not critical\n- Use Euler-Cromer for oscillatory systems when computational efficiency is important\n- Use Midpoint or higher-order methods when accuracy is crucial\n\n## Advanced Methods with SciPy\n\nFor practical applications, SciPy provides sophisticated ODE solvers with adaptive step size control, error estimation, and specialized algorithms for different types of problems. These methods represent the state-of-the-art in numerical integration and are what working physicists typically use for research and advanced applications.\n\n### What Makes These Methods Advanced?\n\n1. **Adaptive Step Size**: Unlike our fixed-step methods, these solvers can automatically adjust the step size based on the local error estimate, taking smaller steps where the solution changes rapidly and larger steps where it's smooth.\n\n2. **Higher-Order Methods**: These solvers use higher-order approximations (4th, 5th, or even 8th order), achieving much higher accuracy with the same computational effort.\n\n3. **Error Control**: They can maintain the error below a specified tolerance, giving you confidence in the accuracy of your results.\n\n4. **Specialized Methods**: Different methods are optimized for different types of problems (stiff vs. non-stiff, conservative vs. dissipative).\n\n### Using solve_ivp for the Harmonic Oscillator\n\n```{pyodide}\n#| autorun: false\n\nfrom scipy.integrate import solve_ivp\n\n# Define the ODE system\ndef SHO(t, y, omega=2.0):\n    \"\"\"\n    Simple Harmonic Oscillator\n\n    This function defines the right-hand side of the ODE system:\n    dx/dt = v\n    dv/dt = -ω²x\n    \"\"\"\n    x, v = y   # Unpack the state vector: position and velocity\n    dxdt = v   # Rate of change of position equals velocity\n    dvdt = -omega**2 * x   # Rate of change of velocity equals acceleration\n    return [dxdt, dvdt]\n\n# Parameters\nt_span = (0, 10)  # Time interval to solve over\ny0 = [1.0, 0.0]   # Initial conditions: [position, velocity]\nomega = 2.0       # Angular frequency (rad/s)\n\n# Create evaluation points (for plotting)\nt_eval = np.linspace(t_span[0], t_span[1], 200)\n\n# Solve using different methods\n\n# RK45: Explicit Runge-Kutta method of order 5(4)\n# This is the default method, good for non-stiff problems\n# It uses a 5th-order method with 4th-order error control\nsolution_RK45 = solve_ivp(\n    lambda t, y: SHO(t, y, omega),\n    t_span,\n    y0,\n    method='RK45',\n    t_eval=t_eval\n)\n\n# BDF: Backward Differentiation Formula\n# This is an implicit method designed for stiff problems\n# (problems where certain components evolve on much faster timescales than others)\nsolution_BDF = solve_ivp(\n    lambda t, y: SHO(t, y, omega),\n    t_span,\n    y0,\n    method='BDF',\n    t_eval=t_eval\n)\n\n# DOP853: Explicit Runge-Kutta method of order 8(5,3)\n# This is a high-order method for high-precision requirements\n# It uses an 8th-order formula for integration with 5th-order error estimation\nsolution_DOP853 = solve_ivp(\n    lambda t, y: SHO(t, y, omega),\n    t_span,\n    y0,\n    method='DOP853',  # Explicit Runge-Kutta method of order 8\n    t_eval=t_eval\n)\n\n# Plot solutions\nplt.figure(figsize=get_size(12, 10))\n\n# Position comparison\n\nplt.plot(solution_RK45.t, solution_RK45.y[0], label='RK45')\nplt.plot(solution_BDF.t, solution_BDF.y[0], '--', label='BDF')\nplt.plot(solution_DOP853.t, solution_DOP853.y[0], '-.', label='DOP853')\nplt.plot(t_eval, analytical_solution(t_eval, y0[0], y0[1], omega), ':', label='Analytical')\nplt.xlabel('Time (s)')\nplt.ylabel('Position x(t)')\nplt.legend()\nplt.tight_layout()\nplt.show()\n```\n\n\n\n```{pyodide}\n# Phase space plot\nplt.figure(figsize=get_size(12, 10))\nplt.plot(solution_RK45.y[0], solution_RK45.y[1], label='Phase Space Trajectory')\nplt.xlabel('Position x')\nplt.ylabel('Velocity v')\nplt.axis('equal')\n\nplt.tight_layout()\nplt.show()\n```\n\n## Advanced Example: Damped Driven Pendulum\n\nLet's apply these methods to a more complex system: a damped driven pendulum. This system can exhibit chaotic behavior under certain conditions, making it a fascinating study in nonlinear dynamics.\n\n::: {#fig-pendulum}\n\n![](/lectures/lecture06/img/pendulum.png){fig-align=\"center\" width=\"300px\"}\n\nThe damped driven pendulum. External driving forces and damping (friction) create a system that can exhibit complex behaviors from regular oscillations to chaos.\n:::\n\n\n### Physical Context\n\nThe damped driven pendulum represents many real physical systems\n\n- A physical pendulum with friction and external driving force\n- An RLC circuit with nonlinear components\n- Josephson junctions in superconductivity\n- Certain types of mechanical and electrical oscillators\n\nThe equation of motion is:\n\n\\begin{equation}\n\\frac{d^2\\theta}{dt^2} + b\\frac{d\\theta}{dt} + \\omega_0^2\\sin\\theta = F_0\\cos(\\omega_d t)\n\\end{equation}\n\nwhere\n\n- $\\theta$ is the angle from vertical (radians)\n- $b$ is the damping coefficient (represents friction or resistance)\n- $\\omega_0 = \\sqrt{g/L}$ is the natural frequency (where $g$ is gravity and $L$ is pendulum length)\n- $F_0$ is the driving amplitude (strength of the external force)\n- $\\omega_d$ is the driving frequency (how rapidly the external force oscillates)\n\nThis equation differs from the harmonic oscillator in three crucial ways:\n\n1. The $\\sin\\theta$ term (rather than just $\\theta$) makes it nonlinear\n2. The damping term $b\\frac{d\\theta}{dt}$ causes energy dissipation\n3. The driving term $F_0\\cos(\\omega_d t)$ adds energy to the system\n\nThese additions make the system much more realistic and rich in behavior. Students could explore this system by varying parameters like the damping coefficient (b), driving amplitude (F0), and driving frequency (omega_d) to observe how the pendulum transitions between regular oscillations, period doubling, and chaos.\n\n```{pyodide}\n#| autorun: false\n\ndef damped_driven_pendulum(t, y, b=0.1, omega0=1.0, F0=0.5, omega_d=0.7):\n    \"\"\"\n    Damped driven pendulum ODE system.\n\n    Parameters:\n    t: time (seconds)\n    y: state vector where y[0] is theta (angle) and y[1] is omega (angular velocity)\n    b: damping coefficient (1/second)\n    omega0: natural frequency (rad/second) = sqrt(g/L)\n    F0: driving amplitude (rad/second²)\n    omega_d: driving frequency (rad/second)\n\n    Returns:\n    [dtheta_dt, domega_dt]: derivatives of state variables\n\n    Physics:\n    This system models a pendulum with:\n    - Gravitational restoration torque: -omega0² * sin(theta)\n    - Damping (friction) torque: -b * omega\n    - External driving torque: F0 * cos(omega_d * t)\n\n    The complete derivation comes from the torque equation:\n    τ = I*α = -mgL*sin(θ) - b'*ω + τ_external\n\n    Dividing by I (moment of inertia) gives this equation.\n    \"\"\"\n    theta, omega = y  # Unpack the state vector\n    dtheta_dt = omega  # Rate of change of angle equals angular velocity\n\n    # Rate of change of angular velocity equals angular acceleration:\n    # (Restoration) + (Damping) + (Driving)\n    domega_dt = -omega0**2 * np.sin(theta) - b*omega + F0 * np.cos(omega_d * t)\n\n    return [dtheta_dt, domega_dt]\n\n# Parameters\nt_span = (0, 150)     # Time span (seconds)\ny0 = [0.1, 0.0]      # Initial conditions: [angle (rad), angular velocity (rad/s)]\nb = 0.1              # Damping coefficient (friction) (1/s)\nomega0 = 1.0         # Natural frequency = sqrt(g/L) (rad/s)\n                     # (equivalent to a pendulum of length L = g/omega0² ≈ 9.81 m if omega0 = 1)\nF0 = 0.1             # Driving amplitude (rad/s²)\nomega_d = 0.7        # Driving frequency (rad/s)\n                     # (driving period = 2π/omega_d ≈ 8.97 s)\n\n# Solve the ODE system using solve_ivp\n# We use RK45 which is well-suited for this type of problem\nsolution = solve_ivp(\n    lambda t, y: damped_driven_pendulum(t, y, b, omega0, F0, omega_d),\n    t_span,\n    y0,\n    method='RK45',\n    t_eval=np.linspace(t_span[0], t_span[1], 1000),  # Points for evaluation\n    rtol=1e-6,  # Relative tolerance - controls accuracy\n    atol=1e-9   # Absolute tolerance - especially important for values near zero\n)\n\n# Plot solution\nplt.figure(figsize=get_size(12, 10))\n\n# Time series\nplt.plot(solution.t, solution.y[0], label='Angle θ')\nplt.plot(solution.t, solution.y[1], label='Angular Velocity ω')\nplt.xlabel('Time (s)')\nplt.ylabel('Value')\nplt.legend()\nplt.tight_layout()\nplt.show()\n```\n\n```{pyodide}\n#| autorun: false\n# Phase space\nplt.figure(figsize=get_size(12, 10))\nplt.plot(solution.y[0] % (2*np.pi), solution.y[1])\nplt.xlabel('Angle θ (mod 2π)')\nplt.ylabel('Angular Velocity ω')\n\n\nplt.tight_layout()\nplt.show()\n```\n\n### Exploring Chaotic Behavior\n\nChaotic motion refers to a deterministic system whose behavior appears random and exhibits extreme sensitivity to initial conditions—the famous \"butterfly effect.\" In the damped driven pendulum, chaos emerges when the system's nonlinear restoring force ($\\sin\\theta$ term) interacts with sufficient driving force. For this system, chaotic behavior typically appears when the driving amplitude $F_0$ exceeds a critical value (approximately 1.0 in our case) while maintaining moderate damping ($b \\approx 0.1$) and a driving frequency $\\omega_d$ that is not too far from the natural frequency $\\omega_0$. In the chaotic regime, the pendulum's trajectory becomes unpredictable over long time scales, despite being governed by deterministic equations. The phase space transforms from closed orbits (regular motion) to a strange attractor with fractal structure, and the system never settles into a periodic oscillation.\n\n::: {.callout-note collapse=true}\n## Lyapunov Exponents: Quantifying Chaos\n\nThe Lyapunov exponent is a powerful mathematical tool for characterizing chaotic systems. It measures the rate at which nearby trajectories in phase space diverge over time. For a system with state vector $\\vec{x}$, if two initial conditions differ by a small displacement $\\delta\\vec{x}_0$, then after time $t$, this separation grows approximately as:\n\n$$|\\delta\\vec{x}(t)| \\approx e^{\\lambda t}|\\delta\\vec{x}_0|$$\n\nwhere $\\lambda$ is the Lyapunov exponent.\n\n- A **positive** Lyapunov exponent ($\\lambda > 0$) indicates chaos: nearby trajectories diverge exponentially, making long-term prediction impossible. The larger the exponent, the more chaotic the system.\n- A **zero** Lyapunov exponent ($\\lambda = 0$) suggests a stable limit cycle or quasiperiodic behavior.\n- A **negative** Lyapunov exponent ($\\lambda < 0$) indicates a stable fixed point, where trajectories converge.\n\nFor our damped driven pendulum, we can numerically estimate the Lyapunov exponent by comparing how two slightly different initial conditions evolve over time. For instance, we might begin with two pendulums at nearly the same angle—perhaps θ₁(0) = 0.1 and θ₂(0) = 0.1001—while keeping their initial angular velocities identical at ω₁(0) = ω₂(0) = 0. As we simulate both systems, we can track the separation between their trajectories in phase space. This separation represents the difference vector δx(t) = [θ₂(t) - θ₁(t), ω₂(t) - ω₁(t)]. Initially, this difference is very small, but in a chaotic system, it grows exponentially with time. By measuring this growth rate, we can compute the Lyapunov exponent as λ ≈ (1/t)ln(|δx(t)|/|δx(0)|). When examining our pendulum system, which has two dimensions (θ and ω), we actually have two Lyapunov exponents forming a spectrum. If either of these exponents is positive, we can conclusively determine that our system exhibits chaotic behavior, indicating fundamental unpredictability despite its deterministic nature.\n:::\n\n```{pyodide}\n#| autorun: false\n\n# Function to solve and plot for different driving amplitudes\ndef solve_for_driving_amplitude(F0):\n    \"\"\"\n    Solves the pendulum equation for a given driving amplitude F0.\n\n    Physics note:\n    - For small F0, the system behaves like a damped oscillator with a periodic response\n    - As F0 increases, the system can undergo period-doubling bifurcations\n    - Above a critical value, the system becomes chaotic\n\n    We skip the first 50 seconds (the \"transient\" behavior) to focus on the\n    long-term behavior (the \"attractor\").\n    \"\"\"\n    solution = solve_ivp(\n        lambda t, y: damped_driven_pendulum(t, y, b=0.1, omega0=1.0, F0=F0, omega_d=0.7),\n        (0, 100),  # Longer time span to observe chaos\n        [0.1, 0.0],\n        method='RK45',\n        t_eval=np.linspace(50, 100, 1000)  # Only plot after transients die out\n    )\n    return solution\n\n# Try different driving amplitudes to observe the transition to chaos\nF0_values = [0.1, 1.0, 1.2]\nplt.figure(figsize=get_size(15, 10))\n\nbehavior_types = [\"Regular\", \"Period-Doubled\", \"Chaotic\"]\n\nfor i, F0 in enumerate(F0_values):\n    solution = solve_for_driving_amplitude(F0)\n\n    # Time series plot\n    plt.subplot(2, 3, i+1)\n    plt.plot(solution.t, solution.y[0])\n    plt.xlabel('Time (s)')\n    plt.ylabel('Angle θ')\n\n    # Phase space plot\n    plt.subplot(2, 3, i+4)\n    plt.plot(solution.y[0] % (2*np.pi), solution.y[1], '.', markersize=1)\n    plt.xlabel('Angle θ (mod 2π)')\n    plt.ylabel('Angular Velocity ω')\n\n    # Add explanatory text for each type of behavior\n    #if i == 0:\n        #plt.text(1, 0, \"Simple attractor\\n(periodic motion)\", fontsize=9)\n    #elif i == 1:\n        #plt.text(1, 0, \"Period doubling\\n(quasi-periodic)\", fontsize=9)\n    #else:\n        #plt.text(1, 0, \"Strange attractor\\n(chaotic motion)\", fontsize=9)\n\nplt.tight_layout()\nplt.show()\n```\n\n## Conclusion\n\nIn this lecture, we've built upon our knowledge of numerical differentiation to solve ordinary differential equations. We've explored:\n\n1. **Implicit Matrix Methods**: Using matrices to solve the entire system at once\n2. **Explicit Integration Methods**: Step-by-step methods like Euler, Euler-Cromer, and Midpoint\n3. **Advanced SciPy Methods**: Leveraging powerful adaptive solvers for complex problems\n\nEach approach has its strengths and is suited to different types of problems. The matrix method is excellent for linear systems, while explicit methods are more versatile for nonlinear problems. SciPy's solvers combine accuracy, stability, and efficiency for practical applications.\n\n### Physical Relevance\n\nThe methods we've learned are essential tools for computational physics because they:\n1. Allow us to study systems with no analytical solutions\n2. Provide insights into nonlinear dynamics and chaos\n3. Enable the modeling of realistic systems with friction, driving forces, and complex interactions\n4. Connect mathematical formulations with observable physical phenomena\n\nAs a second-semester physics student, these numerical tools complement your analytical understanding of mechanics, electromagnetism, and other core physics subjects. When analytical methods reach their limits, these numerical approaches allow you to continue exploring and modeling physical reality.\n\n## Exercises\n\n1. **Damped Harmonic Oscillator**: Modify the harmonic oscillator to include damping term $b\\frac{dx}{dt}$ and solve using both implicit and explicit methods. Compare the rate of energy dissipation between different methods.\n\n2. **Advanced Methods**: Implement the 4th order Runge-Kutta method (RK4) and compare its accuracy with the other methods. For the harmonic oscillator, calculate how the energy error grows with time for each method.\n\n3. **Chaotic Dynamics**: Explore the chaotic behavior of the damped driven pendulum by creating a bifurcation diagram. Plot the Poincaré section (sampling the phase space at intervals of the driving period) as a function of the driving amplitude F₀.\n\n4. **Coupled Systems**: Solve a coupled oscillator system (two masses connected by springs). This system models coupled LC circuits, molecular vibrations, and phonons in solid-state physics.\n\n5. **Physical Application**: Choose a physical system from your current physics courses (e.g., an electric circuit, a mechanical system, or an electromagnetic wave) and model it using the numerical methods learned in this lecture.\n\n6. **Lyapunov Exponents**: For advanced students, calculate the Lyapunov exponent for the damped driven pendulum to quantify the rate of divergence of nearby trajectories, a key measure of chaos.\n\n\n\n::: {.callout-note}\n### Self-Exercise 1: Simple Harmonic Oscillator\nWrite a program to solve the equation of motion for a simple harmonic oscillator. This example demonstrates how to solve a second-order differential equation using scipy's `odeint`.\n\nThe equation of motion is: $\\frac{d^2x}{dt^2} + \\omega^2x = 0$\n\nThis represents an idealized spring-mass system or pendulum with small oscillations.\n\n\n```{pyodide}\n#| exercise: ex_1\n\nimport numpy as np\nfrom scipy.integrate import odeint\nimport matplotlib.pyplot as plt\n\ndef oscillator(state, t, omega):\n    x, v = state\n    return [v, -omega**2 * x]\n\n# Set parameters\nomega = 2.0\nt = np.linspace(0, 10, 1000)\ninitial_state = [1, 0]\n\n# Solve ODE and plot solution\n____\n```\n\n::: {.hint exercise=\"ex_1\"}\nUse `odeint(oscillator, initial_state, t, args=(omega,))` to solve the system. The solution will have two columns: position ([:,0]) and velocity ([:,1]). Create a plot showing position vs time using matplotlib. Remember to label your axes and add a title.\n:::\n\n::: {.solution exercise=\"ex_1\"}\n::: {.callout-note collapse=\"false\"}\n## Solution\n```{pyodide}\nimport numpy as np\nfrom scipy.integrate import odeint\nimport matplotlib.pyplot as plt\n\ndef oscillator(state, t, omega):\n    x, v = state\n    return [v, -omega**2 * x]\n\nomega = 2.0\nt = np.linspace(0, 10, 1000)\ninitial_state = [1, 0]\n\nsolution = odeint(oscillator, initial_state, t, args=(omega,))\n\nplt.figure(figsize=(10, 4))\nplt.plot(t, solution[:, 0])\nplt.xlabel('Time (s)')\nplt.ylabel('Position (m)')\nplt.title('Simple Harmonic Motion')\nplt.grid(True)\nplt.show()\n```\n:::\n:::\n:::\n\n\n\n::: {.callout-note}\n### Self-Exercise 2: Radioactive Decay\nModel the process of radioactive decay, a fundamental concept in nuclear physics. This exercise shows how to solve a first-order differential equation that describes exponential decay.\n\nThe decay equation is: $\\frac{dN}{dt} = -\\lambda N$\n\nWhere $N$ is the number of atoms and $\\lambda$ is the decay constant.\n\n```{pyodide}\n#| exercise: ex_2\n\nimport numpy as np\nfrom scipy.integrate import odeint\nimport matplotlib.pyplot as plt\n\ndef decay(N, t, lambda_):\n    return -lambda_ * N\n\n# Set parameters\nlambda_ = 0.5\nt = np.linspace(0, 10, 100)\nN0 = 1000\n\n# Solve ODE and plot solution\n____\n```\n\n::: {.hint exercise=\"ex_2\"}\nUse `odeint(decay, N0, t, args=(lambda_,))` to solve the equation. The solution will be a 1D array of N values. Plot N vs t to visualize the exponential decay. Consider adding a horizontal line at N0/2 to show the half-life. Use `plt.grid(True)` to make the plot easier to read.\n:::\n\n::: {.solution exercise=\"ex_2\"}\n::: {.callout-note collapse=\"false\"}\n## Solution\n```{pyodide}\nimport numpy as np\nfrom scipy.integrate import odeint\nimport matplotlib.pyplot as plt\n\ndef decay(N, t, lambda_):\n    return -lambda_ * N\n\nlambda_ = 0.5\nt = np.linspace(0, 10, 100)\nN0 = 1000\n\nsolution = odeint(decay, N0, t, args=(lambda_,))\n\nplt.figure(figsize=(8, 5))\nplt.plot(t, solution)\nplt.axhline(y=N0/2, color='r', linestyle='--', label='Half-life')\nplt.xlabel('Time (s)')\nplt.ylabel('Number of Atoms')\nplt.title('Radioactive Decay')\nplt.grid(True)\nplt.legend()\nplt.show()\n```\n:::\n:::\n:::\n\n\n\n## Further Reading\n\n### Numerical Methods\n- \"Numerical Recipes: The Art of Scientific Computing\" by Press, Teukolsky, Vetterling, and Flannery - The standard reference for numerical methods in scientific computing\n- \"Computational Physics\" by Mark Newman - Excellent introduction with Python examples\n- SciPy documentation: [scipy.integrate.solve_ivp](https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.solve_ivp.html)\n\n### Physics Applications\n- \"Computational Physics: Problem Solving with Python\" by Landau, Páez, and Bordeianu - Connects computational methods with physics problems\n- \"An Introduction to Computer Simulation Methods\" by Gould, Tobochnik, and Christian - Comprehensive coverage of simulation techniques in physics\n\n### Dynamical Systems and Chaos\n- \"Differential Equations, Dynamical Systems, and an Introduction to Chaos\" by Hirsch, Smale, and Devaney\n- \"Nonlinear Dynamics And Chaos\" by Steven Strogatz - Accessible introduction to nonlinear dynamics\n- \"Chaos: Making a New Science\" by James Gleick - Popular science book on the history and significance of chaos theory\n\n### Online Resources\n- [Scipy Lecture Notes](https://scipy-lectures.org/) - Tutorials on scientific computing with Python\n- [Matplotlib Gallery](https://matplotlib.org/stable/gallery/index.html) - Examples of scientific visualization\n\n",
    "supporting": [
      "3_solving_ODEs_improved_files"
    ],
    "filters": [],
    "includes": {}
  }
}