{
  "hash": "ddb785637291cc091b5701bb5155159d",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Numerical Differentiation\nformat:\n  live-html:\n    toc: true\n    toc-location: right\npyodide:\n  autorun: false\n  packages:\n    - matplotlib\n    - numpy\n    - scipy\n    - ipywidgets\n---\n\nWhile we did introduce derivatives shortly already when exploring the slicing of arrays, we will now look at the numerical differentiation in more detail. This will require again a little bit of math.\n\n```{pyodide}\n#| edit: false\n#| echo: false\n#| execute: true\n\nimport numpy as np\nimport io\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import curve_fit\nplt.rcParams.update({'font.size': 18})\nfrom ipywidgets import interact, interactive, fixed, interact_manual\nimport ipywidgets as widgets\n\n# default values for plotting\nplt.rcParams.update({'font.size': 12,\n                     'lines.linewidth': 1,\n                     'lines.markersize': 5,\n                     'axes.labelsize': 11,\n                     'xtick.labelsize' : 10,\n                     'ytick.labelsize' : 10,\n                     'xtick.top' : True,\n                     'xtick.direction' : 'in',\n                     'ytick.right' : True,\n                     'ytick.direction' : 'in',})\n\ndef get_size(w,h):\n      return((w/2.54,h/2.54))\n```\n\n## First Order Derivative\n\nOur previous method of finding the derivative was based on the definition of the derivative itself. The derivative of a function $f(x)$ at a point $x$ is defined as the limit of the difference quotient as the interval $\\Delta x$ goes to zero:\n\n$$\nf^{\\prime}(x) = \\lim_{\\Delta x \\rightarrow 0} \\frac{f(x + \\Delta x) - f(x)}{\\Delta x}\n$$\n\nIf we do not take the limit, we can approximate the derivative by:\n\n$$\nf^{\\prime}_{i} \\approx \\frac{f_{i+1} - f_{i}}{\\Delta x}\n$$\n\nHere, we look to the right of the current position $i$ and divide by the interval $\\Delta x$. It is not difficult to see that the resulting local error $\\delta$ at each step is given by:\n\n$$\n\\delta = f_{i+1} - f_{i} - \\Delta x f^{\\prime}(x_i) = \\frac{1}{2} \\Delta x^2 f^{\\prime \\prime}(x_i) + O(\\Delta x^3)\n$$\n\nIt can be seen that the error is proportional to the **square** of the interval $\\Delta x$. This is the reason why the method is called first order accurate. The error is of the order of $\\Delta x^{2}$.\n\nA better expression can be found using the Taylor expansion around the position $x_0$:\n\n$$\nf(x) = f(x_{0}) + (x - x_0) f^{\\prime}(x) + \\frac{(x - x_0)^2}{2!} f^{\\prime\\prime}(x) + \\frac{(x - x_0)^3}{3!} f^{(3)}(x) + \\ldots\n$$\n\nIn discrete notation, this gives:\n\n$$\nf_{i+1} = f_{i} + \\Delta x f_{i}^{\\prime} + \\frac{\\Delta x^2}{2!} f_{i}^{\\prime\\prime} + \\frac{\\Delta x^3}{3!} f_{i}^{(3)} + \\ldots\n$$\n\nThe same can be done to obtain the function value at $i-1$:\n\n$$\nf_{i-1} = f_{i} - \\Delta x f_{i}^{\\prime} + \\frac{\\Delta x^2}{2!} f_{i}^{\\prime\\prime} - \\frac{\\Delta x^3}{3!} f_{i}^{(3)} + \\ldots\n$$\n\nSubtracting these two equations, we get:\n\n$$\nf_{i+1} - f_{i-1} = 2 \\Delta x f_{i}^{\\prime} + O(\\Delta x^3)\n$$\n\nsuch that the second order term in $\\Delta x$ disappears. Neglecting the higher-order terms, we have\n\n$$\nf^{\\prime}_{i} \\approx \\frac{f_{i+1} - f_{i-1}}{2 \\Delta x}\n$$\n\nan thus have a first order derivative which is even more accurate than the one obtained from the definition of the derivative.\n\nWe can continue that type of derivation now to obtain higher order approximation of the first derivative with better accuracy. For that purpose you may calculate now $f_{i\\pm 2}$ and combining that with $f_{i+1}-f_{i-1}$ will lead to\n\n\\begin{equation}\nf_{i}^{\\prime}=\\frac{1}{12 \\Delta x}(f_{i-2}-8f_{i-1}+8f_{i+1}-f_{i+2})\n\\end{equation}\n\nThis can be used to give even better values for the first derivative.\n\nLet`s try out one of the formulas in the following code cell. We will write a function that calculates the derivative of a given function at a given position $x$. The function will take the function $f$ as and argument, which is new to us. We will also introduce a small interval $h=\\Delta x$ which will be used to calculate the derivative. The function will return the derivative of the function at the given position $x$.\n\n```{pyodide}\n#| autorun: false\ndef D(f, x, h=1.e-12, *params):\n    return (f(x+h, *params)-f(x-h, *params))/(2*h)\n```\n\nNote that the definition contains additional parameters `*params` which are passed to the function `f`. This is a general way to pass additional parameters to the function `f` which is used in the definition of the derivative.\n\nWe will try to calculate the derivative of the $\\sin(x)$  function:\n\n```{pyodide}\n#| autorun: false\ndef f(x):\n    return(np.sin(x))\n```\n\nWe can plot this and nicely obtain our cosine function\n\n```{pyodide}\n#| autorun: false\n\nx=np.linspace(0.01,np.pi*4,1000)\n\nplt.plot(x,D(f,x))\n```\n\n### Matrix Version of the First Derivative\n\nIf we supply the above function with an array of positions $x_{i}$ at which we would like to calculate the derivative, we obtain an array of derivative values. We can also write this procedure in a different way, which will be helpful for solving differential equations later.\n\nIf we consider the above finite difference formulas for a set of positions $x_{i}$, we can represent the first derivative at these positions by a matrix operation as well:\n\n$$\nf^{\\prime} = \\frac{1}{\\Delta x}\n\\begin{bmatrix}\n-1 & 1  & 0 & 0 & 0 & 0\\\\\n 0 & -1 & 1 & 0 & 0 & 0\\\\\n 0 & 0  & -1 & 1 & 0 & 0\\\\\n 0 & 0  & 0  & -1 & 1 & 0\\\\\n 0 & 0  & 0  &  0 & -1 & 1\\\\\n 0 & 0  & 0  &  0 &  0 & -1\\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nf_{1}\\\\\nf_{2}\\\\\nf_{3}\\\\\nf_{4}\\\\\nf_{5}\\\\\nf_{6}\\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\frac{f_{2} - f_{1}}{\\Delta x}\\\\\n\\frac{f_{3} - f_{2}}{\\Delta x}\\\\\n\\frac{f_{4} - f_{3}}{\\Delta x}\\\\\n\\frac{f_{5} - f_{4}}{\\Delta x}\\\\\n\\frac{f_{6} - f_{5}}{\\Delta x}\\\\\n\\frac{0 - f_{6}}{\\Delta x}\\\\\n\\end{bmatrix}\n$$\n\nNote that here we took the derivative only to the right side! Each row of the matrix, when multiplied by the vector containing the function values, gives the derivative of the function $f$ at the corresponding position $x_{i}$. The resulting vector represents the derivative in a certain position region.\n\nWe will demonstrate how to generate such a matrix with the `SciPy` module below.\n\n## Second order derivative\n\nWhile we did before calculate the first derivative, we can also calculate the second derivative of a function. In the previous calculations we evaluated $f_{i+1} - f_{i-1}$. We can now also use the sum of both to arrive at\n\n\\begin{equation}\nf_{i}^{\\prime\\prime}\\approx \\frac{f_{i-1}-2f_{i}+f_{i+1}}{\\Delta x^2}\n\\end{equation}\n\nwhich gives the basic equation for calculating the second order derivative and the next order may be obtained from\n\n\\begin{equation}\nf_{i}^{\\prime\\prime}\\approx \\frac{1}{12 \\Delta x^{2}}(-f_{i-2}+16f_{i-1}-30 f_{i}+16f_{i+1}-f_{i+2})\n\\end{equation}\n\nwhich is again better than our previous formula, yet needs more function values to be calculated.\n\n\n## SciPy Module\n\nOf course, we are not the first to define some functions for calculating the derivative of functions numerically. This is already implemented in different modules. One module is the above mentioned `SciPy` module.\n\nThe `SciPy` module provides the method `derivative`, which we can call with\n\n~~~\nderivative(f,x,dx=1.0,n=1):\n~~~\n\nThis will calculate the n$th$ derivative of the function $f$ at the position $x$ with a intervall $dx=1.0$ (default value).\n\n```{pyodide}\n#| autorun: false\n## the derivative method is hidden in the `misc` sub-module of `SciPy`.\nfrom scipy.misc import derivative\n```\n\nWe also have the option to define the order parameter, which is not the order of the derivative but rather the number of points used to calculate the derivative according to our scheme earlier.\n\n```{pyodide}\n#| autorun: false\nderivative(np.sin,np.pi,dx=0.000001,n=2,order=5)\n```\n\n### Matrix Version\n\nThe `SciPy` module allows us to construct matrices as mentioned above. We will need the `diags` method from the `SciPy` module for that purpose.\n\n```{pyodide}\n#| autorun: false\nfrom scipy.sparse import diags\n```\n\nLet's assume we want to calculate the derivative of the `sin` function at certain positions.\n\n```{pyodide}\n#| autorun: false\nN = 100\nx = np.linspace(-5, 5, N)\ny = np.sin(x)\n```\n\nThe `diags` function uses a set of numbers that should be distributed along the diagonals of the matrix. If you supply a list like in the example below, the numbers are distributed using the offsets as defined in the second list. The `shape` keyword defines the shape of the matrix. Try the example in the next cell with the `.todense()` suffix. This converts the otherwise unreadable sparse output to a readable matrix form.\n\n```{pyodide}\n#| autorun: false\nm = diags([-1, 1], [0, 1], shape=(10, 10)).todense()\nprint(m)\n```\n\nTo comply with our previous definition of $N=100$ data points and the interval $\\Delta x$, we define:\n\n```{pyodide}\n#| autorun: false\ndx = x[1] - x[0]\nm = diags([-1, 1], [0, 1], shape=(N, N)) / dx\n```\n\nThe derivative is then simply a matrix-vector multiplication, which is done either by `np.dot(m,y)` or just by the `@` operator.\n\n```{pyodide}\n#| autorun: false\ndiff = m @ y\n```\n\nLet's plot the original function and its numerical derivative.\n\n```{pyodide}\n#| autorun: false\nplt.figure(figsize=get_size(14,8))\nplt.plot(x[:-1], diff[:-1], label=r'$f^{\\prime}(x)$')\nplt.plot(x, y, label=r'$f(x)=\\sin(x)$')\nplt.xlabel('x')\nplt.ylabel(r'$f$, $f^{\\prime}$')\nplt.legend(loc='upper left', bbox_to_anchor=(1, 1))\nplt.ylim(-1, 1)\nplt.tight_layout()\nplt.show()\n```\n\nCheck for yourself that the following line of code will calculate the second derivative.\n\n```{pyodide}\n#| autorun: false\nm = diags([1, -2, 1], [-1, 0, 1], shape=(N, N)) / dx**2\nsecond_diff = m @ y\n```\n\nLet's plot the original function and its second numerical derivative.\n\n```{pyodide}\n#| autorun: false\nplt.figure(figsize=get_size(14,8))\nplt.plot(x[1:-1], second_diff[1:-1], label=r'$f^{\\prime\\prime}(x)$')\nplt.plot(x, y, label=r'$f(x)=\\sin(x)$')\nplt.xlabel('x')\nplt.ylabel(r'$f$, $f^{\\prime\\prime}$')\nplt.legend(loc='upper left', bbox_to_anchor=(1, 1))\nplt.ylim(-1, 1)\nplt.tight_layout()\nplt.show()\n```\n\nThis demonstrates how to use the `SciPy` module to construct matrices for numerical differentiation and how to apply these matrices to compute first and second derivatives.\n\n\n::: {.callout-note collapse=true}\n## Applications of the Matrix Method\nThe matrix method for computing derivatives is particularly useful in several contexts, especially in numerical analysis and computational mathematics. Here are some key applications:\n\n1. **Solving Differential Equations**:\n   - **Ordinary Differential Equations (ODEs)**: The matrix method can be used to discretize ODEs, transforming them into a system of linear equations that can be solved using linear algebra techniques.\n   - **Partial Differential Equations (PDEs)**: Similarly, PDEs can be discretized using finite difference methods, where derivatives are approximated by matrix operations. This is essential in fields like fluid dynamics, heat transfer, and electromagnetics.\n\n2. **Numerical Differentiation**:\n   - The matrix method provides a systematic way to approximate derivatives of functions given discrete data points. This is useful in data analysis, signal processing, and any application where you need to estimate the rate of change from sampled data.\n\n3. **Stability and Accuracy Analysis**:\n   - By representing derivative operations as matrices, it becomes easier to analyze the stability and accuracy of numerical schemes. This is crucial for ensuring that numerical solutions to differential equations are reliable.\n\n4. **Optimization Problems**:\n   - In optimization, especially in gradient-based methods, the matrix method can be used to compute gradients and Hessians efficiently. This is important in machine learning, operations research, and various engineering disciplines.\n\n5. **Finite Element Analysis (FEA)**:\n   - In FEA, the matrix method is used to approximate derivatives and integrals over complex geometries. This is widely used in structural engineering, biomechanics, and materials science.\n\n6. **Control Theory**:\n   - In control theory, especially in the design and analysis of control systems, the matrix method can be used to model and simulate the behavior of dynamic systems.\n\n:::\n\n",
    "supporting": [
      "old_differentiation_files"
    ],
    "filters": [],
    "includes": {}
  }
}