{
  "hash": "88f5029ba3981f5a355d9418bc5cb33f",
  "result": {
    "engine": "jupyter",
    "markdown": "---\nformat:\n  live-html:\n    toc: true\n    toc-location: right\npyodide:\n  autorun: false\n  packages:\n    - matplotlib\n    - numpy\n    - scipy\n---\n\n## Numerical Differentiation for Physics\n\nDerivatives form the mathematical backbone of physics. Whether we're calculating velocity from position, acceleration from velocity, or electric field from potential, we're computing derivatives. While calculus provides us with analytical tools to compute derivatives, many real-world physics problems involve functions that are either too complex for analytical solutions or are only known at discrete points (experimental data). This is where numerical differentiation becomes essential for physicists.\n\n```{pyodide}\n#| edit: false\n#| echo: false\n#| execute: true\n\nimport numpy as np\nimport io\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import curve_fit\nplt.rcParams.update({'font.size': 18})\n\n# default values for plotting\nplt.rcParams.update({'font.size': 10,\n                     'lines.linewidth': 1,\n                     'lines.markersize': 5,\n                     'axes.labelsize': 10,\n                     'xtick.labelsize' : 10,\n                     'ytick.labelsize' : 10,\n                     'xtick.top' : True,\n                     'xtick.direction' : 'in',\n                     'ytick.right' : True,\n                     'ytick.direction' : 'in',})\n\ndef get_size(w,h):\n      return((w/2.54,h/2.54))\n```\n\n### The Calculus Foundations\n\nBefore diving into numerical methods, let's revisit the calculus definition of a derivative. The derivative of a function $f(x)$ at a point $x$ is defined as the limit of the difference quotient as the interval $\\Delta x$ approaches zero:\n\n$$\nf^{\\prime}(x) = \\lim_{\\Delta x \\rightarrow 0} \\frac{f(x + \\Delta x) - f(x)}{\\Delta x}\n$$\n\nThis definition captures the instantaneous rate of change of $f$ with respect to $x$. In physics, derivatives represent essential physical quantities:\n\n- The derivative of position with respect to time is velocity\n- The derivative of velocity with respect to time is acceleration\n- The derivative of potential energy with respect to position gives force\n\nHowever, in computational physics, we cannot take the limit to zero as computers work with discrete values. Instead, we approximate the derivative using finite differences. This is also possible for higher order derivatives, which can be approximated using more complex finite difference formulas such as\n\n$$\nf^{(n)}(x)=\\lim _{\\Delta x  \\rightarrow 0} \\frac{1}{\\Delta x ^n} \\sum_{k=0}^n(-1)^{k+n}\\binom{n}{k} f(x+k \\Delta x )\n$$\n\n\n\n### Finite Difference Approximations\n\nNumerical differentiation methods primarily rely on finite difference approximations derived from Taylor series expansions. Let's explore these systematically.\n\n#### Forward Difference\n\nThe simplest approximation comes directly from the definition, where we look at the change in function value as we move forward from the current point:\n\n$$\nf^{\\prime}_{i} \\approx \\frac{f_{i+1} - f_{i}}{\\Delta x}\n$$\n\nThis is called the *forward difference* method. To understand its accuracy, we can analyze the error using Taylor expansion. The resulting local error $\\delta$ at each calculation is:\n\n$$\n\\delta = f_{i+1} - f_{i} - \\Delta x f^{\\prime}(x_i) = \\frac{1}{2} \\Delta x^2 f^{\\prime \\prime}(x_i) + O(\\Delta x^3)\n$$\n\nWe observe that while the local truncation error is proportional to $\\Delta x^2$, the accumulated global error is proportional to $\\Delta x$, making this a first-order accurate method. This means that halving the step size will approximately halve the error in our final derivative approximation.\n\n::: {.callout-note}\n## Local vs. Global Error\n**Local truncation error** refers to the error introduced in a single step of the numerical method due to truncating the Taylor series. For the forward difference method, this error is $O(\\Delta x^2)$.\n\n**Global accumulated error** is the total error that accumulates as we apply the method repeatedly across the domain. For the forward difference method, this accumulated error is $O(\\Delta x)$. Global error is generally one order less accurate than the local error due to error propagation through multiple steps.\n:::\n\n#### Central Difference\n\nWe can derive a more accurate approximation by using function values on both sides of the point of interest. Using Taylor expansions for $f(x+\\Delta x)$ and $f(x-\\Delta x)$:\n\n$$\nf_{i+1} = f_{i} + \\Delta x f_{i}^{\\prime} + \\frac{\\Delta x^2}{2!} f_{i}^{\\prime\\prime} + \\frac{\\Delta x^3}{3!} f_{i}^{(3)} + \\ldots\n$$\n\n$$\nf_{i-1} = f_{i} - \\Delta x f_{i}^{\\prime} + \\frac{\\Delta x^2}{2!} f_{i}^{\\prime\\prime} - \\frac{\\Delta x^3}{3!} f_{i}^{(3)} + \\ldots\n$$\n\nSubtracting these equations cancels out the even-powered terms in $\\Delta x$:\n\n$$\nf_{i+1} - f_{i-1} = 2 \\Delta x f_{i}^{\\prime} + O(\\Delta x^3)\n$$\n\nSolving for $f^{\\prime}_{i}$:\n\n$$\nf^{\\prime}_{i} \\approx \\frac{f_{i+1} - f_{i-1}}{2 \\Delta x}\n$$\n\nThis *central difference* formula has an error proportional to $\\Delta x^2$, making it second-order accurateâ€”significantly more precise than the forward difference method.\n\n#### Higher-Order Approximations\n\nWe can extend this approach to derive higher-order approximations by including more points in our calculation. A common fourth-order accurate formula for the first derivative is:\n\n$$\nf_{i}^{\\prime}=\\frac{1}{12 \\Delta x}(-f_{i-2}+8f_{i-1}-8f_{i+1}+f_{i+2})\n$$\n\nThis formula provides even better accuracy but requires function values at four points.\n\n#### Comparison of Methods\n\nThe following table summarizes the key finite difference methods for first derivatives:\n\n| Method | Formula | Order of Accuracy | Points Required |\n|--------|---------|-------------------|----------------|\n| Forward Difference | $\\frac{f_{i+1} - f_{i}}{\\Delta x}$ | $O(\\Delta x)$ | 2 |\n| Backward Difference | $\\frac{f_{i} - f_{i-1}}{\\Delta x}$ | $O(\\Delta x)$ | 2 |\n| Central Difference | $\\frac{f_{i+1} - f_{i-1}}{2\\Delta x}$ | $O(\\Delta x^2)$ | 3 |\n| Fourth-Order Central | $\\frac{-f_{i+2}+8f_{i+1}-8f_{i-1}+f_{i-2}}{12\\Delta x}$ | $O(\\Delta x^4)$ | 5 |\n\nHigher-order methods generally provide more accurate results but require more computational resources and handle boundaries less efficiently.\n\n### Implementation in Python\n\nLet's implement these numerical differentiation methods in Python, starting with a central difference function:\n\n```{pyodide}\n#| autorun: false\ndef central_difference(f, x, h=1.e-5, *params):\n    \"\"\"Compute the first derivative using central difference\"\"\"\n    return (f(x+h, *params)-f(x-h, *params))/(2*h)\n\ndef fourth_order_central(f, x, h=1.e-3, *params):\n    \"\"\"Compute the first derivative using fourth-order central difference\"\"\"\n    return (-f(x+2*h, *params) + 8*f(x+h, *params) - 8*f(x-h, *params) + f(x-2*h, *params))/(12*h)\n```\n\nWe can test these functions with $\\sin(x)$, whose derivative is $\\cos(x)$:\n\n```{pyodide}\n#| autorun: false\ndef f(x):\n    return np.sin(x)\n\ndef analytical_derivative(x):\n    return np.cos(x)\n\nx_values = np.linspace(0, 2*np.pi, 100)\n\n# Calculate derivatives using different methods\nh_value = 0.01\nforward_diff = [(f(x+h_value) - f(x))/h_value for x in x_values]\ncentral_diff = [central_difference(f, x, h_value) for x in x_values]\nfourth_order = [fourth_order_central(f, x, h_value) for x in x_values]\nanalytical = analytical_derivative(x_values)\n\n# Plotting\nplt.figure(figsize=get_size(12,8))\nplt.plot(x_values, analytical, 'k-', label=r'$\\cos(x)$')\nplt.plot(x_values, forward_diff, 'r--', label='forward difference')\nplt.plot(x_values, central_diff, 'g-.', label='central difference')\nplt.plot(x_values, fourth_order, 'b:', label='4th-order central')\nplt.xlabel('x')\nplt.ylabel(r'derivative of $\\sin(x)$')\nplt.legend()\nplt.show()\n```\n\n#### Error Analysis\n\nLet's examine how the error in our numerical derivative varies with the step size $\\Delta x$:\n\n```{pyodide}\n#| autorun: false\ndelta_x_values = np.logspace(-10, 0, 20)  # Step sizes from 10^-10 to 10^0\nx0 = np.pi/4  # Test point\n\n# Calculate errors for different methods\nforward_errors = [abs((f(x0+dx) - f(x0))/dx - analytical_derivative(x0)) for dx in delta_x_values]\ncentral_errors = [abs((f(x0+dx) - f(x0-dx))/(2*dx) - analytical_derivative(x0)) for dx in delta_x_values]\nfourth_order_errors = [abs((-f(x0+2*dx) + 8*f(x0+dx) - 8*f(x0-dx) + f(x0-2*dx))/(12*dx) - analytical_derivative(x0)) for dx in delta_x_values]\n\n# Plotting errors\nplt.figure(figsize=get_size(12,8))\nplt.loglog(delta_x_values, forward_errors, 'ro-', label='Forward difference')\nplt.loglog(delta_x_values, central_errors, 'go-', label='Central difference')\nplt.loglog(delta_x_values, fourth_order_errors, 'bo-', label='4th-order central')\nplt.loglog(delta_x_values, delta_x_values, 'k--', label=r'$O(\\Delta x)$')\nplt.loglog(delta_x_values, [dx**2 for dx in delta_x_values], 'k-.', label=r'$O(\\Delta x^2)$')\nplt.loglog(delta_x_values, [dx**4 for dx in delta_x_values], 'k:', label=r'$O(\\Delta x^4)$')\nplt.xlabel(r'step size ($\\Delta x$)')\nplt.ylabel('absolute error')\nplt.legend()\n\nplt.show()\n```\n\nThis visualization demonstrates how error behaves with step size for different methods. For very small step sizes, roundoff errors become significant (observe the upturn in error for tiny $\\Delta x$ values), while for larger steps, truncation error dominates.\n\n### Matrix Representation of Derivatives\n\nAn elegant approach to numerical differentiation involves representing the differentiation operation as a matrix multiplication. This representation is particularly valuable when solving differential equations numerically.\n\n### First Derivative Matrix\n\nFor a uniformly spaced grid of points $x_i$, we can represent the first derivative operation as a matrix:\n\n$$\nf^{\\prime} = \\frac{1}{\\Delta x}\n\\begin{bmatrix}\n-1 & 1  & 0 & 0 & \\cdots & 0\\\\\n0 & -1 & 1 & 0 & \\cdots & 0\\\\\n0 & 0  & -1 & 1 & \\cdots & 0\\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\ddots & \\vdots\\\\\n0 & 0  & 0  & \\cdots & -1 & 1\\\\\n0 & 0  & 0  & \\cdots &  0 & -1\\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nf_{1}\\\\\nf_{2}\\\\\nf_{3}\\\\\n\\vdots\\\\\nf_{N-1}\\\\\nf_{N}\\\\\n\\end{bmatrix}\n$$\n\nThis matrix implements the forward difference scheme. For a central difference scheme, the matrix would have entries on both sides of the diagonal.\n\n#### Second Derivative Matrix\n\nSimilarly, the second derivative can be represented as a tridiagonal matrix:\n\n$$\nf^{\\prime\\prime} = \\frac{1}{\\Delta x^2}\n\\begin{bmatrix}\n1 & -2 & 1 & 0 & \\cdots & 0\\\\\n0 & 1 & -2 & 1 & \\cdots & 0\\\\\n0 & 0 & 1 & -2 & \\cdots & 0\\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\ddots & \\vdots\\\\\n0 & 0 & 0 & \\cdots & 1 & -2 & 1\\\\\n0 & 0 & 0 & \\cdots & 0 & 1 & -2\\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nf_{1}\\\\\nf_{2}\\\\\nf_{3}\\\\\n\\vdots\\\\\nf_{N-1}\\\\\nf_{N}\\\\\n\\end{bmatrix}\n$$\n\nThe boundary conditions affect the structure of these matrices, especially the first and last rows.\n\n#### Implementation with SciPy\n\nSciPy provides tools to efficiently construct and work with these differentiation matrices:\n\n```{pyodide}\n#| autorun: false\nfrom scipy.sparse import diags\n\n# Define a grid and a function to differentiate\nN = 100\nx = np.linspace(-5, 5, N)\ndx = x[1] - x[0]\ny = np.sin(x)\n\n# First derivative matrix (forward difference)\nD1_forward = diags([-1, 1], [0, 1], shape=(N, N)) / dx\n\n# First derivative matrix (central difference)\n# Corrected central difference implementation\nD1_central = diags([-1, 0, 1], [-1, 0, 1], shape=(N, N))\n# The central difference formula is (f(x+h) - f(x-h))/(2h)\n# So we need -1 at offset -1, 0 at offset 0, and 1 at offset 1\nD1_central.setdiag(0, 0)  # Center diagonal should be 0 for central difference\nD1_central = D1_central / (2 * dx)\n\n# Second derivative matrix\nD2 = diags([1, -2, 1], [-1, 0, 1], shape=(N, N)) / dx**2\n\n# Compute derivatives\ndy_forward = D1_forward @ y\ndy_central = D1_central @ y\nd2y = D2 @ y\n\n# Plot the results\nplt.figure(figsize=get_size(16,12))\n\nplt.subplot(3, 1, 1)\nplt.plot(x, y, 'k-', label=r'$f(x) = \\sin(x)$')\nplt.legend()\n\n\nplt.subplot(3, 1, 2)\nplt.plot(x[:-1], dy_forward[:-1], 'r--', label='forward difference')\nplt.plot(x, dy_central, 'g-', label='central difference')\nplt.plot(x, np.cos(x), 'k:', label=r'$\\cos(x)$')\nplt.ylim(-1.05,1.05)\nplt.legend()\n\nplt.subplot(3, 1, 3)\nplt.plot(x[1:-1], d2y[1:-1], 'b-', label='num. 2nd derivative')\nplt.plot(x, -np.sin(x), 'k:', label=r'$-\\sin(x)$')\nplt.legend()\n\n\nplt.tight_layout()\nplt.show()\n```\n\n### Boundary Conditions\n\nA critical consideration in numerical differentiation is how to handle the boundaries of the domain. Different approaches include:\n\n1. **One-sided differences**: Using forward differences at the left boundary and backward differences at the right boundary.\n\n2. **Extrapolation**: Extending the domain by extrapolating function values beyond the boundaries.\n\n3. **Periodic boundaries**: For periodic functions, using values from the opposite end of the domain.\n\n4. **Ghost points**: Introducing additional points outside the domain whose values are determined by the boundary conditions.\n\nThe choice of boundary treatment depends on the physical problem and can significantly impact the accuracy of the solution.\n\n### Applications in Physics\n\nNumerical differentiation is foundational to computational physics. Let's explore some specific applications:\n\n#### 1. Solving Differential Equations\n\nMany physics problems are formulated as differential equations. For example, the one-dimensional time-dependent SchrÃ¶dinger equation:\n\n$$\ni\\hbar\\frac{\\partial}{\\partial t}\\Psi(x,t) = -\\frac{\\hbar^2}{2m}\\frac{\\partial^2}{\\partial x^2}\\Psi(x,t) + V(x)\\Psi(x,t)\n$$\n\nNumerical differentiation allows us to approximate the spatial derivatives, reducing this to a system of ordinary differential equations in time.\n\n#### 2. Analysis of Experimental Data\n\nWhen working with experimental measurements, we often need to calculate derivatives from discrete data points. For instance, determining the velocity and acceleration of an object from position measurements.\n\n```{pyodide}\n#| autorun: false\n# Simulated noisy position data (as might come from an experiment)\ntime = np.linspace(0, 10, 100)\nposition = 5*time**2 + np.random.normal(0, 5, len(time))  # x = 5tÂ² + noise\n\n# Calculate velocity using central differences\ndt = time[1] - time[0]\nvelocity = np.zeros_like(position)\nfor i in range(1, len(time)-1):\n    velocity[i] = (position[i+1] - position[i-1]) / (2*dt)\n\n# Theoretical velocity: v = 10t\ntheoretical_velocity = 10 * time\n\n# Plot\nplt.figure(figsize=get_size(16,12))\n\nplt.subplot(2, 1, 1)\nplt.plot(time, position, 'ko',alpha=0.2, label='experiment')\nplt.plot(time, 5*time**2, 'r-', label=r'$x = 5t^2$')\nplt.xlabel('time [s]')\nplt.ylabel('position [m]')\nplt.legend()\n\nplt.subplot(2, 1, 2)\nplt.plot(time[1:-1], velocity[1:-1], 'bo', label='calculated velocity')\nplt.plot(time, theoretical_velocity, 'r-', label=r'theoretical velocity: $v = 10t$')\nplt.xlabel('time [s]')\nplt.ylabel('velocity [m/s]')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n```\n\nNotice how noise in the position measurements gets amplified in the velocity calculations. This highlights a key challenge in numerical differentiation: sensitivity to noise.\n\n#### 3. Electric Field Calculation\n\nIn electrostatics, the electric field $\\vec{E}$ is related to the electric potential $\\phi$ by $\\vec{E} = -\\nabla \\phi$. Numerical differentiation allows us to calculate the electric field from a known potential distribution.\n\n```{pyodide}\n#| autorun: false\n# Create a 2D grid\nx = np.linspace(-3, 3, 400)\ny = np.linspace(-3, 3, 400)\nX, Y = np.meshgrid(x, y)\n\n# Electric potential due to a point charge at origin\npotential = 1 / np.sqrt(X**2 + Y**2 + 0.001)  # Adding 0.01 to avoid division by zero\n\n# Calculate electric field components\ndx = x[1] - x[0]\ndy = y[1] - y[0]\nEx = np.zeros_like(potential)\nEy = np.zeros_like(potential)\n\n# Use central differences for interior points\nfor i in range(1, len(x)-1):\n    for j in range(1, len(y)-1):\n        Ex[j, i] = -(potential[j, i+1] - potential[j, i-1]) / (2*dx)\n        Ey[j, i] = -(potential[j+1, i] - potential[j-1, i]) / (2*dy)\n\n# Plot potential and electric field\nplt.figure(figsize=get_size(16,6.5))\n\nplt.subplot(1, 2, 1)\ncontour = plt.contourf(X, Y, potential, 20, cmap='viridis')\nplt.colorbar(contour, label='electric potential')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.xlim(-2,2)\nplt.ylim(-2,2)\n\n\nplt.subplot(1, 2, 2)\n# Skip some points for clearer visualization\nskip = 5\nplt.streamplot(X[::skip, ::skip], Y[::skip, ::skip],\n               Ex[::skip, ::skip], Ey[::skip, ::skip],\n               color='w', density=1.5)\nplt.contourf(X, Y, np.sqrt(Ex**2 + Ey**2), 20, cmap='plasma', alpha=0.5)\nplt.colorbar(label='electric field magnitude')\n\nplt.xlabel('x')\nplt.ylabel('y')\nplt.xlim(-2,2)\nplt.ylim(-2,2)\n\n\nplt.tight_layout()\nplt.show()\n```\n\n### Practical Considerations and Challenges\n\n#### 1. Step Size Selection\n\nChoosing an appropriate step size is crucial for numerical differentiation. If $\\Delta x$ is too large, the truncation error becomes significant. If $\\Delta x$ is too small, roundoff errors dominate. A general approach is to use:\n\n$$ \\Delta x \\approx \\sqrt{\\epsilon_\\text{machine}} \\times x $$\n\nwhere $\\epsilon_\\text{machine}$ is the machine epsilon (approximately $10^{-16}$ for double precision).\n\n#### 2. Dealing with Noise\n\nNumerical differentiation amplifies noise in the data. Several techniques can help:\n\n- **Smoothing**: Apply a filter to the data before differentiation.\n- **Regularization**: Use methods that inherently provide some smoothing.\n- **Savitzky-Golay filters**: Combine local polynomial fitting with differentiation.\n\n#### 3. Conservation Properties\n\nIn physical simulations, preserving conservation laws (energy, momentum, etc.) is often crucial. Some numerical differentiation schemes conserve these properties better than others.\n\n### Using SciPy for Numerical Differentiation\n\nThe SciPy library provides convenient functions for numerical differentiation:\n\n```{pyodide}\n#| autorun: false\nfrom scipy.misc import derivative\n\n# Calculate the derivative of sin(x) at x = Ï€/4\nx0 = np.pi/4\n\n# First derivative with different accuracies\nfirst_deriv = derivative(np.sin, x0, dx=1e-6, n=1, order=3)\nprint(f\"First derivative of sin(x) at x = Ï€/4: {first_deriv}\")\nprint(f\"Actual value (cos(Ï€/4)): {np.cos(x0)}\")\n\n# Second derivative\nsecond_deriv = derivative(np.sin, x0, dx=1e-6, n=2, order=5)\nprint(f\"Second derivative of sin(x) at x = Ï€/4: {second_deriv}\")\nprint(f\"Actual value (-sin(Ï€/4)): {-np.sin(x0)}\")\n```\n\nThe `order` parameter controls the accuracy of the approximation by using more points in the calculation.\n\n### Conclusion\n\nNumerical differentiation is a fundamental technique in computational physics, bridging the gap between theoretical models and practical computations. By understanding the principles, methods, and challenges of numerical differentiation, physicists can effectively analyze data, solve differential equations, and simulate physical systems.\n\nThe methods we've exploredâ€”from simple finite differences to matrix representationsâ€”provide a comprehensive toolkit for tackling a wide range of physics problems. As you apply these techniques, remember that the choice of method should be guided by the specific requirements of your problem: accuracy needs, computational constraints, and the nature of your data.\n\n### What to try yourself\n\n1. Implement and compare the accuracy of different numerical differentiation schemes for the function $f(x) = e^{-x^2}$.\n\n2. Investigate how noise in the input data affects the accuracy of numerical derivatives and explore techniques to mitigate this effect.\n\n3. Calculate the electric field around two point charges using numerical differentiation of the electric potential.\n\n5. Analyze experimental data from a falling object to determine its acceleration, and compare with the expected value of gravitational acceleration.\n\n::: {.callout-note}\n### Further Reading\n\n- Numerical Recipes: The Art of Scientific Computing by Press, Teukolsky, Vetterling, and Flannery\n- Numerical Methods for Physics by Alejandro Garcia\n- Computational Physics by Mark Newman\n- Applied Numerical Analysis by Curtis F. Gerald and Patrick O. Wheatley\n:::\n\n",
    "supporting": [
      "1-differentiation_files"
    ],
    "filters": [],
    "includes": {}
  }
}