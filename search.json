[
  {
    "objectID": "lectures/lecture01/02-lecture01.html",
    "href": "lectures/lecture01/02-lecture01.html",
    "title": "Variables & Numbers",
    "section": "",
    "text": "Variable names in Python can include alphanumerical characters a-z, A-Z, 0-9, and the special character _. Normal variable names must start with a letter or an underscore. By convention, variable names typically start with a lower-case letter, while Class names start with a capital letter and internal variables start with an underscore.\n\n\n\n\n\n\nReserved Keywords\n\n\n\nPython has keywords that cannot be used as variable names. The most common ones you’ll encounter in physics programming are:\nif, else, for, while, return, and, or, lambda\nNote that lambda is particularly relevant as it could naturally appear in physics code, but since it’s reserved for anonymous functions in Python, it cannot be used as a variable name.\n\n\n\n\n\nThe assignment operator in Python is =. Python is a dynamically typed language, so we do not need to specify the type of a variable when we create one.\nAssigning a value to a new variable creates the variable:\n\n\n\n\n\n\nAlthough not explicitly specified, a variable does have a type associated with it (e.g., integer, float, string). The type is derived from the value that was assigned to it. To determine the type of a variable, we can use the type function.\n\n\n\n\n\n\nIf we assign a new value to a variable, its type can change.\n\n\n\n\n\n\n\n\n\n\n\n\nIf we try to use a variable that has not yet been defined, we get a NameError error.",
    "crumbs": [
      "Python Basics",
      "Lecture 1",
      "Variables & Numbers"
    ]
  },
  {
    "objectID": "lectures/lecture01/02-lecture01.html#variables-in-python",
    "href": "lectures/lecture01/02-lecture01.html#variables-in-python",
    "title": "Variables & Numbers",
    "section": "",
    "text": "Variable names in Python can include alphanumerical characters a-z, A-Z, 0-9, and the special character _. Normal variable names must start with a letter or an underscore. By convention, variable names typically start with a lower-case letter, while Class names start with a capital letter and internal variables start with an underscore.\n\n\n\n\n\n\nReserved Keywords\n\n\n\nPython has keywords that cannot be used as variable names. The most common ones you’ll encounter in physics programming are:\nif, else, for, while, return, and, or, lambda\nNote that lambda is particularly relevant as it could naturally appear in physics code, but since it’s reserved for anonymous functions in Python, it cannot be used as a variable name.\n\n\n\n\n\nThe assignment operator in Python is =. Python is a dynamically typed language, so we do not need to specify the type of a variable when we create one.\nAssigning a value to a new variable creates the variable:\n\n\n\n\n\n\nAlthough not explicitly specified, a variable does have a type associated with it (e.g., integer, float, string). The type is derived from the value that was assigned to it. To determine the type of a variable, we can use the type function.\n\n\n\n\n\n\nIf we assign a new value to a variable, its type can change.\n\n\n\n\n\n\n\n\n\n\n\n\nIf we try to use a variable that has not yet been defined, we get a NameError error.",
    "crumbs": [
      "Python Basics",
      "Lecture 1",
      "Variables & Numbers"
    ]
  },
  {
    "objectID": "lectures/lecture01/02-lecture01.html#number-types",
    "href": "lectures/lecture01/02-lecture01.html#number-types",
    "title": "Variables & Numbers",
    "section": "Number Types",
    "text": "Number Types\nPython supports various number types, including integers, floating-point numbers, and complex numbers. These are some of the basic building blocks of doing arithmetic in any programming language. We will discuss each of these types in more detail.\n\nComparison of Number Types\n\n\n\n\n\n\n\n\n\n\n\nType\nExample\nDescription\nLimits\nUse Cases\n\n\n\n\nint\n42\nWhole numbers\nUnlimited precision (bounded by available memory)\nCounting, indexing\n\n\nfloat\n3.14159\nDecimal numbers\nTypically ±1.8e308 with 15-17 digits of precision (64-bit)\nScientific calculations, prices\n\n\ncomplex\n2 + 3j\nNumbers with real and imaginary parts\nSame as float for both real and imaginary parts\nSignal processing, electrical engineering\n\n\nbool\nTrue / False\nLogical values\nOnly two values: True (1) and False (0)\nConditional operations, flags\n\n\n\n\n\n\nExamples\n\nIntegersFloating Point NumbersComplex Numbers\n\n\nInteger Representation: Integers are whole numbers without a decimal point.\n\n\n\n\n\n\nBinary, Octal, and Hexadecimal: Integers can be represented in different bases:\n\n\n\n\n\n\n\n\nFloating Point Representation: Numbers with a decimal point are treated as floating-point values.\n\n\n\n\n\n\nMaximum Float Value: Python handles large floats, converting them to infinity if they exceed the maximum representable value.\n\n\n\n\n\n\n\n\nComplex Number Representation: Complex numbers have a real and an imaginary part.\n\n\n\n\n\n\n\nAccessors for Complex Numbers:\n\nc.real: Real part of the complex number.\nc.imag: Imaginary part of the complex number.\n\n\n\n\n\n\n\n\nComplex Conjugate: Use the .conjugate() method to get the complex conjugate.",
    "crumbs": [
      "Python Basics",
      "Lecture 1",
      "Variables & Numbers"
    ]
  },
  {
    "objectID": "lectures/lecture01/02-lecture01.html#operators",
    "href": "lectures/lecture01/02-lecture01.html#operators",
    "title": "Variables & Numbers",
    "section": "Operators",
    "text": "Operators\nPython provides a variety of operators for performing operations on variables and values. Here we’ll cover the most common operators used in scientific programming.\n\nArithmetic OperatorsComparison OperatorsLogical OperatorsAssignment Operators\n\n\nThese operators perform basic mathematical operations:\n\n\n\nOperator\nName\nExample\nResult\n\n\n\n\n+\nAddition\n5 + 3\n8\n\n\n-\nSubtraction\n5 - 3\n2\n\n\n*\nMultiplication\n5 * 3\n15\n\n\n/\nDivision\n5 / 3\n1.6666…\n\n\n//\nFloor Division\n5 // 3\n1\n\n\n%\nModulus (remainder)\n5 % 3\n2\n\n\n**\nExponentiation\n5 ** 3\n125\n\n\n\n\n\n\n\n\n\n\n\nThese operators are used to compare values:\n\n\n\nOperator\nDescription\nExample\n\n\n\n\n==\nEqual to\nx == y\n\n\n!=\nNot equal to\nx != y\n\n\n&gt;\nGreater than\nx &gt; y\n\n\n&lt;\nLess than\nx &lt; y\n\n\n&gt;=\nGreater than or equal to\nx &gt;= y\n\n\n&lt;=\nLess than or equal to\nx &lt;= y\n\n\n\n\n\n\n\n\n\n\n\nUsed to combine conditional statements:\n\n\n\n\n\n\n\n\nOperator\nDescription\nExample\n\n\n\n\nand\nReturns True if both statements are true\nx &gt; 0 and x &lt; 10\n\n\nor\nReturns True if one of the statements is true\nx &lt; 0 or x &gt; 10\n\n\nnot\nReverses the result, returns False if the result is true\nnot(x &gt; 0 and x &lt; 10)\n\n\n\n\n\n\n\n\n\n\n\nPython provides shorthand operators for updating variables:\n\n\n\nOperator\nExample\nEquivalent to\n\n\n\n\n=\nx = 5\nx = 5\n\n\n+=\nx += 3\nx = x + 3\n\n\n-=\nx -= 3\nx = x - 3\n\n\n*=\nx *= 3\nx = x * 3\n\n\n/=\nx /= 3\nx = x / 3\n\n\n//=\nx //= 3\nx = x // 3\n\n\n%=\nx %= 3\nx = x % 3\n\n\n**=\nx **= 3\nx = x ** 3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOperator Precedence\n\n\n\n\n\nPython follows the standard mathematical order of operations (PEMDAS):\n\nParentheses\nExponentiation (**)\nMultiplication and Division (*, /, //, %)\nAddition and Subtraction (+, -)\n\nWhen operators have the same precedence, they are evaluated from left to right.",
    "crumbs": [
      "Python Basics",
      "Lecture 1",
      "Variables & Numbers"
    ]
  },
  {
    "objectID": "lectures/lecture01/01-lecture01.html",
    "href": "lectures/lecture01/01-lecture01.html",
    "title": "Jupyter Notebooks",
    "section": "",
    "text": "Throughout this course we will have to create and edit python code. We will primarily use this webpage for convenience, but for day-to-day work in the laboratory, it’s beneficial to utilize a code editor or a notebook environment like JupyterLab. JupyterLab is a robust platform that enables you to develop and modify notebooks within a web browser, while also offering comprehensive capabilities for analyzing and visualizing data.",
    "crumbs": [
      "Python Basics",
      "Lecture 1",
      "Jupyter Notebooks"
    ]
  },
  {
    "objectID": "lectures/lecture01/01-lecture01.html#what-is-a-jupyter-notebook",
    "href": "lectures/lecture01/01-lecture01.html#what-is-a-jupyter-notebook",
    "title": "Jupyter Notebooks",
    "section": "What is a Jupyter Notebook?",
    "text": "What is a Jupyter Notebook?\nA Jupyter Notebook is a web browser based interactive computing environment that enables users to create documents that include code to be executed, results from the executed code such as plots and images,and finally also an additional documentation in form of markdown text including equations in LaTeX.\nThese documents provide a complete and self-contained record of a computation that can be converted to various formats and shared with others using email, version control systems (like git/GitHub) or nbviewer.jupyter.org.\n\nKey Components of a Notebook\nThe Jupyter Notebook ecosystem consists of three main components:\n\nNotebook Editor\nKernels\nNotebook Documents\n\nLet’s explore each of these components in detail:\n\nNotebook Editor\nThe Notebook editor is an interactive web-based application for creating and editing notebook documents. It enables users to write and run code, add rich text, and multimedia content. When running Jupyter on a server, users typically use either the classic Jupyter Notebook interface or JupyterLab, an advanced version with more features.\nKey features of the Notebook editor include:\n\nCode Editing: Write and edit code in individual cells.\nCode Execution: Run code cells in any order and display computation results in various formats (HTML, LaTeX, PNG, SVG, PDF).\nInteractive Widgets: Create and use JavaScript widgets that connect user interface controls to kernel-side computations.\nRich Text: Add documentation using Markdown markup language, including LaTeX equations.\n\n\n\n\n\n\n\nAdvance Notebook Editor Info\n\n\n\n\n\nThe Notebook editor in Jupyter offers several advanced features:\n\nCell Metadata: Each cell has associated metadata that can be used to control its behavior. This includes tags for slideshows, hiding code cells, and controlling cell execution.\nMagic Commands: Special commands prefixed with % (line magics) or %% (cell magics) that provide additional functionality, such as timing code execution or displaying plots inline.\nAuto-completion: The editor provides context-aware auto-completion for Python code, helping users write code more efficiently.\nCode Folding: Users can collapse long code blocks for better readability.\nMultiple Cursors: Advanced editing with multiple cursors for simultaneous editing at different locations.\nSplit View: The ability to split the notebook view, allowing users to work on different parts of the notebook simultaneously.\nVariable Inspector: A tool to inspect and manage variables in the kernel’s memory.\nIntegrated Debugger: Some Jupyter environments offer an integrated debugger for step-by-step code execution and inspection.\n\n\n\n\n\n\n\nKernels\nKernels are the computational engines that execute the code contained in a notebook. They are separate processes that run independently of the notebook editor.\nKey responsibilities of kernels include: * Executing user code * Returning computation results to the notebook editor * Handling computations for interactive widgets * Providing features like tab completion and introspection\n\n\n\n\n\n\nAdvanced Kernel Info\n\n\n\n\n\nJupyter notebooks are language-agnostic. Different kernels can be installed to support various programming languages such as Python, R, Julia, and many others. The default kernel runs Python code, but users can select different kernels for each notebook via the Kernel menu.\nKernels communicate with the notebook editor using a JSON-based protocol over ZeroMQ/WebSockets. For more technical details, see the messaging specification.\nEach kernel runs in its own environment, which can be customized to include specific libraries and dependencies. This allows users to create isolated environments for different projects, ensuring that dependencies do not conflict.\nKernels also support interactive features such as:\n\nTab Completion: Provides suggestions for variable names, functions, and methods as you type, improving coding efficiency.\nIntrospection: Allows users to inspect objects, view documentation, and understand the structure of code elements.\nRich Output: Supports various output formats, including text, images, videos, and interactive widgets, enhancing the interactivity of notebooks.\n\nAdvanced users can create custom kernels to support additional languages or specialized computing environments. This involves writing a kernel specification and implementing the necessary communication protocols.\nFor managing kernels, Jupyter provides several commands and options:\n\nStarting a Kernel: Automatically starts when a notebook is opened.\nInterrupting a Kernel: Stops the execution of the current code cell, useful for halting long-running computations.\nRestarting a Kernel: Clears the kernel’s memory and restarts it, useful for resetting the environment or recovering from errors.\nShutting Down a Kernel: Stops the kernel and frees up system resources.\n\nUsers can also monitor kernel activity and resource usage through the Jupyter interface, ensuring efficient and effective use of computational resources.\n\n\n\n\n\nJupyterLab Example\nThe following is an example of a JupyterLab interface with a notebook editor, code cells, markdown cells, and a kernel selector:\nFull Screen\n\n\n\n\n\nNotebook Documents\nNotebook documents are self-contained files that encapsulate all content created in the notebook editor. They include code inputs/outputs, Markdown text, equations, images, and other media. Each document is associated with a specific kernel and serves as both a human-readable record of analysis and an executable script to reproduce the work.\nCharacteristics of notebook documents:\n\nFile Extension: Notebooks are stored as files with a .ipynb extension.\nStructure: Notebooks consist of a linear sequence of cells, which can be one of three types:\n\nCode cells: Contain executable code and its output.\nMarkdown cells: Contain formatted text, including LaTeX equations.\nRaw cells: Contain unformatted text, preserved when converting notebooks to other formats.\n\n\n\n\n\n\n\n\nAdvanced Notebook Documents Info\n\n\n\n\n\n\nVersion Control: Notebook documents can be version controlled using systems like Git. This allows users to track changes, collaborate with others, and revert to previous versions if needed. Tools like nbdime provide diff and merge capabilities specifically designed for Jupyter Notebooks.\nCell Tags: Cells in a notebook can be tagged with metadata to control their behavior during execution, export, or presentation. For example, tags can be used to hide input or output, skip execution, or designate cells as slides in a presentation.\nInteractive Widgets: Notebook documents can include interactive widgets that allow users to manipulate parameters and visualize changes in real-time. This is particularly useful for data exploration and interactive simulations.\nExtensions: The Jupyter ecosystem supports a wide range of extensions that enhance the functionality of notebook documents. These extensions can add features like spell checking, code formatting, and integration with external tools and services.\nSecurity: Notebook documents can include code that executes on the user’s machine, which poses security risks. Jupyter provides mechanisms to sanitize notebooks and prevent the execution of untrusted code. Users should be cautious when opening notebooks from unknown sources.\nCollaboration: Jupyter Notebooks can be shared and collaboratively edited in real-time using platforms like Google Colab, Microsoft Azure Notebooks, and JupyterHub. These platforms provide cloud-based environments where multiple users can work on the same notebook simultaneously.\nCustomization: Users can customize the appearance and behavior of notebook documents using CSS and JavaScript. This allows for the creation of tailored interfaces and enhanced user experiences.\nExport Options: In addition to static formats, notebooks can be exported to interactive formats like dashboards and web applications. Tools like Voila convert notebooks into standalone web applications that can be shared and deployed.\nProvenance: Notebooks can include provenance information that tracks the origin and history of data and computations. This is important for reproducibility and transparency in scientific research.\nDocumentation: Notebook documents can serve as comprehensive documentation for projects, combining code, results, and narrative text. This makes them valuable for teaching, tutorials, and sharing research findings.\nPerformance: Large notebooks with many cells and outputs can become slow and unwieldy. Techniques like cell output clearing, using lightweight data formats, and splitting notebooks into smaller parts can help maintain performance.\nIntegration: Jupyter Notebooks can integrate with a wide range of data sources, libraries, and tools. This includes databases, cloud storage, machine learning frameworks, and visualization libraries, making them a versatile tool for data science and research.\nInternal Format: Notebook files are JSON text files with binary data encoded in base64, making them easy to manipulate programmatically.\nExportability: Notebooks can be exported to various static formats (HTML, reStructuredText, LaTeX, PDF, slide shows) using Jupyter’s nbconvert utility.\nSharing: Notebooks can be shared via nbviewer, which renders notebooks from public URLs or GitHub as static web pages, allowing others to view the content without installing Jupyter.\n\n\n\n\nThis integrated system of editor, kernels, and documents makes Jupyter Notebooks a powerful tool for interactive computing, data analysis, and sharing of computational narratives.",
    "crumbs": [
      "Python Basics",
      "Lecture 1",
      "Jupyter Notebooks"
    ]
  },
  {
    "objectID": "lectures/lecture01/01-lecture01.html#using-the-notebook-editor",
    "href": "lectures/lecture01/01-lecture01.html#using-the-notebook-editor",
    "title": "Jupyter Notebooks",
    "section": "Using the Notebook Editor",
    "text": "Using the Notebook Editor\n\n\n\nJupyter Notebook Editor\n\n\nThe Jupyter Notebook editor provides an interactive environment for writing code, creating visualizations, and documenting computational workflows. It consists of a web-based interface that allows users to create and edit notebook documents containing code, text, equations, images, and interactive elements. A Jupyter Notebook provides an interface with essentially two modes of operation:\n\nedit mode the mode where you edit a cells content.\ncommand mode the mode where you execute the cells content.\n\nIn the more advanced version of JupyterLab you can also have a presentation mode where you can present your notebook as a slideshow.\n\nEdit mode\nEdit mode is indicated by a blue cell border and a prompt showing in the editor area when a cell is selected. You can enter edit mode by pressing Enter or using the mouse to click on a cell’s editor area.\n\n\n\nEdit Mode\n\n\nWhen a cell is in edit mode, you can type into the cell, like a normal text editor\n\n\nCommand mode\nCommand mode is indicated by a grey cell border with a blue left margin. When you are in command mode, you are able to edit the notebook as a whole, but not type into individual cells. Most importantly, in command mode, the keyboard is mapped to a set of shortcuts that let you perform notebook and cell actions efficiently.\n\n\n\nCommand Mode\n\n\nIf you have a hardware keyboard connected to your iOS device, you can use Jupyter keyboard shortcuts. The modal user interface of the Jupyter Notebook has been optimized for efficient keyboard usage. This is made possible by having two different sets of keyboard shortcuts: one set that is active in edit mode and another in command mode.\n\n\nKeyboard navigation\nIn edit mode, most of the keyboard is dedicated to typing into the cell’s editor area. Thus, in edit mode there are relatively few shortcuts available. In command mode, the entire keyboard is available for shortcuts, so there are many more. Most important ones are:\n\nSwitch command and edit mods: Enter for edit mode, and Esc or Control for command mode.\nBasic navigation: ↑/k, ↓/j\nRun or render currently selected cell: Shift+Enter or Control+Enter\nSaving the notebook: s\nChange Cell types: y to make it a code cell, m for markdown and r for raw\nInserting new cells: a to insert above, b to insert below\nManipulating cells using pasteboard: x for cut, c for copy, v for paste, d for delete and z for undo delete\nKernel operations: i to interrupt and 0 to restart\n\n\n\nRunning code\nCode cells allow you to enter and run code. Run a code cell by pressing the ▶︎ button in the bottom-right panel, or Control+Enter on your hardware keyboard.\n\nv = 23752636\nprint(v)\n\n23752636\n\n\nThere are a couple of keyboard shortcuts for running code:\n\nControl+Enter run the current cell and enters command mode.\nShift+Enter runs the current cell and moves selection to the one below.\nOption+Enter runs the current cell and inserts a new one below.",
    "crumbs": [
      "Python Basics",
      "Lecture 1",
      "Jupyter Notebooks"
    ]
  },
  {
    "objectID": "lectures/lecture01/01-lecture01.html#managing-the-kernel",
    "href": "lectures/lecture01/01-lecture01.html#managing-the-kernel",
    "title": "Jupyter Notebooks",
    "section": "Managing the kernel",
    "text": "Managing the kernel\nCode is run in a separate process called the kernel, which can be interrupted or restarted. You can see kernel indicator in the top-right corner reporting current kernel state: ⚪︎ means kernel is ready to execute code, and ⚫︎ means kernel is currently busy. Tapping kernel indicator will open kernel menu, where you can reconnect, interrupt or restart kernel.\nTry running the following cell — kernel indicator will switch from ⚪︎ to ⚫︎, i.e. reporting kernel as “busy”. This means that you won’t be able to run any new cells until current execution finishes, or until kernel is interrupted. You can then go to kernel menu by tapping the kernel indicator and select “Interrupt”.",
    "crumbs": [
      "Python Basics",
      "Lecture 1",
      "Jupyter Notebooks"
    ]
  },
  {
    "objectID": "lectures/lecture01/01-lecture01.html#markdown-in-notebooks",
    "href": "lectures/lecture01/01-lecture01.html#markdown-in-notebooks",
    "title": "Jupyter Notebooks",
    "section": "Markdown in Notebooks",
    "text": "Markdown in Notebooks\nText can be added to Jupyter Notebooks using Markdown cells. This is extremely useful providing a complete documentation of your calculations or simulations. In this way, everything really becomes an notebook. You can change the cell type to Markdown by using the “Cell Actions” menu, or with a hardware keyboard shortcut m. Markdown is a popular markup language that is a superset of HTML. Its specification can be found here:\nhttps://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet\nMarkdown cells can either be rendered or unrendered.\nWhen they are rendered, you will see a nice formatted representation of the cell’s contents.\nWhen they are unrendered, you will see the raw text source of the cell. To render the selected cell, click the ▶︎ button or shift+ enter. To unrender, select the markdown cell, and press enter or just double click.\n\nMarkdown basics\nBelow are some basic markdown examples, in its rendered form. If you which to access how to create specific appearances, double click the individual cells to put the into an unrendered edit mode.\nYou can make text italic or bold. You can build nested itemized or enumerated lists:\n\n\nMarkdown lists example\n\nFirst item\n\nFirst subitem\n\nFirst sub-subitem\n\nSecond subitem\n\nFirst subitem of second subitem\nSecond subitem of second subitem\n\n\nSecond item\n\nFirst subitem\n\nThird item\n\nFirst subitem\n\n\nNow another list:\n\nHere we go\n\nSublist\n\nSublist\n\n\nThere we go\nNow this\n\n\n\nBlockquote example\n\nBeautiful is better than ugly. Explicit is better than implicit. Simple is better than complex. Complex is better than complicated. Flat is better than nested. Sparse is better than dense. Readability counts. Special cases aren’t special enough to break the rules. Namespaces are one honking great idea – let’s do more of those!\n\n\n\nWeb links example\nJupyter’s website\n\n\nHeadings\nYou can add headings by starting a line with one (or multiple) # followed by a space and the title of your section. The number of # you use will determine the size of the heading\n# Heading 1\n# Heading 2\n## Heading 2.1\n## Heading 2.2\n### Heading 2.2.1\n\n\nEmbedded code\nYou can embed code meant for illustration instead of execution in Python:\ndef f(x):\n    \"\"\"a docstring\"\"\"\n    return x**2\n\n\nLaTeX equations\nCourtesy of MathJax, you can include mathematical expressions both inline: \\(e^{i\\pi} + 1 = 0\\) and displayed:\n\\[e^x=\\sum_{i=0}^\\infty \\frac{1}{i!}x^i\\]\nInline expressions can be added by surrounding the latex code with $:\n$e^{i\\pi} + 1 = 0$\nExpressions on their own line are surrounded by $$:\n$$e^x=\\sum_{i=0}^\\infty \\frac{1}{i!}x^i$$\n\n\nImages\nImages may be also directly integrated into a Markdown block.\nTo include images use\n![alternative text](url)\nfor example\n\n\n\nalternative text\n\n\n\n\nVideos\nTo include videos, we use HTML code like\n&lt;video src=\"mov/movie.mp4\" width=\"320\" height=\"200\" controls preload&gt;&lt;/video&gt;\nin the Markdown cell. This works with videos stored locally.\n\n\nYou can embed YouTube Videos as well by using the IPython module.\n\nfrom IPython.display import YouTubeVideo\nYouTubeVideo('QlLx32juGzI',width=600)",
    "crumbs": [
      "Python Basics",
      "Lecture 1",
      "Jupyter Notebooks"
    ]
  },
  {
    "objectID": "lectures/lecture01/python_lecture01.html",
    "href": "lectures/lecture01/python_lecture01.html",
    "title": "Python & Anatomy of a Python Program",
    "section": "",
    "text": "Python is a high-level, interpreted programming language known for its readability and simplicity. Created by Guido van Rossum in 1991, it emphasizes code readability with its clear syntax and use of indentation. Python supports multiple programming paradigms, including procedural, object-oriented, and functional programming. It comes with a comprehensive standard library and has a vast ecosystem of third-party packages, making it suitable for various applications such as web development, data analysis, artificial intelligence, scientific computing, and automation. Python’s “batteries included” philosophy and gentle learning curve have contributed to its popularity among beginners and experienced developers alike.\nFor physics students specifically, Python has become the language of choice for data analysis, simulation, and visualization in scientific research. Libraries like NumPy, SciPy, and Matplotlib provide powerful tools for solving physics problems, from basic mechanics to quantum mechanics.",
    "crumbs": [
      "Python Basics",
      "Lecture 1",
      "Python & Anatomy of a Program"
    ]
  },
  {
    "objectID": "lectures/lecture01/python_lecture01.html#what-is-python",
    "href": "lectures/lecture01/python_lecture01.html#what-is-python",
    "title": "Python & Anatomy of a Python Program",
    "section": "",
    "text": "Python is a high-level, interpreted programming language known for its readability and simplicity. Created by Guido van Rossum in 1991, it emphasizes code readability with its clear syntax and use of indentation. Python supports multiple programming paradigms, including procedural, object-oriented, and functional programming. It comes with a comprehensive standard library and has a vast ecosystem of third-party packages, making it suitable for various applications such as web development, data analysis, artificial intelligence, scientific computing, and automation. Python’s “batteries included” philosophy and gentle learning curve have contributed to its popularity among beginners and experienced developers alike.\nFor physics students specifically, Python has become the language of choice for data analysis, simulation, and visualization in scientific research. Libraries like NumPy, SciPy, and Matplotlib provide powerful tools for solving physics problems, from basic mechanics to quantum mechanics.",
    "crumbs": [
      "Python Basics",
      "Lecture 1",
      "Python & Anatomy of a Program"
    ]
  },
  {
    "objectID": "lectures/lecture01/python_lecture01.html#anatomy-of-a-python-program",
    "href": "lectures/lecture01/python_lecture01.html#anatomy-of-a-python-program",
    "title": "Python & Anatomy of a Python Program",
    "section": "Anatomy of a Python Program",
    "text": "Anatomy of a Python Program\nUnderstanding the basic structure of a Python program is essential for beginners. Let’s break down the fundamental elements that make up a typical Python program.\n\nBasic Elements\n\n\n\n\n\n\n\n\nElement\nDescription\nExample\n\n\n\n\nStatements\nIndividual instructions that Python executes\nx = 10\n\n\nExpressions\nCombinations of values, variables, and operators that evaluate to a value\nx + 5\n\n\nBlocks\nGroups of statements indented at the same level\nFunction bodies, loops\n\n\nFunctions\nReusable blocks of code that perform specific tasks\ndef calculate_area(radius):\n\n\nComments\nNotes in the code that are ignored by the interpreter\n# This is a comment\n\n\nImports\nStatements that give access to external modules\nimport numpy as np\n\n\n\n\n\nVisual Structure of a Python Program\n# 1. Import statements (external libraries)\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.integrate import odeint  # For solving differential equations\n\n# 2. Constants and global variables\nGRAVITY = 9.81  # m/s^2\nPLANCK_CONSTANT = 6.626e-34  # J·s\nELECTRON_MASS = 9.109e-31  # kg\n\n# 3. Function definitions\ndef calculate_kinetic_energy(mass, velocity):\n    \"\"\"\n    Calculate the kinetic energy of an object.\n\n    Parameters:\n        mass (float): Mass of the object in kg\n        velocity (float): Velocity of the object in m/s\n\n    Returns:\n        float: Kinetic energy in Joules\n    \"\"\"\n    return 0.5 * mass * velocity**2\n\ndef spring_force(k, displacement):\n    \"\"\"\n    Calculate the force exerted by a spring.\n\n    Parameters:\n        k (float): Spring constant in N/m\n        displacement (float): Displacement from equilibrium in m\n\n    Returns:\n        float: Force in Newtons (negative for restoring force)\n    \"\"\"\n    return -k * displacement\n\n# 4. Class definitions (if applicable)\nclass Particle:\n    def __init__(self, mass, position, velocity):\n        self.mass = mass\n        self.position = position\n        self.velocity = velocity\n\n    def update_position(self, time_step):\n        # Simple Euler integration\n        self.position += self.velocity * time_step\n\n    def potential_energy(self, height, g=GRAVITY):\n        \"\"\"Calculate gravitational potential energy\"\"\"\n        return self.mass * g * height\n\n    def momentum(self):\n        \"\"\"Calculate momentum\"\"\"\n        return self.mass * self.velocity\n\n# 5. Main execution code\nif __name__ == \"__main__\":\n    # Create objects or variables\n    particle = Particle(1.0, np.array([0.0, 0.0]), np.array([1.0, 2.0]))\n\n    # Set up simulation parameters\n    time_step = 0.01  # seconds\n    total_time = 1.0  # seconds\n    n_steps = int(total_time / time_step)\n\n    # Arrays to store results\n    positions = np.zeros((n_steps, 2))\n    times = np.zeros(n_steps)\n\n    # Process data/perform calculations - simulate motion\n    for i in range(n_steps):\n        particle.update_position(time_step)\n        positions[i] = particle.position\n        times[i] = i * time_step\n\n    # Output results\n    print(f\"Final position: {particle.position}\")\n    print(f\"Final kinetic energy: {calculate_kinetic_energy(particle.mass, np.linalg.norm(particle.velocity))} J\")\n\n    # Visualize results (if applicable)\n    plt.figure(figsize=(10, 6))\n    plt.subplot(1, 2, 1)\n    plt.plot(positions[:, 0], positions[:, 1], 'r-')\n    plt.xlabel('X position (m)')\n    plt.ylabel('Y position (m)')\n    plt.title('Particle Trajectory')\n    plt.grid(True)\n\n    plt.subplot(1, 2, 2)\n    plt.plot(times, positions[:, 0], 'b-', label='x-position')\n    plt.plot(times, positions[:, 1], 'g-', label='y-position')\n    plt.xlabel('Time (s)')\n    plt.ylabel('Position (m)')\n    plt.title('Position vs Time')\n    plt.legend()\n    plt.grid(True)\n\n    plt.tight_layout()\n    plt.show()\n\n\nKey Concepts\n\nModularity: Python programs are typically organized into functions and classes that encapsulate specific functionality.\nIndentation: Python uses indentation (typically 4 spaces) to define code blocks, unlike other languages that use braces {}.\nDocumentation: Good Python code includes docstrings (triple-quoted strings) that explain what functions and classes do.\nMain Block: The if __name__ == \"__main__\": block ensures code only runs when the file is executed directly, not when imported.\nReadability: Python emphasizes code readability with clear variable names and logical organization.\nPhysics Modeling: For physics problems, we typically model physical systems as objects with properties (mass, position, etc.) and behaviors (update_position, calculate_energy, etc.).\nNumerical Integration: Many physics problems require solving differential equations numerically using methods like Euler integration or Runge-Kutta.\nUnits: Always include appropriate SI units in your comments and documentation to ensure clarity in physics calculations.\n\n\n\n\n\n\n\nBest Practices\n\n\n\n\n\n\nKeep functions short and focused on a single task\nUse meaningful variable and function names\nInclude comments to explain why rather than what (the code should be self-explanatory)\nFollow PEP 8 style guidelines for consistent formatting\nStructure larger programs into multiple modules (files)\nFor physics simulations, validate your code against known analytical solutions when possible\nRemember to handle units consistently throughout your calculations\nConsider the appropriate numerical methods for the physical system you’re modeling\n\n\n\n\n\n\n\n\n\n\nPhysics-Specific Python Libraries\n\n\n\n\n\n\nNumPy: Provides array operations and mathematical functions\nSciPy: Scientific computing tools including optimization, integration, and differential equations\nMatplotlib: Plotting and visualization\nSymPy: Symbolic mathematics for analytical solutions\nPandas: Data manipulation and analysis\nastropy: Astronomy and astrophysics\nscikit-learn: Machine learning for data analysis\nPyMC: Probabilistic programming for statistical analysis",
    "crumbs": [
      "Python Basics",
      "Lecture 1",
      "Python & Anatomy of a Program"
    ]
  },
  {
    "objectID": "lectures/lecture06/2-integration.html",
    "href": "lectures/lecture06/2-integration.html",
    "title": "Computer-Based Physical Modelling",
    "section": "",
    "text": "Numerical integration stands as one of the fundamental computational tools in physics, enabling us to solve problems where analytical solutions are either impossible or impractical. In physics, we frequently encounter integrals when calculating quantities such as:\n\nWork done by a variable force\nElectric and magnetic fields from complex charge distributions\nCenter of mass of irregularly shaped objects\nProbability distributions in quantum mechanics\nEnergy levels in quantum wells with arbitrary potentials\nHeat flow in non-uniform materials\n\nThe need for numerical integration arises because many physical systems are described by functions that cannot be integrated analytically. For example, the potential energy of a complex molecular system, the trajectory of a spacecraft under multiple gravitational influences, or the behavior of quantum particles in complex potentials.\nIn this lecture, we’ll explore three progressively more accurate numerical integration methods: the Box method, Trapezoid method, and Simpson’s method. We’ll analyze their accuracy, efficiency, and appropriate applications in physical problems.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Box Method Illustration\n\n\n\nThe Box method (also known as the Rectangle method) represents the simplest approach for numerical integration. It approximates the function in each interval \\(\\Delta x\\) with a constant value taken at a specific point of the interval—typically the left endpoint, although midpoint or right endpoint variants exist.\nMathematically, the definite integral is approximated as:\n\\[\\begin{equation}\n\\int_{a}^{b} f(x) dx \\approx \\sum_{i=1}^{N} f(x_{i}) \\Delta x\n\\end{equation}\\]\nWhere:\n\n\\(a\\) and \\(b\\) are the integration limits\n\\(N\\) is the number of intervals\n\\(\\Delta x = \\frac{b-a}{N}\\) is the width of each interval\n\\(x_i\\) represents the left endpoint of each interval\n\nThe method gets its name from the visual representation of the approximation as a series of rectangular boxes.\n\n\n\n\nPhysics Application: Work CalculationConvergence Analysis\n\n\nConsider a particle moving along a straight line under a variable force \\(F(x) = kx^2\\) where \\(k\\) is some constant. The work done by this force when moving the particle from position \\(x=0\\) to \\(x=L\\) is given by:\n\\[\\begin{equation}\nW = \\int_{0}^{L} F(x) dx = \\int_{0}^{L} kx^2 dx\n\\end{equation}\\]\nWe can approximate this integral using the Box method, particularly useful when the force follows a complex pattern measured at discrete points.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Trapezoid Method Illustration\n\n\n\nThe Trapezoid method improves upon the Box method by approximating the function with linear segments between consecutive points. Instead of using constant values within each interval, it connects adjacent points with straight lines, forming trapezoids.\nThe mathematical formula for the Trapezoid method is:\n\\[\\begin{equation}\n\\int_{a}^{b} f(x) dx \\approx \\sum_{i=1}^{N-1} \\frac{f(x_i) + f(x_{i+1})}{2} \\Delta x\n\\end{equation}\\]\nWhere: - \\(\\Delta x = \\frac{b-a}{N-1}\\) is the width of each interval - \\(x_i\\) are the sample points\nThis method is particularly effective for smoothly varying functions, which are common in physical systems.\n\n\n\n\nPhysics Application: Electric Potential from Charge DistributionConvergence Analysis\n\n\nA practical application in electromagnetism involves calculating the electric potential at a point due to a non-uniform charge distribution along a line. For a linear charge density \\(\\lambda(x)\\) along the x-axis, the potential at point \\((0,d)\\) is given by:\n\\[\\begin{equation}\nV(0,d) = \\frac{1}{4\\pi\\epsilon_0} \\int_{a}^{b} \\frac{\\lambda(x)}{\\sqrt{x^2 + d^2}} dx\n\\end{equation}\\]\nThe Trapezoid method is well-suited for this calculation, especially when \\(\\lambda(x)\\) is provided as experimental data points.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Simpson’s Method Illustration\n\n\n\nSimpson’s method represents a significant improvement in accuracy over the previous methods by approximating the function with parabolic segments rather than straight lines. This approach is particularly effective for functions with curvature, which are ubiquitous in physics problems.\nThe mathematical formulation of Simpson’s rule is:\n\\[\\begin{equation}\n\\int_{a}^{b} f(x) dx \\approx \\frac{\\Delta x}{3} \\sum_{i=0}^{(N-1)/2} \\left(f(x_{2i}) + 4f(x_{2i+1}) + f(x_{2i+2})\\right)\n\\end{equation}\\]\nWhere: - \\(N\\) is the number of intervals (must be even) - \\(\\Delta x = \\frac{b-a}{N}\\) is the width of each interval\nSimpson’s rule is derived from fitting a quadratic polynomial through every three consecutive points and then integrating these polynomials.\n\n\n\n\nPhysics Application: Quantum Mechanical ProbabilityConvergence Analysis\n\n\nA fundamental application in quantum mechanics involves calculating the probability of finding a particle in a region of space. For a wavefunction \\(\\psi(x)\\), the probability of finding the particle between positions \\(a\\) and \\(b\\) is:\n\\[\\begin{equation}\nP(a \\leq x \\leq b) = \\int_{a}^{b} |\\psi(x)|^2 dx\n\\end{equation}\\]\nSimpson’s method provides high accuracy for these calculations, especially important when dealing with oscillatory wavefunctions.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimpson’s Rule for Numerical Integration\n\n\n\n\n\nSimpson’s Rule is a method for numerical integration that approximates the definite integral of a function by using quadratic polynomials.\n\nFor an integral \\(\\int_a^b f(x)dx\\), Simpson’s Rule fits a quadratic function through three points:\n\n\\(f(a)\\)\n\\(f(\\frac{a+b}{2})\\)\n\\(f(b)\\)\n\nLet’s define:\n\n\\(h = \\frac{b-a}{2}\\)\n\\(x_0 = a\\)\n\\(x_1 = \\frac{a+b}{2}\\)\n\\(x_2 = b\\)\n\nThe quadratic approximation has the form: \\[P(x) = Ax^2 + Bx + C\\]\nThis polynomial must satisfy: \\[f(x_0) = Ax_0^2 + Bx_0 + C\\] \\[f(x_1) = Ax_1^2 + Bx_1 + C\\] \\[f(x_2) = Ax_2^2 + Bx_2 + C\\]\nUsing Lagrange interpolation: \\[P(x) = f(x_0)L_0(x) + f(x_1)L_1(x) + f(x_2)L_2(x)\\]\nwhere \\(L_0\\), \\(L_1\\), \\(L_2\\) are the Lagrange basis functions.\n\n\n\nThe integration of this polynomial leads to Simpson’s Rule:\n\\[\\int_a^b f(x)dx \\approx \\frac{h}{3}[f(a) + 4f(\\frac{a+b}{2}) + f(b)]\\]\n\n\n\nThe error in Simpson’s Rule is proportional to:\n\\[-\\frac{h^5}{90}f^{(4)}(\\xi)\\]\nfor some \\(\\xi \\in [a,b]\\)\n\n\nFor better accuracy, we can divide the interval into \\(n\\) subintervals (where \\(n\\) is even):\n\\[\\int_a^b f(x)dx \\approx \\frac{h}{3}[f(x_0) + 4\\sum_{i=1}^{n/2}f(x_{2i-1}) + 2\\sum_{i=1}^{n/2-1}f(x_{2i}) + f(x_n)]\\]\nwhere \\(h = \\frac{b-a}{n}\\)\nThe method is particularly effective for integrating functions that can be well-approximated by quadratic polynomials over small intervals.\n\n\n\n\n\n\n\n\n\nChoosing the appropriate numerical integration method for a physics problem requires consideration of several factors:\n\n\n\n\n\n\n\n\n\n\nMethod\nError Order\nOptimal For\nLimitations\nExample Physics Applications\n\n\n\n\nBox\n\\(O(N^{-1})\\)\n- Simple, rapid calculations- Step functions- Real-time processing- Discontinuous functions\n- Low accuracy- Requires many points for decent results\n- Basic data analysis- Signals with sharp transitions- First approximations in mechanics\n\n\nTrapezoid\n\\(O(N^{-2})\\)\n- Smooth, continuous functions- Moderate accuracy requirements- Periodic functions\n- Struggles with sharp peaks- Not ideal for higher derivatives\n- Electric and magnetic fields- Orbital mechanics- Path integrals- Work/energy calculations\n\n\nSimpson\n\\(O(N^{-4})\\)\n- Functions with significant curvature- High precision requirements- Oscillatory integrands\n- More computationally intensive- Requires evenly spaced points\n- Quantum mechanical probabilities- Wave optics- Statistical mechanics- Thermal physics\n\n\n\n\n\n\nAdaptive methods can be more efficient when dealing with functions that have varying behavior across the integration range\nImproper integrals (with infinite limits or singularities) often require specialized techniques beyond these basic methods\nHigher-dimensional integration problems (common in statistical and quantum mechanics) may benefit from Monte Carlo methods rather than these quadrature rules",
    "crumbs": [
      "Python Basics",
      "Lecture 6",
      "Numerical Integration"
    ]
  },
  {
    "objectID": "lectures/lecture06/2-integration.html#introduction-to-numerical-integration-in-physics",
    "href": "lectures/lecture06/2-integration.html#introduction-to-numerical-integration-in-physics",
    "title": "Computer-Based Physical Modelling",
    "section": "",
    "text": "Numerical integration stands as one of the fundamental computational tools in physics, enabling us to solve problems where analytical solutions are either impossible or impractical. In physics, we frequently encounter integrals when calculating quantities such as:\n\nWork done by a variable force\nElectric and magnetic fields from complex charge distributions\nCenter of mass of irregularly shaped objects\nProbability distributions in quantum mechanics\nEnergy levels in quantum wells with arbitrary potentials\nHeat flow in non-uniform materials\n\nThe need for numerical integration arises because many physical systems are described by functions that cannot be integrated analytically. For example, the potential energy of a complex molecular system, the trajectory of a spacecraft under multiple gravitational influences, or the behavior of quantum particles in complex potentials.\nIn this lecture, we’ll explore three progressively more accurate numerical integration methods: the Box method, Trapezoid method, and Simpson’s method. We’ll analyze their accuracy, efficiency, and appropriate applications in physical problems.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Box Method Illustration\n\n\n\nThe Box method (also known as the Rectangle method) represents the simplest approach for numerical integration. It approximates the function in each interval \\(\\Delta x\\) with a constant value taken at a specific point of the interval—typically the left endpoint, although midpoint or right endpoint variants exist.\nMathematically, the definite integral is approximated as:\n\\[\\begin{equation}\n\\int_{a}^{b} f(x) dx \\approx \\sum_{i=1}^{N} f(x_{i}) \\Delta x\n\\end{equation}\\]\nWhere:\n\n\\(a\\) and \\(b\\) are the integration limits\n\\(N\\) is the number of intervals\n\\(\\Delta x = \\frac{b-a}{N}\\) is the width of each interval\n\\(x_i\\) represents the left endpoint of each interval\n\nThe method gets its name from the visual representation of the approximation as a series of rectangular boxes.\n\n\n\n\nPhysics Application: Work CalculationConvergence Analysis\n\n\nConsider a particle moving along a straight line under a variable force \\(F(x) = kx^2\\) where \\(k\\) is some constant. The work done by this force when moving the particle from position \\(x=0\\) to \\(x=L\\) is given by:\n\\[\\begin{equation}\nW = \\int_{0}^{L} F(x) dx = \\int_{0}^{L} kx^2 dx\n\\end{equation}\\]\nWe can approximate this integral using the Box method, particularly useful when the force follows a complex pattern measured at discrete points.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Trapezoid Method Illustration\n\n\n\nThe Trapezoid method improves upon the Box method by approximating the function with linear segments between consecutive points. Instead of using constant values within each interval, it connects adjacent points with straight lines, forming trapezoids.\nThe mathematical formula for the Trapezoid method is:\n\\[\\begin{equation}\n\\int_{a}^{b} f(x) dx \\approx \\sum_{i=1}^{N-1} \\frac{f(x_i) + f(x_{i+1})}{2} \\Delta x\n\\end{equation}\\]\nWhere: - \\(\\Delta x = \\frac{b-a}{N-1}\\) is the width of each interval - \\(x_i\\) are the sample points\nThis method is particularly effective for smoothly varying functions, which are common in physical systems.\n\n\n\n\nPhysics Application: Electric Potential from Charge DistributionConvergence Analysis\n\n\nA practical application in electromagnetism involves calculating the electric potential at a point due to a non-uniform charge distribution along a line. For a linear charge density \\(\\lambda(x)\\) along the x-axis, the potential at point \\((0,d)\\) is given by:\n\\[\\begin{equation}\nV(0,d) = \\frac{1}{4\\pi\\epsilon_0} \\int_{a}^{b} \\frac{\\lambda(x)}{\\sqrt{x^2 + d^2}} dx\n\\end{equation}\\]\nThe Trapezoid method is well-suited for this calculation, especially when \\(\\lambda(x)\\) is provided as experimental data points.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Simpson’s Method Illustration\n\n\n\nSimpson’s method represents a significant improvement in accuracy over the previous methods by approximating the function with parabolic segments rather than straight lines. This approach is particularly effective for functions with curvature, which are ubiquitous in physics problems.\nThe mathematical formulation of Simpson’s rule is:\n\\[\\begin{equation}\n\\int_{a}^{b} f(x) dx \\approx \\frac{\\Delta x}{3} \\sum_{i=0}^{(N-1)/2} \\left(f(x_{2i}) + 4f(x_{2i+1}) + f(x_{2i+2})\\right)\n\\end{equation}\\]\nWhere: - \\(N\\) is the number of intervals (must be even) - \\(\\Delta x = \\frac{b-a}{N}\\) is the width of each interval\nSimpson’s rule is derived from fitting a quadratic polynomial through every three consecutive points and then integrating these polynomials.\n\n\n\n\nPhysics Application: Quantum Mechanical ProbabilityConvergence Analysis\n\n\nA fundamental application in quantum mechanics involves calculating the probability of finding a particle in a region of space. For a wavefunction \\(\\psi(x)\\), the probability of finding the particle between positions \\(a\\) and \\(b\\) is:\n\\[\\begin{equation}\nP(a \\leq x \\leq b) = \\int_{a}^{b} |\\psi(x)|^2 dx\n\\end{equation}\\]\nSimpson’s method provides high accuracy for these calculations, especially important when dealing with oscillatory wavefunctions.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimpson’s Rule for Numerical Integration\n\n\n\n\n\nSimpson’s Rule is a method for numerical integration that approximates the definite integral of a function by using quadratic polynomials.\n\nFor an integral \\(\\int_a^b f(x)dx\\), Simpson’s Rule fits a quadratic function through three points:\n\n\\(f(a)\\)\n\\(f(\\frac{a+b}{2})\\)\n\\(f(b)\\)\n\nLet’s define:\n\n\\(h = \\frac{b-a}{2}\\)\n\\(x_0 = a\\)\n\\(x_1 = \\frac{a+b}{2}\\)\n\\(x_2 = b\\)\n\nThe quadratic approximation has the form: \\[P(x) = Ax^2 + Bx + C\\]\nThis polynomial must satisfy: \\[f(x_0) = Ax_0^2 + Bx_0 + C\\] \\[f(x_1) = Ax_1^2 + Bx_1 + C\\] \\[f(x_2) = Ax_2^2 + Bx_2 + C\\]\nUsing Lagrange interpolation: \\[P(x) = f(x_0)L_0(x) + f(x_1)L_1(x) + f(x_2)L_2(x)\\]\nwhere \\(L_0\\), \\(L_1\\), \\(L_2\\) are the Lagrange basis functions.\n\n\n\nThe integration of this polynomial leads to Simpson’s Rule:\n\\[\\int_a^b f(x)dx \\approx \\frac{h}{3}[f(a) + 4f(\\frac{a+b}{2}) + f(b)]\\]\n\n\n\nThe error in Simpson’s Rule is proportional to:\n\\[-\\frac{h^5}{90}f^{(4)}(\\xi)\\]\nfor some \\(\\xi \\in [a,b]\\)\n\n\nFor better accuracy, we can divide the interval into \\(n\\) subintervals (where \\(n\\) is even):\n\\[\\int_a^b f(x)dx \\approx \\frac{h}{3}[f(x_0) + 4\\sum_{i=1}^{n/2}f(x_{2i-1}) + 2\\sum_{i=1}^{n/2-1}f(x_{2i}) + f(x_n)]\\]\nwhere \\(h = \\frac{b-a}{n}\\)\nThe method is particularly effective for integrating functions that can be well-approximated by quadratic polynomials over small intervals.\n\n\n\n\n\n\n\n\n\nChoosing the appropriate numerical integration method for a physics problem requires consideration of several factors:\n\n\n\n\n\n\n\n\n\n\nMethod\nError Order\nOptimal For\nLimitations\nExample Physics Applications\n\n\n\n\nBox\n\\(O(N^{-1})\\)\n- Simple, rapid calculations- Step functions- Real-time processing- Discontinuous functions\n- Low accuracy- Requires many points for decent results\n- Basic data analysis- Signals with sharp transitions- First approximations in mechanics\n\n\nTrapezoid\n\\(O(N^{-2})\\)\n- Smooth, continuous functions- Moderate accuracy requirements- Periodic functions\n- Struggles with sharp peaks- Not ideal for higher derivatives\n- Electric and magnetic fields- Orbital mechanics- Path integrals- Work/energy calculations\n\n\nSimpson\n\\(O(N^{-4})\\)\n- Functions with significant curvature- High precision requirements- Oscillatory integrands\n- More computationally intensive- Requires evenly spaced points\n- Quantum mechanical probabilities- Wave optics- Statistical mechanics- Thermal physics\n\n\n\n\n\n\nAdaptive methods can be more efficient when dealing with functions that have varying behavior across the integration range\nImproper integrals (with infinite limits or singularities) often require specialized techniques beyond these basic methods\nHigher-dimensional integration problems (common in statistical and quantum mechanics) may benefit from Monte Carlo methods rather than these quadrature rules",
    "crumbs": [
      "Python Basics",
      "Lecture 6",
      "Numerical Integration"
    ]
  },
  {
    "objectID": "lectures/lecture06/2-integration.html#error-analysis",
    "href": "lectures/lecture06/2-integration.html#error-analysis",
    "title": "Computer-Based Physical Modelling",
    "section": "Error Analysis",
    "text": "Error Analysis\nThe error behavior of numerical integration methods is crucial for understanding their applicability to physics problems:\n\nBox Method: Error \\(\\propto\\) \\(O(\\Delta x)\\) = \\(O(N^{-1})\\) (linear convergence)\n\nThe error is proportional to the step size\nThe dominant error term comes from the first derivative of the function\n\nTrapezoid Method: Error \\(\\propto\\) \\(O(\\Delta x^2)\\) = \\(O(N^{-2})\\) (quadratic convergence)\n\nThe error is proportional to the square of the step size\nThe dominant error term involves the second derivative of the function\n\nSimpson’s Method: Error \\(\\propto\\) \\(O(\\Delta x^4)\\) = \\(O(N^{-4})\\) (fourth-order convergence)\n\nThe error is proportional to the fourth power of the step size\nThe dominant error term involves the fourth derivative of the function\n\n\nConsequently, if we double the number of points:\n\nBox method: error reduced by a factor of 2\nTrapezoid method: error reduced by a factor of 4\nSimpson’s method: error reduced by a factor of 16\n\nThe practical impact of these convergence rates is substantial. For example, to achieve an error of \\(10^{-6}\\) for a well-behaved function:\n\nBox method might require millions of points\nTrapezoid method might require thousands of points\nSimpson’s method might require only hundreds of points\n\nThis explains why higher-order methods are generally preferred for physics applications requiring high precision, such as quantum mechanical calculations, gravitational wave analysis, or computational fluid dynamics.\n\nConclusion\nNumerical integration stands as a cornerstone of computational physics, bridging the gap between theoretical models and practical analysis of real-world systems. In this lecture, we’ve explored three fundamental methods—Box, Trapezoid, and Simpson’s method—that provide different balances of simplicity, accuracy, and computational efficiency.\nThe key insights from our study include:\n\nMethod selection matters: The choice of integration technique can dramatically impact both accuracy and computational efficiency. For typical physics applications requiring high precision, Simpson’s method often provides the optimal balance of accuracy and computation cost.\nError scaling: Understanding how errors scale with the number of sampling points is crucial for reliable scientific computation. The higher-order convergence of Simpson’s method (\\(O(N^{-4})\\)) makes it particularly valuable for precision-critical applications in physics.\nPhysical context: The nature of the underlying physical system should guide your choice of numerical method. Smoothly varying functions benefit from higher-order methods, while functions with discontinuities may require adaptive or specialized approaches.\nVerification: Always verify numerical results against analytical solutions when possible, or compare results from different numerical methods with increasing resolution to establish confidence in your calculations.\n\nAs you progress in your physics education, these numerical integration techniques will become essential tools in your computational toolkit, enabling you to tackle increasingly complex physical systems from quantum mechanics to astrophysics, fluid dynamics, and beyond.\nIn advanced courses, you’ll explore additional techniques such as Gaussian quadrature, Romberg integration, and specialized methods for oscillatory, singular, or multi-dimensional integrals—each designed to address specific challenges encountered in modern physics.\nRemember that numerical integration is not merely a computational technique but a powerful approach to understanding physical systems that resist analytical treatment, making it an indispensable skill for the modern physicist.\n\n\n\n\n\n\nAdvanced Topics in Numerical Integration\n\n\n\n\n\n\nAdaptive Integration Methods\nThe methods we’ve discussed so far use equal spacing between sampling points. However, most real-world physics problems involve functions that vary dramatically across the integration range. Adaptive methods adjust the point distribution to concentrate more points where the function changes rapidly.\n\n\n\n\n\n\n\n\nMulti-dimensional Integration\nMany physics problems require integration over multiple dimensions, such as calculating mass moments of inertia, electric fields from volume charge distributions, or statistical mechanics partition functions.\nFor 2D integration, we can extend our 1D methods using the concept of iterated integrals:\n\\[\\begin{equation}\n\\int_{a}^{b}\\int_{c}^{d} f(x,y) dy dx \\approx \\sum_{i=1}^{N_x} \\sum_{j=1}^{N_y} w_i w_j f(x_i, y_j)\n\\end{equation}\\]\nWhere \\(w_i\\) and \\(w_j\\) are the weights for the respective 1D methods.\n\n\n\n\n\n\n\n\nMonte Carlo Integration\nFor higher-dimensional integrals and complex domains, Monte Carlo methods become increasingly efficient. These methods use random sampling to approximate integrals and are particularly valuable in quantum and statistical physics.\n\n\n\n\n\n\n\n\nApplication to Real Physics Problems\nLet’s examine two common scenarios in physics that benefit from numerical integration:\n\nNon-uniform Magnetic Field: When a charged particle moves through a non-uniform magnetic field, the work done can be calculated as:\n\\[W = q\\int_{\\vec{r}_1}^{\\vec{r}_2} \\vec{v} \\times \\vec{B}(\\vec{r}) \\cdot d\\vec{r}\\]\nQuantum Tunneling: The tunneling probability through a potential barrier is given by:\n\\[T \\approx \\exp\\left(-\\frac{2}{\\hbar}\\int_{x_1}^{x_2} \\sqrt{2m(V(x) - E)}\\, dx\\right)\\]\n\nIn both cases, the integrals frequently cannot be solved analytically due to the complex spatial dependence of the fields or potentials, making numerical integration indispensable for modern physics.",
    "crumbs": [
      "Python Basics",
      "Lecture 6",
      "Numerical Integration"
    ]
  },
  {
    "objectID": "lectures/lecture06/3_solving_ODEs_old.html",
    "href": "lectures/lecture06/3_solving_ODEs_old.html",
    "title": "Putting it all together",
    "section": "",
    "text": "All the stuff we have defined in the previous sections is useful for solving ordinary differential equations. This will bring us closer to solving out physics problems now."
  },
  {
    "objectID": "lectures/lecture06/3_solving_ODEs_old.html#solving-odes",
    "href": "lectures/lecture06/3_solving_ODEs_old.html#solving-odes",
    "title": "Putting it all together",
    "section": "",
    "text": "All the stuff we have defined in the previous sections is useful for solving ordinary differential equations. This will bring us closer to solving out physics problems now."
  },
  {
    "objectID": "lectures/lecture06/3_solving_ODEs_old.html#harmonic-oscillator",
    "href": "lectures/lecture06/3_solving_ODEs_old.html#harmonic-oscillator",
    "title": "Putting it all together",
    "section": "Harmonic Oscillator",
    "text": "Harmonic Oscillator\n\n\n\n\n\n\nPhysics Interlude: The harmonic oscillator\n\n\n\nWe are going to tackle as a first very simple problem, the harmonic oscillator and we will demonstrate that with the matrix (Crank-Nicholson method or implicit scheme), the Euler type integration method and using some ‘unknown’ integrator in the module SciPy.\nThe equation of motion for a classical harmonic oscillator is given\n\\[\\begin{equation}\n\\frac{\\mathrm{d}^2x}{\\mathrm{d}t^2}+\\omega^2 x=0\n\\end{equation}\\]\nThis is a second order differential equation which requires for its solution two initial conditions. The first initial condition is the initial elongation \\(x(t=0)=x_{0}\\) and the second the initial velocity \\(\\dot{x}(t=0)=v_{0}\\)."
  },
  {
    "objectID": "lectures/lecture06/3_solving_ODEs_old.html#implicit-solution",
    "href": "lectures/lecture06/3_solving_ODEs_old.html#implicit-solution",
    "title": "Putting it all together",
    "section": "Implicit Solution",
    "text": "Implicit Solution\nLets start with the matrix appraoch we have just learned about. Using the matrix version, we can transform the above equation into a system of coupled equations, which we can solve with some standard methods available from e.g. the SciPy module.\n\nDefine Matrices\nOur matrix will consist of two parts. The first containing the second derivative and the second just the elongation. Suppose we want to calculate the position \\(x(t)\\) at 6 instances in time \\(t_{i}\\) then the matrix version of the second derivative reads as\n(\\(x_{1}=x(t_{1}), \\ldots\\)).\n\\(T=\\frac{d^2x}{dt^2}=\\frac{1}{\\delta t^2}\n\\begin{bmatrix}\n-2 & 1  & 0 & 0 & 0 & 0\\\\\n1 & -2 & 1 & 0 & 0 & 0\\\\\n0 & 1  & -2 & 1 & 0 & 0\\\\\n0 & 0  & 1  & -2 & 1 & 0\\\\\n0 & 0  & 0  &  1 & -2 & 1\\\\\n0 & 0  & 0  &  0 &  1 & -2\\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nx_{1}\\\\\nx_{2}\\\\\nx_{3}\\\\\nx_{4}\\\\\nx_{5}\\\\\nx_{6}\n\\end{bmatrix}\\)\nThe second term in the equation of motion is a multiplication of the elongation \\(x(t_{i})\\) by \\(\\omega^{2}\\) and can be written as\n\\(V=\\omega^2 x=\\begin{bmatrix}\n\\omega^2  & 0  & 0 & 0 & 0 & 0\\\\\n0 & \\omega^2  & 0 & 0 & 0 & 0\\\\\n0 & 0  & \\omega^2  & 0 & 0 & 0\\\\\n0 & 0  & 0  & \\omega^2  & 0 & 0\\\\\n0 & 0  & 0  &  0 & \\omega^2  & 0\\\\\n0 & 0  & 0  &  0 &  0 & \\omega^2 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nx_{1}\\\\\nx_{2}\\\\\nx_{3}\\\\\nx_{4}\\\\\nx_{5}\\\\\nx_{6}\n\\end{bmatrix}\\)\nThe left hand side of the would threfore contain a sum of the two matrices \\(M=T+V\\) multiplied by the vector \\(x\\). We have therfore almost all things together to solve this differential equation with the help of an implicit scheme. What we have ignored so far are the initial conditions.\n\n\nUse Initial Conditions\nThe matrix given for the second detivative actually implies already some initial (bounary) conditions. You probably noticed that the matrix contains incomplete coefficients for the second derivative in the first and last line. The first line contains \\((-2,1)\\), but the second derivative should contain \\((1,-2,1)\\). This \\((-2,1)\\) thus always includes the boundary condition that \\(x_{0}=0\\). To include our own initial/boundary conditions, we have to construct the matrix for the second derivative slightly differently and modify the differential equation to\n\\[\\begin{equation}\n\\frac{\\mathrm{d}^2x}{\\mathrm{d}t^2}+\\omega^2 x=b\n\\end{equation}\\]\nwhere the vector b takes care of the initial conditions.\nIf we have \\(N\\) positions in time at which we calculate the elongation \\(x\\), we have a \\(N\\times N\\) matrix of for the second derivatives. The lower \\(N-2\\) lines will contain the the coefficients for the second derivative \\((1,-2,1)\\). The first two lines supply the initial/boundary conditions.\nThe initial condition for the elongation \\(x(t=0)=x_{0}\\) is obtained when the first element of the first line is a 1. The matrix multiplication \\(M\\, x=b\\) for yields thus in the first line \\(x_{1}=b_{1}\\) and we set \\(b_{1}=x_{0}\\). The second line shall give the initial velocity. So the matrix entries of the second line contain a first derivative \\((-1,1)\\). The matrix multiplication thus yields \\(x_{2}-x_{1}=b_{2}\\). We can therefore need to set \\(b_{2}=v_{0}\\delta t\\). All of the other entries of \\(b\\) shall be set to zero according to the differential equation of the harmonic oscillator.\nOur final problem \\(M\\, x=b\\) will thus have the following shape\n\\[\\begin{equation}\n\\begin{bmatrix}\n1 & 0  & 0 & 0 & 0 & 0\\\\\n-1 & 1 & 0 & 0 & 0 & 0\\\\\n1 & -2+\\omega^2*\\delta t^2  & 1 & 0 & 0 & 0\\\\\n0 & 1  & -2+\\omega^2*\\delta t^2  & 1 & 0 & 0\\\\\n0 & 0  & 1  &  -2+\\omega^2*\\delta t^2 & 1 & 0\\\\\n0 & 0  & 0  &  1 &  -2+\\omega^2*\\delta t^2 & 1\\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nx_{1}\\\\\nx_{2}\\\\\nx_{3}\\\\\nx_{4}\\\\\nx_{5}\\\\\nx_{6}\n\\end{bmatrix}=\n\\begin{bmatrix}\nx_{0}\\\\\nv_{0}\\delta t\\\\\n0\\\\\n0\\\\\n0\\\\\n0\n\\end{bmatrix}\n\\end{equation}\\]\n\n\nSolution\nThis is the final system of coupled equations which we can supply to any matrix solver. We will use a solver from the scipy.linalg module. Lets have a look at the details below.\nN=10\n\n(diags([-2., 1., 1.], [-1,-2, 0],\n    shape=(N, N))+diags([1], [-1], shape=(N, N))* omega**2*dt**2)"
  },
  {
    "objectID": "lectures/lecture06/3_solving_ODEs_old.html#explicit-solution---numerical-integration",
    "href": "lectures/lecture06/3_solving_ODEs_old.html#explicit-solution---numerical-integration",
    "title": "Putting it all together",
    "section": "Explicit Solution - Numerical Integration",
    "text": "Explicit Solution - Numerical Integration\nBefore implementing explicit numerical schemes, let’s develop a standardized approach for solving ODEs. This framework will allow us to solve different problems using various methods with minimal code modification.\nLet’s examine the free fall problem as an example:\n\\[\\begin{equation}\n\\ddot{x}= -g\n\\end{equation}\\]\nThis second-order equation can be transformed into a system of two first-order equations:\n\\[\\begin{eqnarray}\n\\dot{x} &= v \\\\\n\\dot{v} &= -g\n\\end{eqnarray}\\]\nUsing the Euler method, these equations become:\n\\[\\begin{eqnarray}\nx_{i+1} &= x_i + v_i \\Delta t \\\\\nv_{i+1} &= v_i - g\\Delta t\n\\end{eqnarray}\\]\nNote: The original equations had \\(\\dot{x}\\) and \\(\\dot{v}\\) in the right-hand side, which should be replaced with their actual values (\\(v\\) and \\(-g\\) respectively).\nThese equations can be written more compactly in vector form:\n\\[\\begin{equation}\n\\vec{y}_{i+1} = \\vec{y}_i + \\dot{\\vec{y}}_i \\Delta t\n\\end{equation}\\]\nwhere\n\\[\\begin{equation}\n\\vec{y}=\n\\begin{bmatrix}\nx \\\\\nv\n\\end{bmatrix}\n\\end{equation}\\]\nand\n\\[\\begin{equation}\n\\dot{\\vec{y}}=\n\\begin{bmatrix}\nv \\\\\n-g\n\\end{bmatrix}\n\\end{equation}\\]\nThis vector formulation allows us to separate:\n\nProblem definition (specifying \\(\\dot{\\vec{y}}\\) as a function of \\(\\vec{y}\\) and \\(t\\))\nSolution method (implementing the numerical integration scheme)\n\nWe’ll explore three numerical methods:\n\nEuler Method: First-order accurate\nEuler-Cromer Method: Modified Euler method, better for oscillatory systems\nMidpoint Method: Second-order accurate\n\nMore sophisticated methods like the Runge-Kutta family offer higher accuracy but are not covered here.\n\nEuler Method\nThe Euler method is derived from the Taylor expansion of the solution \\(\\vec{y}(t)\\) around the current time \\(t\\):\n\\[\\begin{equation}\n\\vec{y}(t+\\Delta t)=\\vec{y}(t)+\\dot{\\vec{y}}(t)\\Delta t+\\frac{1}{2}\\ddot{\\vec{y}}(t)\\Delta t^{2}+ \\mathcal{O}(\\Delta t^3)\n\\end{equation}\\]\nThe Euler method approximates this by truncating after the first-order term:\n\\[\\begin{equation}\n\\vec{y}(t+\\Delta t) \\approx \\vec{y}(t) + \\dot{\\vec{y}}(t) \\Delta t\n\\end{equation}\\]\nFor our free fall example, this becomes:\n\\[\\begin{equation}\n\\begin{bmatrix} x_{i+1} \\\\ v_{i+1} \\end{bmatrix} =\n\\begin{bmatrix} x_i \\\\ v_i \\end{bmatrix} +\n\\begin{bmatrix} v_i \\\\ -g \\end{bmatrix} \\Delta t\n\\end{equation}\\]\nError Analysis: The method has two distinct types of errors. The local truncation error, which represents the error made in a single step, is of order \\(\\mathcal{O}(\\Delta t^2)\\). This corresponds to the first term omitted in the Taylor expansion. The global truncation error, which accumulates over the entire integration interval \\([0,\\tau]\\), is of order \\(\\mathcal{O}(\\Delta t)\\). This can be understood by considering that we take \\(N = \\tau/\\Delta t\\) steps, each contributing an error proportional to \\(\\Delta t^2\\). The total error thus scales as \\(N \\cdot \\Delta t^2 = \\tau \\Delta t\\).\nLimitations and Extensions: The method is directly applicable only to first-order systems of the form \\(\\dot{\\vec{y}} = \\vec{f}(\\vec{y},t)\\). However, this is not a fundamental limitation as higher-order equations can be converted to systems of first-order equations. For example, a second-order equation \\(\\ddot{x} = f(x,\\dot{x},t)\\) can be transformed into a system of two first-order equations by introducing the velocity as an additional variable. The resulting system becomes:\n\\[\\begin{equation}\n\\begin{bmatrix} \\dot{x} \\\\ \\dot{v} \\end{bmatrix} =\n\\begin{bmatrix} v \\\\ f(x,v,t) \\end{bmatrix}\n\\end{equation}\\]\nThis transformation allows us to apply the method to a wider class of problems while maintaining its fundamental characteristics.\n\n\nEuler-Cromer Method\nThe Euler-Cromer method (also known as the semi-implicit Euler method) modifies the basic Euler method by using the updated velocity when calculating the position. For a system described by position and velocity:\n\\[\\begin{equation}\n\\begin{aligned}\n\\dot{x} &= v \\\\\n\\dot{v} &= f(x,v,t)\n\\end{aligned}\n\\end{equation}\\]\nThe integration steps are:\n\\[\\begin{equation}\n\\begin{aligned}\nv_{i+1} &= v_i + f(x_i,v_i,t_i)\\Delta t \\\\\nx_{i+1} &= x_i + v_{i+1}\\Delta t\n\\end{aligned}\n\\end{equation}\\]\nFor our free fall example: \\[\\begin{equation}\n\\begin{aligned}\nv_{i+1} &= v_i - g\\Delta t \\\\\nx_{i+1} &= x_i + v_{i+1}\\Delta t\n\\end{aligned}\n\\end{equation}\\]\nEnergy Behavior: The method shows improved energy conservation for oscillatory systems compared to the standard Euler method. While the Euler method typically increases energy over time, the Euler-Cromer method exhibits small energy oscillations around the correct value.\nError Analysis: The method maintains a local truncation error of \\(\\mathcal{O}(\\Delta t^2)\\) and a global truncation error of \\(\\mathcal{O}(\\Delta t)\\). Despite having the same order of accuracy as the Euler method, it provides more stable solutions for oscillatory systems.\nAdvantages: The Euler-Cromer method represents a simple modification of the Euler method that achieves better stability for oscillatory systems without requiring additional function evaluations.\nLimitations: The method remains first-order accurate globally and is not symmetric in time. While it performs well for certain types of problems, particularly oscillatory systems, it may not be suitable for all differential equations.\nComparison with Euler Method:\n# Euler Method\nv[i+1] = v[i] + f(x[i],v[i],t[i])*dt\nx[i+1] = x[i] + v[i]*dt       # Uses old velocity\n\n# Euler-Cromer Method\nv[i+1] = v[i] + f(x[i],v[i],t[i])*dt\nx[i+1] = x[i] + v[i+1]*dt     # Uses new velocity\n\n\nMidpoint Method\nThe Midpoint Method (also known as the second-order Runge-Kutta method) improves upon both the Euler and Euler-Cromer methods by using the average of the derivatives at the current point and an estimated midpoint.\nFor a system of first-order differential equations:\n\\[\\begin{equation}\n\\dot{\\vec{y}} = \\vec{f}(\\vec{y},t)\n\\end{equation}\\]\nThe algorithm proceeds in two steps:\n\nCalculate an intermediate point using an Euler step to the midpoint: \\[\\begin{equation}\n\\vec{k}_1 = \\vec{f}(\\vec{y}_i,t_i)\n\\end{equation}\\] \\[\\begin{equation}\n\\vec{y}_{i+1/2} = \\vec{y}_i + \\frac{\\Delta t}{2}\\vec{k}_1\n\\end{equation}\\]\nUse the derivative at this midpoint for the full step: \\[\\begin{equation}\n\\vec{k}_2 = \\vec{f}(\\vec{y}_{i+1/2},t_i+\\Delta t/2)\n\\end{equation}\\] \\[\\begin{equation}\n\\vec{y}_{i+1} = \\vec{y}_i + \\Delta t\\vec{k}_2\n\\end{equation}\\]\n\nFor our free fall example, this becomes:\n\\[\\begin{equation}\n\\begin{aligned}\nv_{i+1/2} &= v_i - \\frac{g\\Delta t}{2} \\\\\nx_{i+1/2} &= x_i + v_i\\frac{\\Delta t}{2} \\\\\nv_{i+1} &= v_i - g\\Delta t \\\\\nx_{i+1} &= x_i + v_{i+1/2}\\Delta t\n\\end{aligned}\n\\end{equation}\\]\nError Analysis: The method achieves higher accuracy than both Euler and Euler-Cromer methods with:\n\nLocal truncation error: \\(\\mathcal{O}(\\Delta t^3)\\)\nGlobal truncation error: \\(\\mathcal{O}(\\Delta t^2)\\)\n\nImplementation:\ndef midpoint_step(y, t, dt, f):\n    # Calculate k1\n    k1 = f(y, t)\n\n    # Calculate midpoint\n    y_mid = y + 0.5 * dt * k1\n\n    # Calculate k2 at midpoint\n    k2 = f(y_mid, t + 0.5*dt)\n\n    # Full step using midpoint derivative\n    return y + dt * k2\n\n\n\n\n\n\n\n\n\nNow we can implement our numerical solution by combining our understanding of both the physical system and numerical methods. This implementation consists of two main parts: defining the differential equation and solving it numerically.\n\nThe Definition of the Problem\nFor the simple harmonic oscillator, we start with the second-order differential equation:\n\\[\\begin{equation}\n\\frac{d^2x}{dt^2} + \\omega^2x = 0\n\\end{equation}\\]\nTo solve this numerically, we convert it to a system of first-order equations using our state vector \\(\\vec{y} = [x, v]^T\\):\n\\[\\begin{equation}\n\\frac{d}{dt}\\begin{bmatrix} x \\\\ v \\end{bmatrix} =\n\\begin{bmatrix} v \\\\ -\\omega^2x \\end{bmatrix}\n\\end{equation}\\]\nThis is implemented as: ~~~ def SHO(state, time): ““” Define the harmonic oscillator system. state[0] : position x state[1] : velocity v returns : [dx/dt, dv/dt] ““” g0 = state[1] # dx/dt = v g1 = -k/m*state[0] # dv/dt = -ω²x return np.array([g0, g1]) ~~~\nThis function defines our physical system by returning the derivatives of our state variables at any given point.\n\n\nSolving the Problem\nWith our system defined, we can implement the numerical solution using Euler’s method. The basic algorithm takes the current state and advances it by one time step:\ndef euler(y, t, dt, derivs):\n    \"\"\"\n    Perform one step of the Euler method.\n    y      : current state [x, v]\n    t      : current time\n    dt     : time step\n    derivs : function returning derivatives\n    \"\"\"\n    y_next = y + derivs(y, t) * dt\n    return y_next\nThis simple structure allows us to solve different physical problems by just changing the derivative function. For example, we can solve the free fall problem with initial conditions \\(x_0=0\\) and \\(v_0=10\\), or the harmonic oscillator with specified spring constant \\(k\\) and mass \\(m\\).\nThe key advantage of this structure lies in its flexibility. We can change the physical system by providing a different derivative function, implement various numerical methods by modifying the integration step, and explore the system behavior by adjusting parameters and initial conditions. This modular approach allows us to study a wide range of physical systems using the same basic numerical framework."
  },
  {
    "objectID": "lectures/lecture06/3_solving_ODEs_old.html#sec-solving-ODE",
    "href": "lectures/lecture06/3_solving_ODEs_old.html#sec-solving-ODE",
    "title": "Putting it all together",
    "section": "Solving the Harmonic Oscillator with SciPy",
    "text": "Solving the Harmonic Oscillator with SciPy\nHaving explored basic numerical integration methods, we can now utilize more sophisticated tools available in SciPy. The scipy.integrate.odeint() function provides a robust and accurate integration method with several advantages over our simple implementations.\nTo use SciPy’s integrator:\nfrom scipy.integrate import odeint\nThe basic syntax is:\nsolution = odeint(derivative_function, initial_conditions, time_points)\nwhere:\n\nderivative_function defines the system (like our SHO function)\ninitial_conditions is a vector containing \\([x_0, v_0]\\)\ntime_points is an array of times at which to compute the solution\n\nThe odeint function offers several significant advantages over our simple implementations. It features adaptive step size control, which automatically adjusts the integration step size based on the local error. The function performs continuous error estimation and correction to maintain accuracy throughout the integration. It also provides various integration methods that can be selected based on the problem’s requirements. The function is capable of handling stiff equations, which are particularly challenging for simpler methods, and generally provides better numerical stability across a wide range of problems.\nFor example, to solve the harmonic oscillator:\ndef SHO(state, t, k=1.0, m=1.0):\n    x, v = state\n    return [v, -k/m * x]\n\n# Initial conditions\ny0 = [1.0, 0.0]  # x₀ = 1, v₀ = 0\nt = np.linspace(0, 10, 1000)\n\n# Solve the system\nsolution = odeint(SHO, y0, t)\nThe solution array contains:\n\nsolution[:, 0]: position values\nsolution[:, 1]: velocity values\n\nHaving understood the fundamentals of numerical integration through our implementations of Euler and other methods, we can now confidently use this more sophisticated tool for solving differential equations more accurately and efficiently.\n\nSetup\n\n\n\n\n\n\n\n\nDefinition\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nPlotting"
  },
  {
    "objectID": "lectures/lecture06/3_solving_ODEs_old.html#damped-driven-pendulum-in-scipy",
    "href": "lectures/lecture06/3_solving_ODEs_old.html#damped-driven-pendulum-in-scipy",
    "title": "Putting it all together",
    "section": "Damped Driven Pendulum in SciPy",
    "text": "Damped Driven Pendulum in SciPy\nWrite a derivs function for a damped driven pendulum:\n\\[\\begin{equation}\n\\ddot{\\theta}=-\\frac{g}{L}\\sin(\\theta)-b \\dot{\\theta}+\\beta\\cos(\\omega t)\n\\end{equation}\\]\nUse this derivs function with the SciPy solver and plot the result for different parameters. Vary the damping parameter \\(b\\). Observe the contributions of the homogeneous and the particular solution. Plot the amplitude of the stationary solution as a function of frequency!\n\nSetup\n\n\n\n\n\n\n\n\nDefinition\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nPlotting"
  },
  {
    "objectID": "lectures/lecture07/2_coupled_pendula.html",
    "href": "lectures/lecture07/2_coupled_pendula.html",
    "title": "Computer-Based Physical Modelling",
    "section": "",
    "text": "In our previous lecture, we explored methods for solving ordinary differential equations (ODEs) by examining the harmonic oscillator. We saw how a single differential equation can describe the motion of a simple physical system. However, most real-world systems involve multiple components that interact with each other, leading to coupled differential equations.\nCoupled differential equations arise when the behavior of one variable directly influences another, and vice versa. These systems exhibit rich dynamics not possible in single-equation systems, including phenomena like synchronization, beats, and energy transfer. Understanding coupled systems is essential across physics, engineering, biology, and economics.\nThe coupled pendula system serves as an ideal introduction to coupled differential equations. It’s conceptually simple enough to visualize yet complex enough to demonstrate important physical phenomena. By connecting two pendula with a spring, we create a system where the motion of each pendulum affects the other, resulting in fascinating behaviors like normal modes and energy exchange.\nIn this lecture, we’ll formulate the coupled equations of motion, implement numerical solutions using SciPy’s advanced ODE solvers, visualize the results, and analyze specific cases like in-phase and out-of-phase motion. This will build directly on our previous work with single ODEs while introducing new concepts unique to coupled systems.",
    "crumbs": [
      "Python Basics",
      "Lecture 7",
      "Coupled Pendula"
    ]
  },
  {
    "objectID": "lectures/lecture07/2_coupled_pendula.html#coupled-differential-equations",
    "href": "lectures/lecture07/2_coupled_pendula.html#coupled-differential-equations",
    "title": "Computer-Based Physical Modelling",
    "section": "",
    "text": "In our previous lecture, we explored methods for solving ordinary differential equations (ODEs) by examining the harmonic oscillator. We saw how a single differential equation can describe the motion of a simple physical system. However, most real-world systems involve multiple components that interact with each other, leading to coupled differential equations.\nCoupled differential equations arise when the behavior of one variable directly influences another, and vice versa. These systems exhibit rich dynamics not possible in single-equation systems, including phenomena like synchronization, beats, and energy transfer. Understanding coupled systems is essential across physics, engineering, biology, and economics.\nThe coupled pendula system serves as an ideal introduction to coupled differential equations. It’s conceptually simple enough to visualize yet complex enough to demonstrate important physical phenomena. By connecting two pendula with a spring, we create a system where the motion of each pendulum affects the other, resulting in fascinating behaviors like normal modes and energy exchange.\nIn this lecture, we’ll formulate the coupled equations of motion, implement numerical solutions using SciPy’s advanced ODE solvers, visualize the results, and analyze specific cases like in-phase and out-of-phase motion. This will build directly on our previous work with single ODEs while introducing new concepts unique to coupled systems.",
    "crumbs": [
      "Python Basics",
      "Lecture 7",
      "Coupled Pendula"
    ]
  },
  {
    "objectID": "lectures/lecture07/2_coupled_pendula.html#coupled-pendula",
    "href": "lectures/lecture07/2_coupled_pendula.html#coupled-pendula",
    "title": "Computer-Based Physical Modelling",
    "section": "Coupled pendula",
    "text": "Coupled pendula\nWe will continue our course with some physical problems, we are going to tackle. One of the more extensive solutions will consider two coupled pendula. This belongs to the class of coupled oscillators, which are extremely important. They will later yield propagating waves. They are important for phonons, i.e. coupled vibration of atoms in solids, but there are also many other axamples. One can realize the coupled oscillation on different ways. He we will do that not with spring oscillators, by with pendula.",
    "crumbs": [
      "Python Basics",
      "Lecture 7",
      "Coupled Pendula"
    ]
  },
  {
    "objectID": "lectures/lecture07/2_coupled_pendula.html#description-of-the-problem",
    "href": "lectures/lecture07/2_coupled_pendula.html#description-of-the-problem",
    "title": "Computer-Based Physical Modelling",
    "section": "Description of the problem",
    "text": "Description of the problem\n\nSketch\nThe image below depicts the sitution we would like to cover in our first project. These are two pendula, which have the length \\(L_{1}\\) and \\(L_{2}\\). Both are coupled with a spring of spring constant \\(k\\), which is relaxed, when both pendula are at rest. You may want to include a generalized version where the spring is mounted at a distance \\(c\\) from the turning points of the pendula.\nIf you develop the equations of motion, write them down as a sum of torques. Use one equation of motion for each pendulum. The result will be two coupled differential equations for the angular coordinates. They are solved by the scipy solve_ivp function without any friction.\n\n\n\n\n\n\nFigure 1: Sketch of the two coupled pendula.\n\n\n\n\n\nEquations of motion\nThe equations of motion of the two coupled pendula have the following form:\n\\[\\begin{eqnarray}\nI_{1}\\ddot{\\theta_{1}}&=&-m_{1}gL_{1}\\sin(\\theta_{1})-kL_{1}^2[\\sin(\\theta_{1})-\\sin(\\theta_{2})]\\\\\nI_{2}\\ddot{\\theta_{2}}&=&-m_{2}gL_{2}\\sin(\\theta_{2})+kL_{2}^2[\\sin(\\theta_{1})-\\sin(\\theta_{2})]\n\\end{eqnarray}\\]\nThese equations describe how the angular accelerations (\\(\\ddot{\\theta}_{1}\\) and \\(\\ddot{\\theta}_{2}\\)) of the two pendula depend on their positions and the coupling between them. Let’s break down each term:\n\n\\(I_{1}\\ddot{\\theta_{1}}\\) and \\(I_{2}\\ddot{\\theta_{2}}\\) represent the angular acceleration multiplied by the moment of inertia for each pendulum. For a simple pendulum, the moment of inertia is given by \\(I_1 = m_1L_1^2\\) and \\(I_2 = m_2L_2^2\\), where \\(m_1\\) and \\(m_2\\) are the masses at the end of each pendulum, and \\(L_1\\) and \\(L_2\\) are the respective lengths of the pendula.\n\\(-m_{1}gL_{1}\\sin(\\theta_{1})\\) and \\(-m_{2}gL_{2}\\sin(\\theta_{2})\\) are the gravitational torques acting on each pendulum. These terms would appear even if the pendula were not coupled, representing the natural restoring force that pulls each pendulum toward its equilibrium position.\n\\(-kL_{1}^2[\\sin(\\theta_{1})-\\sin(\\theta_{2})]\\) and \\(+kL_{2}^2[\\sin(\\theta_{1})-\\sin(\\theta_{2})]\\) represent the coupling torques due to the spring connecting the pendula. The term \\([\\sin(\\theta_{1})-\\sin(\\theta_{2})]\\) approximates the horizontal displacement difference between the pendula, and these terms have opposite signs because the spring pulls the pendula toward each other. Now the spring pulls directly at the lengths \\(L_1\\) and \\(L_2\\) of the respective pendula rather than at a separate distance \\(c\\).\n\nHere, \\(\\theta_{1}, \\theta_{2}\\) measure the angle of the two pendula with the length \\(L_{1},L_{2}\\). \\(k\\) is the spring constant of the spring coupling both pendula. The spring now connects directly at the end of each pendulum at distances \\(L_1\\) and \\(L_2\\) from their respective pivot points.",
    "crumbs": [
      "Python Basics",
      "Lecture 7",
      "Coupled Pendula"
    ]
  },
  {
    "objectID": "lectures/lecture07/2_coupled_pendula.html#solving-the-problem",
    "href": "lectures/lecture07/2_coupled_pendula.html#solving-the-problem",
    "title": "Computer-Based Physical Modelling",
    "section": "Solving the problem",
    "text": "Solving the problem\n\nSetting up the function\nIn our previous lecture, we used the solve_ivp function of scipy to solve the driven damped harmonic oscillator. Remember that we used the array\nstate[0] -&gt; position\nstate[1] -&gt; velocity\nto exchange position and velocity with the solver via the function that defines the physical problem\ndef SHO(time, state):\n    g0 = state[1]               -&gt;velocity\n    g1 = -k/m * state [0]       -&gt;acceleration\n    return np.array([g0, g1])\nfor a coupled system of different equations, we can now extend the state array. In the case of the coupled system of equations it has the following structure\ndef SHO(time, state):\n    g0 = how the velocity of object 1 depends on the velocities of all objects\n    g1 = how the acceleration of object 1 depends on the positions and velocities of all objects\n    g2 = how the velocity of object 2 depends on the velocities of all objects\n    g3 = how the acceleration of object 2 depends on the positions and velocities of all objects\n    return np.array([g0, g1, g2, g3])\nSo the state vector just gets longer and the coupling is in the definition of the velocities and accelerations. The results are then the positions and velocities of the objects. Use this type of scheme to define the problem and write a function, which returns the state of the objects as before.\n\n\n\n\n\n\n\n\nDefine initial parameters\nWe want to define some parameters of the pendula\n\nlength of the pendulum 1, \\(L_1\\)=3\nlength of the pendulum 2, \\(L_2\\)=3\ngravitational acceleration, \\(g=9.81\\)\nmass at the end of the pendula, \\(m=1\\)\ndistance where the coupling spring is mounted, \\(c=2\\)\nspring constant of the coupling spring, \\(k=5\\)\n\n\n\n\n\n\n\nAs compared to our previous problem of a damped driven pendulum, where we had two initial conditions for the second order differential equation, we have now two second order differential equation. We therefore need 4 initial parameters, which are the initial elongations of both pendula and their corresponding initial angular velocities We will notice, that the solution,i.e. the motion of the pendula, will strongly depend on the initial conditions.\n\n\n\n\n\n\n\n\nSolve the equation of motion\nWe have to define a timeperiod over which we would like to obtain the solution. We use here a period of 400s where we calculate the solution at 10000 points along the 400s.\n\n\n\n\n\n\nWe are now ready to calculate the solution. Finally, we extract also the angles of the individual pendula, their angular velocities and the position of the point masses at the end of the pendulum. This can be then readily used to create some animation.\n\n\n\n\n\n\n\n\nPlotting\nFirst, get some impression of how the angles change over time.",
    "crumbs": [
      "Python Basics",
      "Lecture 7",
      "Coupled Pendula"
    ]
  },
  {
    "objectID": "lectures/lecture07/2_coupled_pendula.html#normal-modes",
    "href": "lectures/lecture07/2_coupled_pendula.html#normal-modes",
    "title": "Computer-Based Physical Modelling",
    "section": "Normal Modes",
    "text": "Normal Modes\nWe will not cover all the physical details here, but you might remember your mechanis lectures, that two coupled oscillators show distinct modes of motion, which we would call the normal modes. Four two coupled pendula there are two normal modes, where both pendula move with the same frequency. We may force the system into one of its normal modes, by specifying its initial conditions properly.\n\nIn-phase motion\nThe first one, will create an in-phase motion of the two pendula by setting their initial elongation equal, i.e. \\(\\theta_{1}(t=0)=\\theta_{2}(t=0)\\). Both pendula then oscillate with their natural frequency and the coupling spring is never elongated.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOut-of-phase motion\nThe second one, will create a motion in which the two pendula are out-of-phase by a phase angle of \\(\\pi\\) , i.e. \\(\\theta_{1}(t=0)=-\\theta_{2}(t=0)\\). Both pendula then oscillate with a frequency higher than their natural frequency. This is due to the fact that there is a higher restoring force due to the action of the spring.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBeat case\nThe last case is not a normal mode but represents a more general case. We start with two different initial angles, i.e. \\(\\theta_{1}(t=0)=\\pi/12\\) and \\(\\theta_{2}(t=0)=0\\). This is the so-called beat case, where the pendula exchange energy. The oscillation, which is at the beginning in only in the first pendulum is then transfer to the second one. This transfer of energy is continuously occurring from one pendulum to the other since there’s nowhere for the energy to go. From this point it’s easily to recognize how the wife is generated. In a set of many coupled pendula one pendulum is starting to oscillate and is transferring it’s energy to the next one and then to the next one and then to the next one and this way the energy is propagating along all oscillators.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComputation of energy (here for the beat case)\nAfter we have had a look at the motion of the individual pendula, we may also check, the energies in the system. We have to calculate the potential and kinetic energies of the pendula and we should not forget the potential energy stored in the spring.\n\nPotential energy of the pendula\nThe potential energy plot below nicely shows the exchange of energy between the two pendula in the beat case.\n\n\n\n\n\n\n\n\nPotential energy of the spring\n\n\n\n\n\n\n\n\nKinetic energies\n\n\n\n\n\n\n\n\nTotal energy\nAs the total energy in the system shall nbe conserved, the sum of all energy contributions should yield a flat line.\n\n\n\n\n\n\n\n\nTotal energy exchange of the pendula\nWhile the plot above Shows the total energy of both pendula we may now have a look at the total energy in each pendulum. The plots clearly show that the energy is exchanged between the two pendula. The residual ripples on the curve results from the fact that we here exclude the potential energy stored in the spring.",
    "crumbs": [
      "Python Basics",
      "Lecture 7",
      "Coupled Pendula"
    ]
  },
  {
    "objectID": "lectures/lecture07/2_coupled_pendula.html#summary-and-key-concepts",
    "href": "lectures/lecture07/2_coupled_pendula.html#summary-and-key-concepts",
    "title": "Computer-Based Physical Modelling",
    "section": "Summary and Key Concepts",
    "text": "Summary and Key Concepts\nIn this lecture, we’ve explored coupled pendula as a quintessential example of coupled differential equations. Through this physical system, we’ve uncovered several fundamental concepts in physics and differential equations:\n\nCoupled Differential Equations: We’ve seen how interactions between two pendula lead to a system of coupled ODEs where the motion of each pendulum affects the other.\nNormal Modes: We discovered that coupled systems possess characteristic oscillation patterns called normal modes (in-phase and out-of-phase), where the system behaves like a single harmonic oscillator.\nBeat Phenomenon: When initial conditions excite both normal modes simultaneously, we observed beats—a periodic exchange of energy between the pendula resulting in amplitude modulation.\nEnergy Transfer: We analyzed how energy flows between the pendula through the coupling spring, highlighting conservation of total energy while individual pendulum energies vary.\nNumerical Solutions: We implemented solutions using solve_ivp with the Runge-Kutta method, demonstrating how modern computational techniques can handle complex coupled systems.\n\nThese concepts extend far beyond pendula, appearing in diverse applications:\n\nMolecular vibrations in chemistry and materials science\nCoupled oscillators in electronic circuits\nNormal modes in structural engineering\nResonance phenomena in mechanical systems\nWave propagation in coupled systems\n\nThis concept of normal modes is critical for understanding more complex molecular systems. Below are visualizations of normal modes in two molecular structures:\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure 2: Normal modes in molecular structures. Left: Linear triatomic molecule showing stretching and bending modes. Right: Benzene molecule exhibiting more complex vibrational patterns.\n\n\n\n\n\n\n\n\n\nSelf-Exercise: Exploring Different Coupling Strengths\n\n\n\nTry changing the coupling constant k to see how it affects the motion of the pendula. Use the “beat case” initial conditions (one pendulum displaced, the other at rest) and observe what happens.\n\nStart with our current value (k = 0.5)\nTry a smaller value (k = 0.1)\nTry a larger value (k = 2.0)\n\nWhat differences do you observe in how quickly energy transfers between the pendula?\n\n\n\n\n\n\n\nThe energy transfer between pendula happens faster with stronger coupling (larger k) and slower with weaker coupling (smaller k).\n\n\n\n\n\n\n\n\nSolution",
    "crumbs": [
      "Python Basics",
      "Lecture 7",
      "Coupled Pendula"
    ]
  },
  {
    "objectID": "lectures/consolidated_numerical_methods.html",
    "href": "lectures/consolidated_numerical_methods.html",
    "title": "Numerical Methods for Differential Equations in Physics",
    "section": "",
    "text": "Differential equations form the mathematical backbone of physics, describing how physical quantities change in relation to one another. Whether we’re calculating velocity from position, acceleration from velocity, electric fields, wave propagation, or quantum systems, we’re working with derivatives and their associated differential equations. This document provides a comprehensive approach to numerical solutions for differential equations, starting with numerical differentiation methods and advancing to solving ordinary differential equations (ODEs).\n\n\n\n\n\n\n\n\n\n\n\nBefore diving into numerical methods, let’s revisit the calculus definition of a derivative. The derivative of a function \\(f(x)\\) at a point \\(x\\) is defined as the limit of the difference quotient as the interval \\(\\Delta x\\) approaches zero:\n\\[\nf^{\\prime}(x) = \\lim_{\\Delta x \\rightarrow 0} \\frac{f(x + \\Delta x) - f(x)}{\\Delta x}\n\\]\nThis definition captures the instantaneous rate of change of \\(f\\) with respect to \\(x\\). In physics, derivatives represent essential physical quantities:\n\nThe derivative of position with respect to time is velocity\nThe derivative of velocity with respect to time is acceleration\nThe derivative of potential energy with respect to position gives force\n\nHowever, in computational physics, we cannot take the limit to zero as computers work with discrete values. Instead, we approximate the derivative using finite differences. This is also possible for higher order derivatives, which can be approximated using more complex finite difference formulas such as\n\\[\nf^{(n)}(x)=\\lim _{\\Delta x  \\rightarrow 0} \\frac{1}{\\Delta x ^n} \\sum_{k=0}^n(-1)^{k+n}\\binom{n}{k} f(x+k \\Delta x )\n\\]\n\n\n\nNumerical differentiation methods primarily rely on finite difference approximations derived from Taylor series expansions. Let’s explore these systematically.\n\n\nThe simplest approximation is the forward difference method. It approximates the derivative using the current point and the next point:\n\\[\nf'(x) \\approx \\frac{f(x + \\Delta x) - f(x)}{\\Delta x} + O(\\Delta x)\n\\]\nWhere \\(O(\\Delta x)\\) represents the error term, indicating that the error decreases linearly with the step size.\n\n\n\nThe central difference method uses points on both sides of the current point to approximate the derivative:\n\\[\nf'(x) \\approx \\frac{f(x + \\Delta x) - f(x - \\Delta x)}{2\\Delta x} + O(\\Delta x^2)\n\\]\nThis method has a higher order of accuracy – the error decreases quadratically with the step size.\n\n\n\nFor applications requiring higher accuracy, we can derive higher-order approximations:\n\\[\nf'(x) \\approx \\frac{-f(x + 2\\Delta x) + 8f(x + \\Delta x) - 8f(x - \\Delta x) + f(x - 2\\Delta x)}{12\\Delta x} + O(\\Delta x^4)\n\\]\n\n\n\n\nLet’s implement these methods and compare their accuracies using a known function:\n\n\n\n\n\n\n\n\nThe plot demonstrates how each method’s error behaves as the step size decreases. Initially, the error decreases at the expected rate based on the order of the method. However, for very small step sizes, roundoff errors begin to dominate due to the limitations of floating-point arithmetic.\n\n\n\n\nFor many physical problems, we need to compute derivatives over an entire spatial or temporal domain. In these cases, we can represent differentiation operations as matrix operations.\n\n\nFor a first derivative on a uniform grid with \\(n\\) points, the central difference approximation can be represented as:\n\\[\nD_1 = \\frac{1}{2\\Delta x}\n\\begin{pmatrix}\n0 & 1 & 0 & \\cdots & 0 \\\\\n-1 & 0 & 1 & \\cdots & 0 \\\\\n0 & -1 & 0 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & 0 & \\cdots & 0\n\\end{pmatrix}\n\\]\n\n\n\nSimilarly, the second derivative can be represented as:\n\\[\nD_2 = \\frac{1}{\\Delta x^2}\n\\begin{pmatrix}\n-2 & 1 & 0 & \\cdots & 0 \\\\\n1 & -2 & 1 & \\cdots & 0 \\\\\n0 & 1 & -2 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & 0 & \\cdots & -2\n\\end{pmatrix}\n\\]\n\n\n\n\n\n\n\n\n\n\nIn physical problems, we often need to specify boundary conditions for our derivatives. Common types include:\n\nDirichlet conditions: Specify the function values at boundaries\nNeumann conditions: Specify the derivative values at boundaries\nPeriodic conditions: The function values and derivatives match at opposite boundaries\n\nThe choice of boundary conditions affects how we construct our derivative matrices, especially at the edges of the domain.\n\n\n\n\nNow that we understand numerical differentiation, we can apply these techniques to solve ordinary differential equations (ODEs), which are ubiquitous in physics.\n\n\nLet’s start with a classic physical system: the harmonic oscillator. The equation of motion is:\n\\[\n\\frac{d^2x}{dt^2} + \\omega^2 x = 0\n\\]\nwhere \\(\\omega\\) is the angular frequency of the oscillator.\n\n\n\n\n\n\nNote\n\n\n\nThis is a second order differential equation which requires two initial conditions for its solution: the initial elongation \\(x(t=0)=x_{0}\\) and the initial velocity \\(\\dot{x}(t=0)=v_{0}\\).\n\n\n\n\n\nUsing the matrix representation of the second derivative that we developed earlier, we can transform the ODE into a system of linear equations that can be solved implicitly.\n\n\nOur matrix will consist of two parts. The first containing the second derivative and the second just the elongation. Suppose we want to calculate the position \\(x(t)\\) at 6 instances in time \\(t_{i}\\), then the matrix version of the second derivative reads as:\n\\(T=\\frac{d^2x}{dt^2}=\\frac{1}{\\delta t^2}\n\\begin{bmatrix}\n-2 & 1  & 0 & 0 & 0 & 0\\\\\n1 & -2 & 1 & 0 & 0 & 0\\\\\n0 & 1  & -2 & 1 & 0 & 0\\\\\n0 & 0  & 1  & -2 & 1 & 0\\\\\n0 & 0  & 0  &  1 & -2 & 1\\\\\n0 & 0  & 0  &  0 &  1 & -2\\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nx_{1}\\\\\nx_{2}\\\\\nx_{3}\\\\\nx_{4}\\\\\nx_{5}\\\\\nx_{6}\n\\end{bmatrix}\\)\nThe second term in the equation of motion can be represented as:\n\\(V=\\omega^2 x=\\begin{bmatrix}\n\\omega^2  & 0  & 0 & 0 & 0 & 0\\\\\n0 & \\omega^2  & 0 & 0 & 0 & 0\\\\\n0 & 0  & \\omega^2  & 0 & 0 & 0\\\\\n0 & 0  & 0  & \\omega^2  & 0 & 0\\\\\n0 & 0  & 0  &  0 & \\omega^2  & 0\\\\\n0 & 0  & 0  &  0 &  0 & \\omega^2\n\\end{bmatrix}\n\\begin{bmatrix}\nx_{1}\\\\\nx_{2}\\\\\nx_{3}\\\\\nx_{4}\\\\\nx_{5}\\\\\nx_{6}\n\\end{bmatrix}\\)\nThe equation of motion can then be written as \\(T \\cdot x + V \\cdot x = 0\\), or \\((T + V) \\cdot x = 0\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\nInstead of solving the entire system implicitly, we can use step-by-step numerical integration methods. These methods are particularly useful for non-linear ODEs.\n\n\nWe can convert any second-order ODE to a system of first-order ODEs by introducing additional variables. For the harmonic oscillator:\n\\[\n\\frac{d^2x}{dt^2} + \\omega^2 x = 0\n\\]\nWe introduce \\(v = \\frac{dx}{dt}\\) and rewrite as:\n\\[\n\\begin{align}\n\\frac{dx}{dt} &= v \\\\\n\\frac{dv}{dt} &= -\\omega^2 x\n\\end{align}\n\\]\n\n\n\nThe simplest numerical integration method is the Euler method:\n\\[\n\\begin{align}\nx_{n+1} &= x_n + v_n \\Delta t \\\\\nv_{n+1} &= v_n - \\omega^2 x_n \\Delta t\n\\end{align}\n\\]\n\n\n\n\n\n\n\n\n\nThe Euler-Cromer method is more stable for oscillatory systems:\n\\[\n\\begin{align}\nv_{n+1} &= v_n - \\omega^2 x_n \\Delta t \\\\\nx_{n+1} &= x_n + v_{n+1} \\Delta t\n\\end{align}\n\\]\n\n\n\n\n\n\n\n\n\nThe midpoint method has better accuracy:\n\\[\n\\begin{align}\nk_1 &= f(t_n, y_n) \\\\\nk_2 &= f(t_n + \\frac{\\Delta t}{2}, y_n + \\frac{\\Delta t}{2}k_1) \\\\\ny_{n+1} &= y_n + \\Delta t \\cdot k_2\n\\end{align}\n\\]\n\n\n\n\n\n\n\n\n\nLet’s compare these methods for the harmonic oscillator:\n\n\n\n\n\n\n\n\n\n\nFor practical applications, the SciPy library provides sophisticated ODE solvers:\n\n\n\n\n\n\n\n\n\n\nLet’s apply our knowledge to a more complex system: a damped driven pendulum. The equation of motion is:\n\\[\n\\frac{d^2\\theta}{dt^2} + b\\frac{d\\theta}{dt} + \\omega_0^2\\sin\\theta = F_0\\cos(\\omega_d t)\n\\]\nwhere \\(\\theta\\) is the angle, \\(b\\) is the damping coefficient, \\(\\omega_0\\) is the natural frequency, \\(F_0\\) is the driving amplitude, and \\(\\omega_d\\) is the driving frequency.\n\n\n\n\n\n\n\n\n\nIn this comprehensive exploration, we’ve covered numerical methods for differentiation and integration, and applied them to solve ordinary differential equations. We’ve progressed from basic concepts to advanced applications, providing a solid foundation for numerical methods in physics.\nThe matrix-based approach is particularly powerful for linear problems, while explicit integration methods are versatile for a wide range of ODEs. For complex problems, SciPy’s ODE solvers provide robust and efficient solutions.\n\n\n\n\nImplement higher-order Runge-Kutta methods and compare their accuracy\nSolve a coupled oscillator system (two or more oscillators connected by springs)\nExplore chaotic behavior in the damped driven pendulum by varying parameters\nImplement adaptive step size methods for improved efficiency\nApply these methods to specific physics problems in your area of interest\n\n\n\n\n\nPress, W. H., Teukolsky, S. A., Vetterling, W. T., & Flannery, B. P. (2007). Numerical Recipes: The Art of Scientific Computing (3rd ed.). Cambridge University Press.\nBurden, R. L., & Faires, J. D. (2015). Numerical Analysis (10th ed.). Cengage Learning.\nHairer, E., Nørsett, S. P., & Wanner, G. (2008). Solving Ordinary Differential Equations I: Nonstiff Problems. Springer.\nLeVeque, R. J. (2007). Finite Difference Methods for Ordinary and Partial Differential Equations: Steady-State and Time-Dependent Problems. SIAM.\nLandau, R. H., Páez, M. J., & Bordeianu, C. C. (2015). Computational Physics: Problem Solving with Python (3rd ed.). Wiley-VCH.\nNewman, M. (2012). Computational Physics. CreateSpace Independent Publishing Platform."
  },
  {
    "objectID": "lectures/consolidated_numerical_methods.html#introduction",
    "href": "lectures/consolidated_numerical_methods.html#introduction",
    "title": "Numerical Methods for Differential Equations in Physics",
    "section": "",
    "text": "Differential equations form the mathematical backbone of physics, describing how physical quantities change in relation to one another. Whether we’re calculating velocity from position, acceleration from velocity, electric fields, wave propagation, or quantum systems, we’re working with derivatives and their associated differential equations. This document provides a comprehensive approach to numerical solutions for differential equations, starting with numerical differentiation methods and advancing to solving ordinary differential equations (ODEs)."
  },
  {
    "objectID": "lectures/consolidated_numerical_methods.html#part-1-numerical-differentiation",
    "href": "lectures/consolidated_numerical_methods.html#part-1-numerical-differentiation",
    "title": "Numerical Methods for Differential Equations in Physics",
    "section": "",
    "text": "Before diving into numerical methods, let’s revisit the calculus definition of a derivative. The derivative of a function \\(f(x)\\) at a point \\(x\\) is defined as the limit of the difference quotient as the interval \\(\\Delta x\\) approaches zero:\n\\[\nf^{\\prime}(x) = \\lim_{\\Delta x \\rightarrow 0} \\frac{f(x + \\Delta x) - f(x)}{\\Delta x}\n\\]\nThis definition captures the instantaneous rate of change of \\(f\\) with respect to \\(x\\). In physics, derivatives represent essential physical quantities:\n\nThe derivative of position with respect to time is velocity\nThe derivative of velocity with respect to time is acceleration\nThe derivative of potential energy with respect to position gives force\n\nHowever, in computational physics, we cannot take the limit to zero as computers work with discrete values. Instead, we approximate the derivative using finite differences. This is also possible for higher order derivatives, which can be approximated using more complex finite difference formulas such as\n\\[\nf^{(n)}(x)=\\lim _{\\Delta x  \\rightarrow 0} \\frac{1}{\\Delta x ^n} \\sum_{k=0}^n(-1)^{k+n}\\binom{n}{k} f(x+k \\Delta x )\n\\]\n\n\n\nNumerical differentiation methods primarily rely on finite difference approximations derived from Taylor series expansions. Let’s explore these systematically.\n\n\nThe simplest approximation is the forward difference method. It approximates the derivative using the current point and the next point:\n\\[\nf'(x) \\approx \\frac{f(x + \\Delta x) - f(x)}{\\Delta x} + O(\\Delta x)\n\\]\nWhere \\(O(\\Delta x)\\) represents the error term, indicating that the error decreases linearly with the step size.\n\n\n\nThe central difference method uses points on both sides of the current point to approximate the derivative:\n\\[\nf'(x) \\approx \\frac{f(x + \\Delta x) - f(x - \\Delta x)}{2\\Delta x} + O(\\Delta x^2)\n\\]\nThis method has a higher order of accuracy – the error decreases quadratically with the step size.\n\n\n\nFor applications requiring higher accuracy, we can derive higher-order approximations:\n\\[\nf'(x) \\approx \\frac{-f(x + 2\\Delta x) + 8f(x + \\Delta x) - 8f(x - \\Delta x) + f(x - 2\\Delta x)}{12\\Delta x} + O(\\Delta x^4)\n\\]\n\n\n\n\nLet’s implement these methods and compare their accuracies using a known function:\n\n\n\n\n\n\n\n\nThe plot demonstrates how each method’s error behaves as the step size decreases. Initially, the error decreases at the expected rate based on the order of the method. However, for very small step sizes, roundoff errors begin to dominate due to the limitations of floating-point arithmetic.\n\n\n\n\nFor many physical problems, we need to compute derivatives over an entire spatial or temporal domain. In these cases, we can represent differentiation operations as matrix operations.\n\n\nFor a first derivative on a uniform grid with \\(n\\) points, the central difference approximation can be represented as:\n\\[\nD_1 = \\frac{1}{2\\Delta x}\n\\begin{pmatrix}\n0 & 1 & 0 & \\cdots & 0 \\\\\n-1 & 0 & 1 & \\cdots & 0 \\\\\n0 & -1 & 0 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & 0 & \\cdots & 0\n\\end{pmatrix}\n\\]\n\n\n\nSimilarly, the second derivative can be represented as:\n\\[\nD_2 = \\frac{1}{\\Delta x^2}\n\\begin{pmatrix}\n-2 & 1 & 0 & \\cdots & 0 \\\\\n1 & -2 & 1 & \\cdots & 0 \\\\\n0 & 1 & -2 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & 0 & \\cdots & -2\n\\end{pmatrix}\n\\]\n\n\n\n\n\n\n\n\n\n\nIn physical problems, we often need to specify boundary conditions for our derivatives. Common types include:\n\nDirichlet conditions: Specify the function values at boundaries\nNeumann conditions: Specify the derivative values at boundaries\nPeriodic conditions: The function values and derivatives match at opposite boundaries\n\nThe choice of boundary conditions affects how we construct our derivative matrices, especially at the edges of the domain."
  },
  {
    "objectID": "lectures/consolidated_numerical_methods.html#part-2-solving-ordinary-differential-equations",
    "href": "lectures/consolidated_numerical_methods.html#part-2-solving-ordinary-differential-equations",
    "title": "Numerical Methods for Differential Equations in Physics",
    "section": "",
    "text": "Now that we understand numerical differentiation, we can apply these techniques to solve ordinary differential equations (ODEs), which are ubiquitous in physics.\n\n\nLet’s start with a classic physical system: the harmonic oscillator. The equation of motion is:\n\\[\n\\frac{d^2x}{dt^2} + \\omega^2 x = 0\n\\]\nwhere \\(\\omega\\) is the angular frequency of the oscillator.\n\n\n\n\n\n\nNote\n\n\n\nThis is a second order differential equation which requires two initial conditions for its solution: the initial elongation \\(x(t=0)=x_{0}\\) and the initial velocity \\(\\dot{x}(t=0)=v_{0}\\).\n\n\n\n\n\nUsing the matrix representation of the second derivative that we developed earlier, we can transform the ODE into a system of linear equations that can be solved implicitly.\n\n\nOur matrix will consist of two parts. The first containing the second derivative and the second just the elongation. Suppose we want to calculate the position \\(x(t)\\) at 6 instances in time \\(t_{i}\\), then the matrix version of the second derivative reads as:\n\\(T=\\frac{d^2x}{dt^2}=\\frac{1}{\\delta t^2}\n\\begin{bmatrix}\n-2 & 1  & 0 & 0 & 0 & 0\\\\\n1 & -2 & 1 & 0 & 0 & 0\\\\\n0 & 1  & -2 & 1 & 0 & 0\\\\\n0 & 0  & 1  & -2 & 1 & 0\\\\\n0 & 0  & 0  &  1 & -2 & 1\\\\\n0 & 0  & 0  &  0 &  1 & -2\\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nx_{1}\\\\\nx_{2}\\\\\nx_{3}\\\\\nx_{4}\\\\\nx_{5}\\\\\nx_{6}\n\\end{bmatrix}\\)\nThe second term in the equation of motion can be represented as:\n\\(V=\\omega^2 x=\\begin{bmatrix}\n\\omega^2  & 0  & 0 & 0 & 0 & 0\\\\\n0 & \\omega^2  & 0 & 0 & 0 & 0\\\\\n0 & 0  & \\omega^2  & 0 & 0 & 0\\\\\n0 & 0  & 0  & \\omega^2  & 0 & 0\\\\\n0 & 0  & 0  &  0 & \\omega^2  & 0\\\\\n0 & 0  & 0  &  0 &  0 & \\omega^2\n\\end{bmatrix}\n\\begin{bmatrix}\nx_{1}\\\\\nx_{2}\\\\\nx_{3}\\\\\nx_{4}\\\\\nx_{5}\\\\\nx_{6}\n\\end{bmatrix}\\)\nThe equation of motion can then be written as \\(T \\cdot x + V \\cdot x = 0\\), or \\((T + V) \\cdot x = 0\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\nInstead of solving the entire system implicitly, we can use step-by-step numerical integration methods. These methods are particularly useful for non-linear ODEs.\n\n\nWe can convert any second-order ODE to a system of first-order ODEs by introducing additional variables. For the harmonic oscillator:\n\\[\n\\frac{d^2x}{dt^2} + \\omega^2 x = 0\n\\]\nWe introduce \\(v = \\frac{dx}{dt}\\) and rewrite as:\n\\[\n\\begin{align}\n\\frac{dx}{dt} &= v \\\\\n\\frac{dv}{dt} &= -\\omega^2 x\n\\end{align}\n\\]\n\n\n\nThe simplest numerical integration method is the Euler method:\n\\[\n\\begin{align}\nx_{n+1} &= x_n + v_n \\Delta t \\\\\nv_{n+1} &= v_n - \\omega^2 x_n \\Delta t\n\\end{align}\n\\]\n\n\n\n\n\n\n\n\n\nThe Euler-Cromer method is more stable for oscillatory systems:\n\\[\n\\begin{align}\nv_{n+1} &= v_n - \\omega^2 x_n \\Delta t \\\\\nx_{n+1} &= x_n + v_{n+1} \\Delta t\n\\end{align}\n\\]\n\n\n\n\n\n\n\n\n\nThe midpoint method has better accuracy:\n\\[\n\\begin{align}\nk_1 &= f(t_n, y_n) \\\\\nk_2 &= f(t_n + \\frac{\\Delta t}{2}, y_n + \\frac{\\Delta t}{2}k_1) \\\\\ny_{n+1} &= y_n + \\Delta t \\cdot k_2\n\\end{align}\n\\]\n\n\n\n\n\n\n\n\n\nLet’s compare these methods for the harmonic oscillator:\n\n\n\n\n\n\n\n\n\n\nFor practical applications, the SciPy library provides sophisticated ODE solvers:"
  },
  {
    "objectID": "lectures/consolidated_numerical_methods.html#advanced-example-damped-driven-pendulum",
    "href": "lectures/consolidated_numerical_methods.html#advanced-example-damped-driven-pendulum",
    "title": "Numerical Methods for Differential Equations in Physics",
    "section": "",
    "text": "Let’s apply our knowledge to a more complex system: a damped driven pendulum. The equation of motion is:\n\\[\n\\frac{d^2\\theta}{dt^2} + b\\frac{d\\theta}{dt} + \\omega_0^2\\sin\\theta = F_0\\cos(\\omega_d t)\n\\]\nwhere \\(\\theta\\) is the angle, \\(b\\) is the damping coefficient, \\(\\omega_0\\) is the natural frequency, \\(F_0\\) is the driving amplitude, and \\(\\omega_d\\) is the driving frequency."
  },
  {
    "objectID": "lectures/consolidated_numerical_methods.html#conclusion",
    "href": "lectures/consolidated_numerical_methods.html#conclusion",
    "title": "Numerical Methods for Differential Equations in Physics",
    "section": "",
    "text": "In this comprehensive exploration, we’ve covered numerical methods for differentiation and integration, and applied them to solve ordinary differential equations. We’ve progressed from basic concepts to advanced applications, providing a solid foundation for numerical methods in physics.\nThe matrix-based approach is particularly powerful for linear problems, while explicit integration methods are versatile for a wide range of ODEs. For complex problems, SciPy’s ODE solvers provide robust and efficient solutions."
  },
  {
    "objectID": "lectures/consolidated_numerical_methods.html#what-to-try-yourself",
    "href": "lectures/consolidated_numerical_methods.html#what-to-try-yourself",
    "title": "Numerical Methods for Differential Equations in Physics",
    "section": "",
    "text": "Implement higher-order Runge-Kutta methods and compare their accuracy\nSolve a coupled oscillator system (two or more oscillators connected by springs)\nExplore chaotic behavior in the damped driven pendulum by varying parameters\nImplement adaptive step size methods for improved efficiency\nApply these methods to specific physics problems in your area of interest"
  },
  {
    "objectID": "lectures/consolidated_numerical_methods.html#further-reading",
    "href": "lectures/consolidated_numerical_methods.html#further-reading",
    "title": "Numerical Methods for Differential Equations in Physics",
    "section": "",
    "text": "Press, W. H., Teukolsky, S. A., Vetterling, W. T., & Flannery, B. P. (2007). Numerical Recipes: The Art of Scientific Computing (3rd ed.). Cambridge University Press.\nBurden, R. L., & Faires, J. D. (2015). Numerical Analysis (10th ed.). Cengage Learning.\nHairer, E., Nørsett, S. P., & Wanner, G. (2008). Solving Ordinary Differential Equations I: Nonstiff Problems. Springer.\nLeVeque, R. J. (2007). Finite Difference Methods for Ordinary and Partial Differential Equations: Steady-State and Time-Dependent Problems. SIAM.\nLandau, R. H., Páez, M. J., & Bordeianu, C. C. (2015). Computational Physics: Problem Solving with Python (3rd ed.). Wiley-VCH.\nNewman, M. (2012). Computational Physics. CreateSpace Independent Publishing Platform."
  },
  {
    "objectID": "lectures/lecture05/2_integration_part2.html",
    "href": "lectures/lecture05/2_integration_part2.html",
    "title": "Advanced Topics in Numerical Integration",
    "section": "",
    "text": "Adaptive Integration Methods\nThe methods we’ve discussed so far use equal spacing between sampling points. However, most real-world physics problems involve functions that vary dramatically across the integration range. Adaptive methods adjust the point distribution to concentrate more points where the function changes rapidly.\n\n\n\n\n\n\n\n\nMulti-dimensional Integration\nMany physics problems require integration over multiple dimensions, such as calculating mass moments of inertia, electric fields from volume charge distributions, or statistical mechanics partition functions.\nFor 2D integration, we can extend our 1D methods using the concept of iterated integrals:\n\\[\\begin{equation}\n\\int_{a}^{b}\\int_{c}^{d} f(x,y) dy dx \\approx \\sum_{i=1}^{N_x} \\sum_{j=1}^{N_y} w_i w_j f(x_i, y_j)\n\\end{equation}\\]\nWhere \\(w_i\\) and \\(w_j\\) are the weights for the respective 1D methods.\n\n\n\n\n\n\n\n\nMonte Carlo Integration\nFor higher-dimensional integrals and complex domains, Monte Carlo methods become increasingly efficient. These methods use random sampling to approximate integrals and are particularly valuable in quantum and statistical physics.\n\n\n\n\n\n\n\n\nApplication to Real Physics Problems\nLet’s examine two common scenarios in physics that benefit from numerical integration:\n\nNon-uniform Magnetic Field: When a charged particle moves through a non-uniform magnetic field, the work done can be calculated as:\n\\[W = q\\int_{\\vec{r}_1}^{\\vec{r}_2} \\vec{v} \\times \\vec{B}(\\vec{r}) \\cdot d\\vec{r}\\]\nQuantum Tunneling: The tunneling probability through a potential barrier is given by:\n\\[T \\approx \\exp\\left(-\\frac{2}{\\hbar}\\int_{x_1}^{x_2} \\sqrt{2m(V(x) - E)}\\, dx\\right)\\]\n\nIn both cases, the integrals frequently cannot be solved analytically due to the complex spatial dependence of the fields or potentials, making numerical integration indispensable for modern physics."
  },
  {
    "objectID": "lectures/lecture05/trapezoid_short.html",
    "href": "lectures/lecture05/trapezoid_short.html",
    "title": "Trapezoid Method",
    "section": "",
    "text": "Theory and Implementation\n\n\n\nTrapezoid Method Illustration\n\n\nThe Trapezoid method improves upon the Box method by approximating the function with linear segments between consecutive points. Instead of using constant values within each interval, it connects adjacent points with straight lines, forming trapezoids.\nThe mathematical formula for the Trapezoid method is:\n\\[\\begin{equation}\n\\int_{a}^{b} f(x) dx \\approx \\sum_{i=1}^{N-1} \\frac{f(x_i) + f(x_{i+1})}{2} \\Delta x\n\\end{equation}\\]\nWhere: - \\(\\Delta x = \\frac{b-a}{N-1}\\) is the width of each interval - \\(x_i\\) are the sample points\nThis method is particularly effective for smoothly varying functions, which are common in physical systems.\n\n\n\n\n\n\n\n\nPhysics Application: Electric Potential Calculation\nThe Trapezoid method is well-suited for calculating the electric potential due to a charge distribution. For a linear charge density \\(\\lambda(x)\\) along the x-axis, the potential at point \\((0,d)\\) is given by:\n\\[\\begin{equation}\nV(0,d) = \\frac{1}{4\\pi\\epsilon_0} \\int_{a}^{b} \\frac{\\lambda(x)}{\\sqrt{x^2 + d^2}} dx\n\\end{equation}\\]\nThis integral can be efficiently computed using the Trapezoid method, especially when the charge distribution is known at discrete points."
  },
  {
    "objectID": "lectures/lecture05/1-differentiation.html",
    "href": "lectures/lecture05/1-differentiation.html",
    "title": "Computer-Based Physical Modelling",
    "section": "",
    "text": "Derivatives form the mathematical backbone of physics. Whether we’re calculating velocity from position, acceleration from velocity, or electric field from potential, we’re computing derivatives. While calculus provides us with analytical tools to compute derivatives, many real-world physics problems involve functions that are either too complex for analytical solutions or are only known at discrete points (experimental data). This is where numerical differentiation becomes essential for physicists.\n\n\n\n\n\n\n\n\nBefore diving into numerical methods, let’s revisit the calculus definition of a derivative. The derivative of a function \\(f(x)\\) at a point \\(x\\) is defined as the limit of the difference quotient as the interval \\(\\Delta x\\) approaches zero:\n\\[\nf^{\\prime}(x) = \\lim_{\\Delta x \\rightarrow 0} \\frac{f(x + \\Delta x) - f(x)}{\\Delta x}\n\\]\nThis definition captures the instantaneous rate of change of \\(f\\) with respect to \\(x\\). In physics, derivatives represent essential physical quantities:\n\nThe derivative of position with respect to time is velocity\nThe derivative of velocity with respect to time is acceleration\nThe derivative of potential energy with respect to position gives force\n\nHowever, in computational physics, we cannot take the limit to zero as computers work with discrete values. Instead, we approximate the derivative using finite differences. This is also possible for higher order derivatives, which can be approximated using more complex finite difference formulas such as\n\\[\nf^{(n)}(x)=\\lim _{\\Delta x  \\rightarrow 0} \\frac{1}{\\Delta x ^n} \\sum_{k=0}^n(-1)^{k+n}\\binom{n}{k} f(x+k \\Delta x )\n\\]\n\n\n\nNumerical differentiation methods primarily rely on finite difference approximations derived from Taylor series expansions. Let’s explore these systematically.\n\n\nThe simplest approximation comes directly from the definition, where we look at the change in function value as we move forward from the current point:\n\\[\nf^{\\prime}_{i} \\approx \\frac{f_{i+1} - f_{i}}{\\Delta x}\n\\]\nThis is called the forward difference method. To understand its accuracy, we can analyze the error using Taylor expansion. The resulting local error \\(\\delta\\) at each calculation is:\n\\[\n\\delta = f_{i+1} - f_{i} - \\Delta x f^{\\prime}(x_i) = \\frac{1}{2} \\Delta x^2 f^{\\prime \\prime}(x_i) + O(\\Delta x^3)\n\\]\nWe observe that while the local truncation error is proportional to \\(\\Delta x^2\\), the accumulated global error is proportional to \\(\\Delta x\\), making this a first-order accurate method. This means that halving the step size will approximately halve the error in our final derivative approximation.\n\n\n\n\n\n\nLocal vs. Global Error\n\n\n\nLocal truncation error refers to the error introduced in a single step of the numerical method due to truncating the Taylor series. For the forward difference method, this error is \\(O(\\Delta x^2)\\).\nGlobal accumulated error is the total error that accumulates as we apply the method repeatedly across the domain. For the forward difference method, this accumulated error is \\(O(\\Delta x)\\). Global error is generally one order less accurate than the local error due to error propagation through multiple steps.\n\n\n\n\n\nWe can derive a more accurate approximation by using function values on both sides of the point of interest. Using Taylor expansions for \\(f(x+\\Delta x)\\) and \\(f(x-\\Delta x)\\):\n\\[\nf_{i+1} = f_{i} + \\Delta x f_{i}^{\\prime} + \\frac{\\Delta x^2}{2!} f_{i}^{\\prime\\prime} + \\frac{\\Delta x^3}{3!} f_{i}^{(3)} + \\ldots\n\\]\n\\[\nf_{i-1} = f_{i} - \\Delta x f_{i}^{\\prime} + \\frac{\\Delta x^2}{2!} f_{i}^{\\prime\\prime} - \\frac{\\Delta x^3}{3!} f_{i}^{(3)} + \\ldots\n\\]\nSubtracting these equations cancels out the even-powered terms in \\(\\Delta x\\):\n\\[\nf_{i+1} - f_{i-1} = 2 \\Delta x f_{i}^{\\prime} + O(\\Delta x^3)\n\\]\nSolving for \\(f^{\\prime}_{i}\\):\n\\[\nf^{\\prime}_{i} \\approx \\frac{f_{i+1} - f_{i-1}}{2 \\Delta x}\n\\]\nThis central difference formula has an error proportional to \\(\\Delta x^2\\), making it second-order accurate—significantly more precise than the forward difference method.\n\n\n\nWe can extend this approach to derive higher-order approximations by including more points in our calculation. A common fourth-order accurate formula for the first derivative is:\n\\[\nf_{i}^{\\prime}=\\frac{1}{12 \\Delta x}(-f_{i-2}+8f_{i-1}-8f_{i+1}+f_{i+2})\n\\]\nThis formula provides even better accuracy but requires function values at four points.\n\n\n\nThe following table summarizes the key finite difference methods for first derivatives:\n\n\n\n\n\n\n\n\n\nMethod\nFormula\nOrder of Accuracy\nPoints Required\n\n\n\n\nForward Difference\n\\(\\frac{f_{i+1} - f_{i}}{\\Delta x}\\)\n\\(O(\\Delta x)\\)\n2\n\n\nBackward Difference\n\\(\\frac{f_{i} - f_{i-1}}{\\Delta x}\\)\n\\(O(\\Delta x)\\)\n2\n\n\nCentral Difference\n\\(\\frac{f_{i+1} - f_{i-1}}{2\\Delta x}\\)\n\\(O(\\Delta x^2)\\)\n3\n\n\nFourth-Order Central\n\\(\\frac{-f_{i+2}+8f_{i+1}-8f_{i-1}+f_{i-2}}{12\\Delta x}\\)\n\\(O(\\Delta x^4)\\)\n5\n\n\n\nHigher-order methods generally provide more accurate results but require more computational resources and handle boundaries less efficiently.\n\n\n\n\nLet’s implement these numerical differentiation methods in Python, starting with a central difference function:\n\n\n\n\n\n\nWe can test these functions with \\(\\sin(x)\\), whose derivative is \\(\\cos(x)\\):\n\n\n\n\n\n\n\n\nLet’s examine how the error in our numerical derivative varies with the step size \\(\\Delta x\\):\n\n\n\n\n\n\nThis visualization demonstrates how error behaves with step size for different methods. For very small step sizes, roundoff errors become significant (observe the upturn in error for tiny \\(\\Delta x\\) values), while for larger steps, truncation error dominates.\n\n\n\n\nAn elegant approach to numerical differentiation involves representing the differentiation operation as a matrix multiplication. This representation is particularly valuable when solving differential equations numerically.\n\n\n\nFor a uniformly spaced grid of points \\(x_i\\), we can represent the first derivative operation as a matrix:\n\\[\nf^{\\prime} = \\frac{1}{\\Delta x}\n\\begin{bmatrix}\n-1 & 1  & 0 & 0 & \\cdots & 0\\\\\n0 & -1 & 1 & 0 & \\cdots & 0\\\\\n0 & 0  & -1 & 1 & \\cdots & 0\\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\ddots & \\vdots\\\\\n0 & 0  & 0  & \\cdots & -1 & 1\\\\\n0 & 0  & 0  & \\cdots &  0 & -1\\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nf_{1}\\\\\nf_{2}\\\\\nf_{3}\\\\\n\\vdots\\\\\nf_{N-1}\\\\\nf_{N}\\\\\n\\end{bmatrix}\n\\]\nThis matrix implements the forward difference scheme. For a central difference scheme, the matrix would have entries on both sides of the diagonal.\n\n\nSimilarly, the second derivative can be represented as a tridiagonal matrix:\n\\[\nf^{\\prime\\prime} = \\frac{1}{\\Delta x^2}\n\\begin{bmatrix}\n1 & -2 & 1 & 0 & \\cdots & 0\\\\\n0 & 1 & -2 & 1 & \\cdots & 0\\\\\n0 & 0 & 1 & -2 & \\cdots & 0\\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\ddots & \\vdots\\\\\n0 & 0 & 0 & \\cdots & 1 & -2 & 1\\\\\n0 & 0 & 0 & \\cdots & 0 & 1 & -2\\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nf_{1}\\\\\nf_{2}\\\\\nf_{3}\\\\\n\\vdots\\\\\nf_{N-1}\\\\\nf_{N}\\\\\n\\end{bmatrix}\n\\]\nThe boundary conditions affect the structure of these matrices, especially the first and last rows.\n\n\n\nSciPy provides tools to efficiently construct and work with these differentiation matrices:\n\n\n\n\n\n\n\n\n\n\nA critical consideration in numerical differentiation is how to handle the boundaries of the domain. Different approaches include:\n\nOne-sided differences: Using forward differences at the left boundary and backward differences at the right boundary.\nExtrapolation: Extending the domain by extrapolating function values beyond the boundaries.\nPeriodic boundaries: For periodic functions, using values from the opposite end of the domain.\nGhost points: Introducing additional points outside the domain whose values are determined by the boundary conditions.\n\nThe choice of boundary treatment depends on the physical problem and can significantly impact the accuracy of the solution.\n\n\n\nNumerical differentiation is foundational to computational physics. Let’s explore some specific applications:\n\n\nMany physics problems are formulated as differential equations. For example, the one-dimensional time-dependent Schrödinger equation:\n\\[\ni\\hbar\\frac{\\partial}{\\partial t}\\Psi(x,t) = -\\frac{\\hbar^2}{2m}\\frac{\\partial^2}{\\partial x^2}\\Psi(x,t) + V(x)\\Psi(x,t)\n\\]\nNumerical differentiation allows us to approximate the spatial derivatives, reducing this to a system of ordinary differential equations in time.\n\n\n\nWhen working with experimental measurements, we often need to calculate derivatives from discrete data points. For instance, determining the velocity and acceleration of an object from position measurements.\n\n\n\n\n\n\nNotice how noise in the position measurements gets amplified in the velocity calculations. This highlights a key challenge in numerical differentiation: sensitivity to noise.\n\n\n\nIn electrostatics, the electric field \\(\\vec{E}\\) is related to the electric potential \\(\\phi\\) by \\(\\vec{E} = -\\nabla \\phi\\). Numerical differentiation allows us to calculate the electric field from a known potential distribution.\n\n\n\n\n\n\n\n\n\n\n\n\nChoosing an appropriate step size is crucial for numerical differentiation. If \\(\\Delta x\\) is too large, the truncation error becomes significant. If \\(\\Delta x\\) is too small, roundoff errors dominate. A general approach is to use:\n\\[ \\Delta x \\approx \\sqrt{\\epsilon_\\text{machine}} \\times x \\]\nwhere \\(\\epsilon_\\text{machine}\\) is the machine epsilon (approximately \\(10^{-16}\\) for double precision).\n\n\n\nNumerical differentiation amplifies noise in the data. Several techniques can help:\n\nSmoothing: Apply a filter to the data before differentiation.\nRegularization: Use methods that inherently provide some smoothing.\nSavitzky-Golay filters: Combine local polynomial fitting with differentiation.\n\n\n\n\nIn physical simulations, preserving conservation laws (energy, momentum, etc.) is often crucial. Some numerical differentiation schemes conserve these properties better than others.\n\n\n\n\nThe SciPy library provides convenient functions for numerical differentiation:\n\n\n\n\n\n\nThe order parameter controls the accuracy of the approximation by using more points in the calculation.\n\n\n\nNumerical differentiation is a fundamental technique in computational physics, bridging the gap between theoretical models and practical computations. By understanding the principles, methods, and challenges of numerical differentiation, physicists can effectively analyze data, solve differential equations, and simulate physical systems.\nThe methods we’ve explored—from simple finite differences to matrix representations—provide a comprehensive toolkit for tackling a wide range of physics problems. As you apply these techniques, remember that the choice of method should be guided by the specific requirements of your problem: accuracy needs, computational constraints, and the nature of your data.\n\n\n\n\nImplement and compare the accuracy of different numerical differentiation schemes for the function \\(f(x) = e^{-x^2}\\).\nInvestigate how noise in the input data affects the accuracy of numerical derivatives and explore techniques to mitigate this effect.\nCalculate the electric field around two point charges using numerical differentiation of the electric potential.\nAnalyze experimental data from a falling object to determine its acceleration, and compare with the expected value of gravitational acceleration.\n\n\n\n\n\n\n\nFurther Reading\n\n\n\n\nNumerical Recipes: The Art of Scientific Computing by Press, Teukolsky, Vetterling, and Flannery\nNumerical Methods for Physics by Alejandro Garcia\nComputational Physics by Mark Newman\nApplied Numerical Analysis by Curtis F. Gerald and Patrick O. Wheatley",
    "crumbs": [
      "Python Basics",
      "Lecture 5",
      "Numerical Differentiation"
    ]
  },
  {
    "objectID": "lectures/lecture05/1-differentiation.html#numerical-differentiation-for-physics",
    "href": "lectures/lecture05/1-differentiation.html#numerical-differentiation-for-physics",
    "title": "Computer-Based Physical Modelling",
    "section": "",
    "text": "Derivatives form the mathematical backbone of physics. Whether we’re calculating velocity from position, acceleration from velocity, or electric field from potential, we’re computing derivatives. While calculus provides us with analytical tools to compute derivatives, many real-world physics problems involve functions that are either too complex for analytical solutions or are only known at discrete points (experimental data). This is where numerical differentiation becomes essential for physicists.\n\n\n\n\n\n\n\n\nBefore diving into numerical methods, let’s revisit the calculus definition of a derivative. The derivative of a function \\(f(x)\\) at a point \\(x\\) is defined as the limit of the difference quotient as the interval \\(\\Delta x\\) approaches zero:\n\\[\nf^{\\prime}(x) = \\lim_{\\Delta x \\rightarrow 0} \\frac{f(x + \\Delta x) - f(x)}{\\Delta x}\n\\]\nThis definition captures the instantaneous rate of change of \\(f\\) with respect to \\(x\\). In physics, derivatives represent essential physical quantities:\n\nThe derivative of position with respect to time is velocity\nThe derivative of velocity with respect to time is acceleration\nThe derivative of potential energy with respect to position gives force\n\nHowever, in computational physics, we cannot take the limit to zero as computers work with discrete values. Instead, we approximate the derivative using finite differences. This is also possible for higher order derivatives, which can be approximated using more complex finite difference formulas such as\n\\[\nf^{(n)}(x)=\\lim _{\\Delta x  \\rightarrow 0} \\frac{1}{\\Delta x ^n} \\sum_{k=0}^n(-1)^{k+n}\\binom{n}{k} f(x+k \\Delta x )\n\\]\n\n\n\nNumerical differentiation methods primarily rely on finite difference approximations derived from Taylor series expansions. Let’s explore these systematically.\n\n\nThe simplest approximation comes directly from the definition, where we look at the change in function value as we move forward from the current point:\n\\[\nf^{\\prime}_{i} \\approx \\frac{f_{i+1} - f_{i}}{\\Delta x}\n\\]\nThis is called the forward difference method. To understand its accuracy, we can analyze the error using Taylor expansion. The resulting local error \\(\\delta\\) at each calculation is:\n\\[\n\\delta = f_{i+1} - f_{i} - \\Delta x f^{\\prime}(x_i) = \\frac{1}{2} \\Delta x^2 f^{\\prime \\prime}(x_i) + O(\\Delta x^3)\n\\]\nWe observe that while the local truncation error is proportional to \\(\\Delta x^2\\), the accumulated global error is proportional to \\(\\Delta x\\), making this a first-order accurate method. This means that halving the step size will approximately halve the error in our final derivative approximation.\n\n\n\n\n\n\nLocal vs. Global Error\n\n\n\nLocal truncation error refers to the error introduced in a single step of the numerical method due to truncating the Taylor series. For the forward difference method, this error is \\(O(\\Delta x^2)\\).\nGlobal accumulated error is the total error that accumulates as we apply the method repeatedly across the domain. For the forward difference method, this accumulated error is \\(O(\\Delta x)\\). Global error is generally one order less accurate than the local error due to error propagation through multiple steps.\n\n\n\n\n\nWe can derive a more accurate approximation by using function values on both sides of the point of interest. Using Taylor expansions for \\(f(x+\\Delta x)\\) and \\(f(x-\\Delta x)\\):\n\\[\nf_{i+1} = f_{i} + \\Delta x f_{i}^{\\prime} + \\frac{\\Delta x^2}{2!} f_{i}^{\\prime\\prime} + \\frac{\\Delta x^3}{3!} f_{i}^{(3)} + \\ldots\n\\]\n\\[\nf_{i-1} = f_{i} - \\Delta x f_{i}^{\\prime} + \\frac{\\Delta x^2}{2!} f_{i}^{\\prime\\prime} - \\frac{\\Delta x^3}{3!} f_{i}^{(3)} + \\ldots\n\\]\nSubtracting these equations cancels out the even-powered terms in \\(\\Delta x\\):\n\\[\nf_{i+1} - f_{i-1} = 2 \\Delta x f_{i}^{\\prime} + O(\\Delta x^3)\n\\]\nSolving for \\(f^{\\prime}_{i}\\):\n\\[\nf^{\\prime}_{i} \\approx \\frac{f_{i+1} - f_{i-1}}{2 \\Delta x}\n\\]\nThis central difference formula has an error proportional to \\(\\Delta x^2\\), making it second-order accurate—significantly more precise than the forward difference method.\n\n\n\nWe can extend this approach to derive higher-order approximations by including more points in our calculation. A common fourth-order accurate formula for the first derivative is:\n\\[\nf_{i}^{\\prime}=\\frac{1}{12 \\Delta x}(-f_{i-2}+8f_{i-1}-8f_{i+1}+f_{i+2})\n\\]\nThis formula provides even better accuracy but requires function values at four points.\n\n\n\nThe following table summarizes the key finite difference methods for first derivatives:\n\n\n\n\n\n\n\n\n\nMethod\nFormula\nOrder of Accuracy\nPoints Required\n\n\n\n\nForward Difference\n\\(\\frac{f_{i+1} - f_{i}}{\\Delta x}\\)\n\\(O(\\Delta x)\\)\n2\n\n\nBackward Difference\n\\(\\frac{f_{i} - f_{i-1}}{\\Delta x}\\)\n\\(O(\\Delta x)\\)\n2\n\n\nCentral Difference\n\\(\\frac{f_{i+1} - f_{i-1}}{2\\Delta x}\\)\n\\(O(\\Delta x^2)\\)\n3\n\n\nFourth-Order Central\n\\(\\frac{-f_{i+2}+8f_{i+1}-8f_{i-1}+f_{i-2}}{12\\Delta x}\\)\n\\(O(\\Delta x^4)\\)\n5\n\n\n\nHigher-order methods generally provide more accurate results but require more computational resources and handle boundaries less efficiently.\n\n\n\n\nLet’s implement these numerical differentiation methods in Python, starting with a central difference function:\n\n\n\n\n\n\nWe can test these functions with \\(\\sin(x)\\), whose derivative is \\(\\cos(x)\\):\n\n\n\n\n\n\n\n\nLet’s examine how the error in our numerical derivative varies with the step size \\(\\Delta x\\):\n\n\n\n\n\n\nThis visualization demonstrates how error behaves with step size for different methods. For very small step sizes, roundoff errors become significant (observe the upturn in error for tiny \\(\\Delta x\\) values), while for larger steps, truncation error dominates.\n\n\n\n\nAn elegant approach to numerical differentiation involves representing the differentiation operation as a matrix multiplication. This representation is particularly valuable when solving differential equations numerically.\n\n\n\nFor a uniformly spaced grid of points \\(x_i\\), we can represent the first derivative operation as a matrix:\n\\[\nf^{\\prime} = \\frac{1}{\\Delta x}\n\\begin{bmatrix}\n-1 & 1  & 0 & 0 & \\cdots & 0\\\\\n0 & -1 & 1 & 0 & \\cdots & 0\\\\\n0 & 0  & -1 & 1 & \\cdots & 0\\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\ddots & \\vdots\\\\\n0 & 0  & 0  & \\cdots & -1 & 1\\\\\n0 & 0  & 0  & \\cdots &  0 & -1\\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nf_{1}\\\\\nf_{2}\\\\\nf_{3}\\\\\n\\vdots\\\\\nf_{N-1}\\\\\nf_{N}\\\\\n\\end{bmatrix}\n\\]\nThis matrix implements the forward difference scheme. For a central difference scheme, the matrix would have entries on both sides of the diagonal.\n\n\nSimilarly, the second derivative can be represented as a tridiagonal matrix:\n\\[\nf^{\\prime\\prime} = \\frac{1}{\\Delta x^2}\n\\begin{bmatrix}\n1 & -2 & 1 & 0 & \\cdots & 0\\\\\n0 & 1 & -2 & 1 & \\cdots & 0\\\\\n0 & 0 & 1 & -2 & \\cdots & 0\\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\ddots & \\vdots\\\\\n0 & 0 & 0 & \\cdots & 1 & -2 & 1\\\\\n0 & 0 & 0 & \\cdots & 0 & 1 & -2\\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nf_{1}\\\\\nf_{2}\\\\\nf_{3}\\\\\n\\vdots\\\\\nf_{N-1}\\\\\nf_{N}\\\\\n\\end{bmatrix}\n\\]\nThe boundary conditions affect the structure of these matrices, especially the first and last rows.\n\n\n\nSciPy provides tools to efficiently construct and work with these differentiation matrices:\n\n\n\n\n\n\n\n\n\n\nA critical consideration in numerical differentiation is how to handle the boundaries of the domain. Different approaches include:\n\nOne-sided differences: Using forward differences at the left boundary and backward differences at the right boundary.\nExtrapolation: Extending the domain by extrapolating function values beyond the boundaries.\nPeriodic boundaries: For periodic functions, using values from the opposite end of the domain.\nGhost points: Introducing additional points outside the domain whose values are determined by the boundary conditions.\n\nThe choice of boundary treatment depends on the physical problem and can significantly impact the accuracy of the solution.\n\n\n\nNumerical differentiation is foundational to computational physics. Let’s explore some specific applications:\n\n\nMany physics problems are formulated as differential equations. For example, the one-dimensional time-dependent Schrödinger equation:\n\\[\ni\\hbar\\frac{\\partial}{\\partial t}\\Psi(x,t) = -\\frac{\\hbar^2}{2m}\\frac{\\partial^2}{\\partial x^2}\\Psi(x,t) + V(x)\\Psi(x,t)\n\\]\nNumerical differentiation allows us to approximate the spatial derivatives, reducing this to a system of ordinary differential equations in time.\n\n\n\nWhen working with experimental measurements, we often need to calculate derivatives from discrete data points. For instance, determining the velocity and acceleration of an object from position measurements.\n\n\n\n\n\n\nNotice how noise in the position measurements gets amplified in the velocity calculations. This highlights a key challenge in numerical differentiation: sensitivity to noise.\n\n\n\nIn electrostatics, the electric field \\(\\vec{E}\\) is related to the electric potential \\(\\phi\\) by \\(\\vec{E} = -\\nabla \\phi\\). Numerical differentiation allows us to calculate the electric field from a known potential distribution.\n\n\n\n\n\n\n\n\n\n\n\n\nChoosing an appropriate step size is crucial for numerical differentiation. If \\(\\Delta x\\) is too large, the truncation error becomes significant. If \\(\\Delta x\\) is too small, roundoff errors dominate. A general approach is to use:\n\\[ \\Delta x \\approx \\sqrt{\\epsilon_\\text{machine}} \\times x \\]\nwhere \\(\\epsilon_\\text{machine}\\) is the machine epsilon (approximately \\(10^{-16}\\) for double precision).\n\n\n\nNumerical differentiation amplifies noise in the data. Several techniques can help:\n\nSmoothing: Apply a filter to the data before differentiation.\nRegularization: Use methods that inherently provide some smoothing.\nSavitzky-Golay filters: Combine local polynomial fitting with differentiation.\n\n\n\n\nIn physical simulations, preserving conservation laws (energy, momentum, etc.) is often crucial. Some numerical differentiation schemes conserve these properties better than others.\n\n\n\n\nThe SciPy library provides convenient functions for numerical differentiation:\n\n\n\n\n\n\nThe order parameter controls the accuracy of the approximation by using more points in the calculation.\n\n\n\nNumerical differentiation is a fundamental technique in computational physics, bridging the gap between theoretical models and practical computations. By understanding the principles, methods, and challenges of numerical differentiation, physicists can effectively analyze data, solve differential equations, and simulate physical systems.\nThe methods we’ve explored—from simple finite differences to matrix representations—provide a comprehensive toolkit for tackling a wide range of physics problems. As you apply these techniques, remember that the choice of method should be guided by the specific requirements of your problem: accuracy needs, computational constraints, and the nature of your data.\n\n\n\n\nImplement and compare the accuracy of different numerical differentiation schemes for the function \\(f(x) = e^{-x^2}\\).\nInvestigate how noise in the input data affects the accuracy of numerical derivatives and explore techniques to mitigate this effect.\nCalculate the electric field around two point charges using numerical differentiation of the electric potential.\nAnalyze experimental data from a falling object to determine its acceleration, and compare with the expected value of gravitational acceleration.\n\n\n\n\n\n\n\nFurther Reading\n\n\n\n\nNumerical Recipes: The Art of Scientific Computing by Press, Teukolsky, Vetterling, and Flannery\nNumerical Methods for Physics by Alejandro Garcia\nComputational Physics by Mark Newman\nApplied Numerical Analysis by Curtis F. Gerald and Patrick O. Wheatley",
    "crumbs": [
      "Python Basics",
      "Lecture 5",
      "Numerical Differentiation"
    ]
  },
  {
    "objectID": "lectures/lecture02/01-lecture02.html",
    "href": "lectures/lecture02/01-lecture02.html",
    "title": "Data Types",
    "section": "",
    "text": "It’s time to look at different data types we may find useful in our course. Besides the number types mentioned previously, there are also other types like strings, lists, tuples, dictionaries and sets.\nEach of these data types has a number of connected methods (functions) which allow you to manipulate the data contained in a variable. If you want to know which methods are available for a certain object use the command dir, e.g.\nThe output would be:\nThe following few cells will give you a short introduction into each type.",
    "crumbs": [
      "Python Basics",
      "Lecture 2",
      "Data Types"
    ]
  },
  {
    "objectID": "lectures/lecture02/01-lecture02.html#data-types",
    "href": "lectures/lecture02/01-lecture02.html#data-types",
    "title": "Data Types",
    "section": "Data Types",
    "text": "Data Types\n\nNumeric TypesStringsListsTuplesDictionariesBooleanSets\n\n\nPython supports several numeric data types including integers, floats, and complex numbers.\n\n\n\n\n\n\nYou can perform various arithmetic operations with numeric types:\n\n\n\n\n\n\nType conversion works between numeric types:\n\n\n\n\n\n\n\n\nStrings are lists of keyboard characters as well as other characters not on your keyboard. They are useful for printing results on the screen, during reading and writing of data.\n\n\n\n\n\n\n\n\n\n\n\n\nString can be concatenated using the + operator.\n\n\n\n\n\n\n\n\n\n\n\n\nAs strings are lists, each character in a string can be accessed by addressing the position in the string (see Lists section)\n\n\n\n\n\n\nStrings can also be made out of numbers.\n\n\n\n\n\n\nIf you want to obtain a number of a string, you can use what is known as type casting. Using type casting you may convert the string or any other data type into a different type if this is possible. To find out if a string is a pure number you may use the str.isnumeric method. For the above string, we may want to do a conversion to the type int by typing:\n\n\n\n\n\n\n\n\n\n\n\n\nThere are a number of methods connected to the string data type. Usually the relate to formatting or finding sub-strings. Formatting will be a topic in our next lecture. Here we just refer to one simple find example.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLists are ordered, mutable collections that can store items of different data types.\n\n\n\n\n\n\nYou can access and modify list elements:\n\n\n\n\n\n\nCommon list methods:\n\n\n\n\n\n\n\n\nTuples are ordered, immutable sequences.\n\n\n\n\n\n\nTuples are immutable, meaning you cannot change their elements after creation:\n\n\n\n\n\n\n\n\nDictionaries store data as key-value pairs. They are mutable and unordered.\n\n\n\n\n\n\nAccessing and modifying dictionary elements:\n\n\n\n\n\n\nCommon dictionary methods:\n\n\n\n\n\n\n\n\nThe Boolean type has only two possible values: True and False.\n\n\n\n\n\n\nBoolean values are commonly used in conditional statements:\n\n\n\n\n\n\nBoolean operations:\n\n\n\n\n\n\n\n\nSets are unordered collections of unique elements.\n\n\n\n\n\n\nCommon set operations:\n\n\n\n\n\n\nAdding and removing elements:",
    "crumbs": [
      "Python Basics",
      "Lecture 2",
      "Data Types"
    ]
  },
  {
    "objectID": "lectures/lecture02/01-lecture02.html#type-casting",
    "href": "lectures/lecture02/01-lecture02.html#type-casting",
    "title": "Data Types",
    "section": "Type Casting",
    "text": "Type Casting\nType casting is the process of converting a value from one data type to another. Python provides built-in functions for type conversion.\nPython offers several built-in functions for type conversion: - int(): Converts to integer - float(): Converts to float - str(): Converts to string - bool(): Converts to boolean - list(): Converts to list - tuple(): Converts to tuple - set(): Converts to set - dict(): Converts from mappings or iterables of key-value pairs\nLet’s explore various type conversion examples with practical code demonstrations. These examples show how Python handles conversions between different data types.\nNumeric Conversions\nWhen converting between numeric types, it’s important to understand how precision and data may change. For example, converting floats to integers removes the decimal portion without rounding.\n\n\n\n\n\n\nString Conversions\nString conversions are commonly used when processing user input or preparing data for output. Python provides straightforward functions for converting between strings and numeric types.\n\n\n\n\n\n\nCollection Type Conversions\nPython allows for easy conversion between different collection types, which is useful for changing the properties of your data structure (like making elements unique with sets).\n\n\n\n\n\n\nBoolean Conversion\nBoolean conversion is essential for conditional logic. Python follows specific rules to determine truthiness of values, with certain “empty” or “zero” values converting to False.\nWhen converting to boolean with bool(), the following values are considered False: - 0 (integer) - 0.0 (float) - \"\" (empty string) - [] (empty list) - () (empty tuple) - {} (empty dictionary) - set() (empty set) - None\nEverything else converts to True.\n\n\n\n\n\n\nSpecial Cases and Errors\nType conversion can sometimes fail, especially when the source value cannot be logically converted to the target type. Understanding these limitations helps prevent runtime errors in your code.\nNot all type conversions are possible. Python will raise an error when the conversion is not possible.\n\n\n\n\n\n\nTo handle potential errors in type conversion, you can use exception handling with try/except blocks:",
    "crumbs": [
      "Python Basics",
      "Lecture 2",
      "Data Types"
    ]
  },
  {
    "objectID": "lectures/lecture03/02-lecture03.html",
    "href": "lectures/lecture03/02-lecture03.html",
    "title": "NumPy Module",
    "section": "",
    "text": "Numpy is, besides SciPy, the core library for scientific computing in Python. It provides a high-performance multidimensional array object and tools for working with these arrays. The NumPy array, formally called ndarray in NumPy documentation, is the real workhorse of data structures for scientific and engineering applications. The NumPy array is similar to a list but where all the elements of the list are of the same type. The elements of a NumPy array are usually numbers, but can also be booleans, strings, or other objects. When the elements are numbers, they must all be of the same type.\nFor physics applications, NumPy is essential because it enables efficient numerical calculations on large datasets, handling of vectors and matrices, and implementation of mathematical models that describe physical phenomena. Whether simulating particle motion, analyzing experimental data, or solving equations of motion, NumPy provides the computational foundation needed for modern physics.",
    "crumbs": [
      "Python Basics",
      "Lecture 3",
      "Numpy Module"
    ]
  },
  {
    "objectID": "lectures/lecture03/02-lecture03.html#creating-numpy-arrays",
    "href": "lectures/lecture03/02-lecture03.html#creating-numpy-arrays",
    "title": "NumPy Module",
    "section": "Creating Numpy Arrays",
    "text": "Creating Numpy Arrays\nThere are a number of ways to initialize new numpy arrays, for example from\n\na Python list or tuples\nusing functions that are dedicated to generating numpy arrays, such as arange, linspace, etc.\nreading data from files which will be covered in the files section\n\n\nFrom listsUsing array-generating functions\n\n\nFor example, to create new vector and matrix arrays from Python lists we can use the numpy.array function.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor larger arrays it is impractical to initialize the data manually, using explicit python lists. Instead we can use one of the many functions in numpy that generate arrays of different forms. Some of the more common are:\n\n\n\n\n\n\n\n\n\n\n\n\n\nlinspace and logspace\nThe linspace function creates an array of N evenly spaced points between a starting point and an ending point. The form of the function is linspace(start, stop, N).If the third argument N is omitted,then N=50.\n\n\n\n\n\n\nlogspace is doing equivalent things with logarithmic spacing. Other types of array creation techniques are listed below. Try around with these commands to get a feeling what they do.\n\n\n\n\n\n\n\n\nmgrid\nmgrid generates a multi-dimensional matrix with increasing value entries, for example in columns and rows:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndiag\ndiag generates a diagonal matrix with the list supplied to it. The values can be also offset from the main diagonal.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nzeros and ones\nzeros and ones creates a matrix with the dimensions given in the argument and filled with 0 or 1.",
    "crumbs": [
      "Python Basics",
      "Lecture 3",
      "Numpy Module"
    ]
  },
  {
    "objectID": "lectures/lecture03/02-lecture03.html#array-attributes",
    "href": "lectures/lecture03/02-lecture03.html#array-attributes",
    "title": "NumPy Module",
    "section": "Array Attributes",
    "text": "Array Attributes\nNumPy arrays have several attributes that provide information about their size, shape, and data type. These attributes are essential for understanding and debugging your code.\n\nshapesizedtype\n\n\nThe shape attribute returns a tuple that gives the size of the array along each dimension.\n\n\n\n\n\n\n\n\nThe size attribute returns the total number of elements in the array.\n\n\n\n\n\n\n\n\nThe dtype attribute returns the data type of the array’s elements.\n\n\n\n\n\n\n\n\n\n\n\n\nThese attributes are particularly useful when debugging operations between arrays, as many NumPy functions require arrays of specific shapes or compatible data types.",
    "crumbs": [
      "Python Basics",
      "Lecture 3",
      "Numpy Module"
    ]
  },
  {
    "objectID": "lectures/lecture03/02-lecture03.html#manipulating-numpy-arrays",
    "href": "lectures/lecture03/02-lecture03.html#manipulating-numpy-arrays",
    "title": "NumPy Module",
    "section": "Manipulating NumPy arrays",
    "text": "Manipulating NumPy arrays\n\nSlicingReshapingAdding a new dimension: newaxisStacking and repeating arrays\n\n\nSlicing is the name for extracting part of an array by the syntax M[lower:upper:step]\n\n\n\n\n\n\n\n\n\n\n\n\nAny of the three parameters in M[lower:upper:step] can be ommited.\n\n\n\n\n\n\n\n\n\n\n\n\nNegative indices counts from the end of the array (positive index from the begining):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIndex slicing works exactly the same way for multidimensional arrays:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDifferences\n\n\n\nSlicing can be effectively used to calculate differences for example for the calculation of derivatives. Here the position \\(y_i\\) of an object has been measured at times \\(t_i\\) and stored in an array each. We wish to calculate the average velocity at the times \\(t_{i}\\) from the arrays by\n\\[\\begin{equation}\nv_{i}=\\frac{y_i-y_{i-1}}{t_{i}-t_{i-1}}\n\\end{equation}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArrays can be reshaped into any form, which contains the same number of elements.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWith newaxis, we can insert new dimensions in an array, for example converting a vector to a column or row matrix.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing function repeat, tile, vstack, hstack, and concatenate we can create larger vectors and matrices from smaller ones. Please try the individual functions yourself in your notebook. We wont discuss them in detail.\n\nTile and repeat\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConcatenate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHstack and vstack",
    "crumbs": [
      "Python Basics",
      "Lecture 3",
      "Numpy Module"
    ]
  },
  {
    "objectID": "lectures/lecture03/02-lecture03.html#applying-mathematical-functions",
    "href": "lectures/lecture03/02-lecture03.html#applying-mathematical-functions",
    "title": "NumPy Module",
    "section": "Applying mathematical functions",
    "text": "Applying mathematical functions\nAll kinds of mathematical operations can be carried out on arrays. Typically these operation act element wise as seen from the examples below.\n\nOperation involving one arrayOperations involving multiple arraysRandom Numbers\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVector operations enable efficient element-wise calculations where corresponding elements at matching positions are processed simultaneously. Instead of handling elements one by one, these operations work on entire arrays at once, making them particularly fast. When multiplying two vectors using these operations, the result is not a single number (as in a dot product) but rather a new array where each element is the product of the corresponding elements from the input vectors. This element-wise multiplication is just one example of vector operations, which can include addition, subtraction, and other mathematical functions.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumPy provides powerful tools for generating random numbers, which are essential for simulations in statistical physics, quantum mechanics, and other fields:\n\n\n\n\n\n\nThese random number generators are particularly useful for Monte Carlo simulations, modeling thermal noise, or simulating quantum mechanical systems.",
    "crumbs": [
      "Python Basics",
      "Lecture 3",
      "Numpy Module"
    ]
  },
  {
    "objectID": "lectures/lecture03/02-lecture03.html#broadcasting",
    "href": "lectures/lecture03/02-lecture03.html#broadcasting",
    "title": "NumPy Module",
    "section": "Broadcasting",
    "text": "Broadcasting\nBroadcasting is a powerful mechanism that allows NumPy to work with arrays of different shapes when performing arithmetic operations. The smaller array is “broadcast” across the larger array so that they have compatible shapes.\nThe rules for broadcasting are:\n\nIf the arrays don’t have the same rank, prepend the shape of the lower rank array with 1s until both shapes have the same length.\nThe size in each dimension of the output shape is the maximum of the sizes of the input arrays along that dimension.\nAn input can be used in the calculation if its size in a particular dimension matches the output size or if its value is exactly 1.\nIf an input has a dimension size of 1, the first element is used for all calculations along that dimension.\n\nLet’s see some examples:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBroadcasting enables efficient computation without the need to create copies of arrays, saving memory and computation time.",
    "crumbs": [
      "Python Basics",
      "Lecture 3",
      "Numpy Module"
    ]
  },
  {
    "objectID": "lectures/lecture03/02-lecture03.html#physics-example-force-calculations",
    "href": "lectures/lecture03/02-lecture03.html#physics-example-force-calculations",
    "title": "NumPy Module",
    "section": "Physics Example: Force Calculations",
    "text": "Physics Example: Force Calculations\nBroadcasting is particularly useful in physics when applying the same operation to multiple objects. For example, when calculating the gravitational force between one massive object and multiple other objects using Newton’s law of universal gravitation:\n\\[\\begin{equation}\nF = \\frac{G M m}{r^2}\n\\end{equation}\\]\nwhere \\(F\\) is the gravitational force, \\(G\\) is the gravitational constant, \\(M\\) and \\(m\\) are the masses of the two objects, and \\(r\\) is the distance between them.",
    "crumbs": [
      "Python Basics",
      "Lecture 3",
      "Numpy Module"
    ]
  },
  {
    "objectID": "lectures/lecture03/03-lecture03.html",
    "href": "lectures/lecture03/03-lecture03.html",
    "title": "Basic Plotting with Matplotlib",
    "section": "",
    "text": "Data visualization is an essential skill for analyzing and presenting scientific data effectively. Python itself doesn’t include plotting capabilities in its core language, but Matplotlib provides powerful and flexible tools for creating visualizations. Matplotlib is the most widely used plotting library in Python and serves as an excellent starting point for creating basic plots.\nMatplotlib works well with NumPy, Python’s numerical computing library, to create a variety of plot types including line plots, scatter plots, bar charts, and more. For this document, we’ve already imported both libraries as you can see in the code below:\nWe’ve also set up some default styling parameters to make our plots more readable and professional-looking:\nThese settings configure the appearance of our plots with appropriate font sizes, line widths, and tick marks. The get_size() function helps us convert dimensions from centimeters to inches, which is useful when specifying figure sizes. With these preparations complete, we’re ready to create various types of visualizations to effectively display our data.\nMatplotlib offers multiple levels of functionality for creating plots. Throughout this section, we’ll primarily focus on using commands that leverage default settings. This approach simplifies the process, as Matplotlib automatically handles much of the graph layout. These high-level commands are ideal for quickly creating effective visualizations without delving into intricate details. Later in this course, we’ll briefly touch upon more advanced techniques that provide greater control over plot elements and layout.",
    "crumbs": [
      "Python Basics",
      "Lecture 3",
      "Basic Plotting"
    ]
  },
  {
    "objectID": "lectures/lecture03/03-lecture03.html#basic-plotting",
    "href": "lectures/lecture03/03-lecture03.html#basic-plotting",
    "title": "Basic Plotting with Matplotlib",
    "section": "Basic Plotting",
    "text": "Basic Plotting\nTo create a basic line plot, use the following command:\nplt.plot(x, y)\nBy default, this generates a line plot. However, you can customize the appearance by adjusting various parameters within the plot() function. For instance, you can modify it to resemble a scatter plot by changing certain arguments. The versatility of this command allows for a range of visual representations beyond simple line plots.\nLet’s create a simple line plot of the sine function over the interval \\([0, 4\\pi]\\). We’ll use NumPy to generate the x-values and calculate the corresponding y-values. The following code snippet demonstrates this process:\n1x = np.linspace(0, 4.*np.pi, 100)\n2y = np.sin(x)\n\n3plt.figure(figsize=get_size(8,6))\n4plt.plot(x, y)\n5plt.tight_layout()\n6plt.show()\n\n1\n\nCreate an array of 100 values between 0 and 4\\(\\pi\\).\n\n2\n\nCalculate the sine of each value in the array.\n\n3\n\ncreate a new figure with a size of (8,6) cm\n\n4\n\nplot the data\n\n5\n\nautomatically adjust the layout\n\n6\n\nshow the figure\n\n\nHere is the code in a Python cell:\n\n\n\n\n\n\nTry to change the values of the x and y arrays and see how the plot changes.\n\n\n\n\n\n\nWhy use plt.tight_layout()\n\n\n\n\n\nplt.tight_layout() is a very useful function in Matplotlib that automatically adjusts the spacing between plot elements to prevent overlapping and ensure that all elements fit within the figure area. Here’s what it does:\n\nPadding Adjustment: It adjusts the padding between and around subplots to prevent overlapping of axis labels, titles, and other elements.\nSubplot Spacing: It optimizes the space between multiple subplots in a figure.\nText Accommodation: It ensures that all text elements (like titles, labels, and legends) fit within the figure without being cut off.\nMargin Adjustment: It adjusts the margins around the entire figure to make sure everything fits neatly.\nAutomatic Resizing: If necessary, it can slightly resize subplot areas to accommodate all elements.\nLegend Positioning: It takes into account the presence and position of legends when adjusting layouts.\n\nKey benefits of using plt.tight_layout():\n\nIt saves time in manual adjustment of plot elements.\nIt helps create more professional-looking and readable plots.\nIt’s particularly useful when creating figures with multiple subplots or when saving figures to files.\n\nYou typically call plt.tight_layout() just before plt.show() or plt.savefig(). For example:\nplt.figure()\n# ... (your plotting code here)\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Python Basics",
      "Lecture 3",
      "Basic Plotting"
    ]
  },
  {
    "objectID": "lectures/lecture03/03-lecture03.html#customizing-plots",
    "href": "lectures/lecture03/03-lecture03.html#customizing-plots",
    "title": "Basic Plotting with Matplotlib",
    "section": "Customizing Plots",
    "text": "Customizing Plots\n\nAxis LabelsLegendsPlotting Multiple LinesCustomizing Line AppearancePlots with Error BarsVisualizing NumPy Arrays\n\n\nTo enhance the clarity and interpretability of our plots, it’s crucial to provide context through proper labeling. The following commands add descriptive axis labels to our diagram:\nplt.xlabel('x-label')\nplt.ylabel('y-label')\nHere’s an example of adding labels to our sine plot:\n\n\n\n\n\n\n\n\nWhen plotting multiple datasets, it’s important to include a legend to identify each line. Use these commands:\nplt.plot(..., label='Label name')\nplt.legend(loc='lower left')\nHere’s an example with a legend:\n\n\n\n\n\n\n\n\nYou can add multiple lines to the same plot:\n\n\n\n\n\n\n\n\nYou can customize the appearance of lines with additional parameters:\n\n\n\n\n\n\n\n\nWhen plotting experimental data, it’s customary to include error bars that graphically indicate measurement uncertainty. The errorbar function can be used to display both vertical and horizontal error bars:\nplt.errorbar(x, y, xerr=x_errors, yerr=y_errors, fmt='format', label='label')\nHere’s an example of a plot with error bars:\n\n\n\n\n\n\n\n\nWe can visualize 2D arrays created with NumPy:",
    "crumbs": [
      "Python Basics",
      "Lecture 3",
      "Basic Plotting"
    ]
  },
  {
    "objectID": "lectures/lecture03/03-lecture03.html#saving-figures",
    "href": "lectures/lecture03/03-lecture03.html#saving-figures",
    "title": "Basic Plotting with Matplotlib",
    "section": "Saving Figures",
    "text": "Saving Figures\nTo save a figure to a file, use the savefig method. Matplotlib supports multiple formats including PNG, JPG, EPS, SVG, PGF and PDF:\nplt.savefig('filename.extension')\nHere’s an example of creating and saving a figure:\n\n\n\n\n\n\nFor scientific papers, PDF format is recommended whenever possible. LaTeX documents compiled with pdflatex can include PDFs using the includegraphics command. PGF can also be a good alternative in some cases.",
    "crumbs": [
      "Python Basics",
      "Lecture 3",
      "Basic Plotting"
    ]
  },
  {
    "objectID": "lectures/lecture03/03-lecture03.html#numpy-with-visualization",
    "href": "lectures/lecture03/03-lecture03.html#numpy-with-visualization",
    "title": "Basic Plotting with Matplotlib",
    "section": "NumPy with Visualization",
    "text": "NumPy with Visualization\nThe arrays and calculations we’ve learned in NumPy form the foundation for scientific data visualization. In the next section, we’ll explore how to use Matplotlib to create visual representations of NumPy arrays, allowing us to interpret and communicate our physics results more effectively.\nFor example, we can visualize the planetary force calculations from our broadcasting example:",
    "crumbs": [
      "Python Basics",
      "Lecture 3",
      "Basic Plotting"
    ]
  },
  {
    "objectID": "lectures/lecture04/02-lecture04.html",
    "href": "lectures/lecture04/02-lecture04.html",
    "title": "Brownian Motion",
    "section": "",
    "text": "Brownian motion is a fundamental physical phenomenon that describes the random movement of particles suspended in a fluid. This lecture explores both the physical understanding and computational modeling of Brownian motion using object-oriented programming techniques.\nWe will apply our newly acquired knowledge about classes to simulate Brownian motion. This task aligns perfectly with the principles of object-oriented programming, as each Brownian particle (or colloid) can be represented as an object instantiated from the same class, albeit with different properties. For instance, some particles might be larger while others are smaller. We have already touched on some aspects of this in previous lectures.",
    "crumbs": [
      "Python Basics",
      "Lecture 4",
      "Brownian Motion Part I"
    ]
  },
  {
    "objectID": "lectures/lecture04/02-lecture04.html#introduction",
    "href": "lectures/lecture04/02-lecture04.html#introduction",
    "title": "Brownian Motion",
    "section": "",
    "text": "Brownian motion is a fundamental physical phenomenon that describes the random movement of particles suspended in a fluid. This lecture explores both the physical understanding and computational modeling of Brownian motion using object-oriented programming techniques.\nWe will apply our newly acquired knowledge about classes to simulate Brownian motion. This task aligns perfectly with the principles of object-oriented programming, as each Brownian particle (or colloid) can be represented as an object instantiated from the same class, albeit with different properties. For instance, some particles might be larger while others are smaller. We have already touched on some aspects of this in previous lectures.",
    "crumbs": [
      "Python Basics",
      "Lecture 4",
      "Brownian Motion Part I"
    ]
  },
  {
    "objectID": "lectures/lecture04/02-lecture04.html#brownian-motion",
    "href": "lectures/lecture04/02-lecture04.html#brownian-motion",
    "title": "Brownian Motion",
    "section": "Brownian Motion",
    "text": "Brownian Motion\n\nWhat is Brownian Motion?\nImagine a dust particle floating in water. If you look at it under a microscope, you’ll see it moving in a random, zigzag pattern. This is Brownian motion!\n\n\nWhy Does This Happen?\nWhen we observe Brownian motion, we’re seeing the effects of countless molecular collisions. Water isn’t just a smooth, continuous fluid - it’s made up of countless tiny molecules that are in constant motion. These water molecules are continuously colliding with our particle from all directions. Each individual collision causes the particle to move just a tiny bit, barely noticeable on its own. However, when millions of these tiny collisions happen every second from random directions, they create the distinctive zigzag motion we observe.\n\nbrownianMotion = {\n  const width = 400;\n  const height = 400;\n\n  // Create SVG container\n  const svg = d3.create(\"svg\")\n    .attr(\"width\", width)\n    .attr(\"height\", height)\n    .attr(\"viewBox\", [0, 0, width, height])\n    .attr(\"style\", \"max-width: 100%; height: auto;\");\n\n  // Add a border for clarity\n  svg.append(\"rect\")\n    .attr(\"width\", width)\n    .attr(\"height\", height)\n    .attr(\"fill\", \"none\")\n    .attr(\"stroke\", \"black\");\n\n  // Parameters for our simulation - adjusted for better physical representation\n  const numSmallParticles = 400;\n  const largeParticleRadius = 10;\n  const smallParticleRadius = 5;  // Increased scale separation\n  const largeParticleColor = \"blue\";\n  const smallParticleColor = \"red\";\n  const trailLength = 300;\n\n  // Physical parameters\n  const temperature = 10.0;  // Normalized temperature\n  const gamma = 0.1;        // Drag coefficient for large particle\n\n  // Calculate masses based on radius^3 (proportional to volume)\n  const largeMass = Math.pow(largeParticleRadius, 3);\n  const smallMass = Math.pow(smallParticleRadius, 3);\n\n  // Maxwell-Boltzmann distribution helper\n  function maxwellBoltzmannVelocity() {\n    // Box-Muller transform for normal distribution\n    const u1 = Math.random();\n    const u2 = Math.random();\n    const mag = Math.sqrt(-2.0 * Math.log(u1)) * Math.sqrt(temperature / smallMass);\n    const theta = 2 * Math.PI * u2;\n    return {\n      vx: mag * Math.cos(theta),\n      vy: mag * Math.sin(theta)\n    };\n  }\n\n  // Initialize large particle in the center\n  let largeParticle = {\n    x: width / 2,\n    y: height / 2,\n    vx: 0,\n    vy: 0,\n    radius: largeParticleRadius,\n    mass: largeMass,\n    // Store previous positions for trail\n    trail: Array(trailLength).fill().map(() =&gt; ({\n      x: width / 2,\n      y: height / 2\n    }))\n  };\n\n  // Initialize small particles with random positions and thermal velocities\n  const smallParticles = Array(numSmallParticles).fill().map(() =&gt; {\n    const vel = maxwellBoltzmannVelocity();\n    return {\n      x: Math.random() * width,\n      y: Math.random() * height,\n      vx: vel.vx * 8,  // Scale for visibility\n      vy: vel.vy * 8,  // Scale for visibility\n      radius: smallParticleRadius,\n      mass: smallMass\n    };\n  });\n\n  // Create the large particle\n  const largeParticleElement = svg.append(\"circle\")\n    .attr(\"cx\", largeParticle.x)\n    .attr(\"cy\", largeParticle.y)\n    .attr(\"r\", largeParticle.radius)\n    .attr(\"fill\", largeParticleColor);\n\n  // Create the trail for the large particle\n  const trailElements = svg.append(\"g\")\n    .selectAll(\"circle\")\n    .data(largeParticle.trail)\n    .join(\"circle\")\n    .attr(\"cx\", d =&gt; d.x)\n    .attr(\"cy\", d =&gt; d.y)\n    .attr(\"r\", (_, i) =&gt; 1)\n    .attr(\"fill\", \"rgba(0, 0, 255, 0.2)\");\n\n  // Create the small particles\n  const smallParticleElements = svg.append(\"g\")\n    .selectAll(\"circle\")\n    .data(smallParticles)\n    .join(\"circle\")\n    .attr(\"cx\", d =&gt; d.x)\n    .attr(\"cy\", d =&gt; d.y)\n    .attr(\"r\", d =&gt; d.radius)\n    .attr(\"fill\", smallParticleColor);\n\n  // Function to update particle positions\n  function updateParticles() {\n    // Apply drag to large particle (Stokes' law)\n    largeParticle.vx *= (1 - gamma);\n    largeParticle.vy *= (1 - gamma);\n\n    // Update small particles\n    smallParticles.forEach((particle, i) =&gt; {\n      // Move according to velocity\n      particle.x += particle.vx;\n      particle.y += particle.vy;\n\n      // Bounce off walls\n      if (particle.x &lt; particle.radius || particle.x &gt; width - particle.radius) {\n        particle.vx *= -1;\n        particle.x = Math.max(particle.radius, Math.min(width - particle.radius, particle.x));\n      }\n      if (particle.y &lt; particle.radius || particle.y &gt; height - particle.radius) {\n        particle.vy *= -1;\n        particle.y = Math.max(particle.radius, Math.min(height - particle.radius, particle.y));\n      }\n\n      // Check for collision with large particle\n      const dx = largeParticle.x - particle.x;\n      const dy = largeParticle.y - particle.y;\n      const distance = Math.sqrt(dx * dx + dy * dy);\n\n      if (distance &lt; largeParticle.radius + particle.radius) {\n        // Physically correct elastic collision\n\n        // Calculate unit normal vector (collision axis)\n        const nx = dx / distance;\n        const ny = dy / distance;\n\n        // Calculate unit tangent vector (perpendicular to collision)\n        const tx = -ny;\n        const ty = nx;\n\n        // Project velocities onto normal and tangential axes\n        const v1n = largeParticle.vx * nx + largeParticle.vy * ny;\n        const v1t = largeParticle.vx * tx + largeParticle.vy * ty;\n        const v2n = particle.vx * nx + particle.vy * ny;\n        const v2t = particle.vx * tx + particle.vy * ty;\n\n        // Calculate new normal velocities using conservation of momentum and energy\n        // Tangential velocities remain unchanged in elastic collision\n        const m1 = largeParticle.mass;\n        const m2 = particle.mass;\n\n        // One-dimensional elastic collision formula\n        const v1nAfter = (v1n * (m1 - m2) + 2 * m2 * v2n) / (m1 + m2);\n        const v2nAfter = (v2n * (m2 - m1) + 2 * m1 * v1n) / (m1 + m2);\n\n        // Convert back to x,y velocities\n        largeParticle.vx = v1nAfter * nx + v1t * tx;\n        largeParticle.vy = v1nAfter * ny + v1t * ty;\n        particle.vx = v2nAfter * nx + v2t * tx;\n        particle.vy = v2nAfter * ny + v2t * ty;\n\n        // Move particles apart to prevent overlap\n        const overlap = largeParticle.radius + particle.radius - distance;\n        const massRatio = m2 / (m1 + m2);\n        const largeMoveRatio = massRatio;\n        const smallMoveRatio = 1 - massRatio;\n\n        // Move particles apart proportional to their masses\n        largeParticle.x += overlap * nx * largeMoveRatio;\n        largeParticle.y += overlap * ny * largeMoveRatio;\n        particle.x -= overlap * nx * smallMoveRatio;\n        particle.y -= overlap * ny * smallMoveRatio;\n      }\n\n      // Occasionally thermostat small particles to maintain temperature\n      if (Math.random() &lt; 0.01) {\n        const vel = maxwellBoltzmannVelocity();\n        particle.vx = vel.vx * 8;  // Scale for visibility\n        particle.vy = vel.vy * 8;  // Scale for visibility\n      }\n\n      // Update small particle display\n      smallParticleElements.filter((_, j) =&gt; i === j)\n        .attr(\"cx\", particle.x)\n        .attr(\"cy\", particle.y);\n    });\n\n    // Update large particle position\n    largeParticle.x += largeParticle.vx;\n    largeParticle.y += largeParticle.vy;\n\n    // Bounce large particle off walls\n    if (largeParticle.x &lt; largeParticle.radius || largeParticle.x &gt; width - largeParticle.radius) {\n      largeParticle.vx *= -1;\n      largeParticle.x = Math.max(largeParticle.radius, Math.min(width - largeParticle.radius, largeParticle.x));\n    }\n    if (largeParticle.y &lt; largeParticle.radius || largeParticle.y &gt; height - largeParticle.radius) {\n      largeParticle.vy *= -1;\n      largeParticle.y = Math.max(largeParticle.radius, Math.min(height - largeParticle.radius, largeParticle.y));\n    }\n\n    // Update trail\n    largeParticle.trail.pop();\n    largeParticle.trail.unshift({x: largeParticle.x, y: largeParticle.y});\n\n    // Update large particle display\n    largeParticleElement\n      .attr(\"cx\", largeParticle.x)\n      .attr(\"cy\", largeParticle.y);\n\n    // Update trail display\n    trailElements.data(largeParticle.trail)\n      .attr(\"cx\", d =&gt; d.x)\n      .attr(\"cy\", d =&gt; d.y);\n  }\n\n  // Start animation\n  const interval = d3.interval(() =&gt; {\n    updateParticles();\n  }, 30);\n\n  // Clean up on invalidation\n  invalidation.then(() =&gt; interval.stop());\n\n  return svg.node();\n}\n\n\n\n\n\n\n\n\nFigure 1: Interactive simulation of Brownian motion. The blue circle represents a larger colloid, which moves randomly due to collisions with the smaller red particles. The fading blue trail shows the random path of the colloid.\n\n\n\n\n\n\nThe Mathematical Model of Brownian Motion\nMathematically, Brownian motion is governed by the Langevin equation, which describes the basic equation of motion:\n\\[m\\frac{d^2\\mathbf{r}}{dt^2} = -\\gamma\\frac{d\\mathbf{r}}{dt} + \\mathbf{F}_\\text{random}(t)\\]\nwhere:\n\n\\(m\\) is the particle mass\n\\(\\mathbf{r}\\) is the position vector\n\\(\\gamma\\) is the drag coefficient\n\\(\\mathbf{F}_\\text{random}(t)\\) represents random forces from molecular collisions\n\nIn the overdamped limit (\\(m=0\\) which applies to colloidal particles), inertia becomes negligible and the equation simplifies to:\n\\[\\frac{d\\mathbf{r}}{dt} = \\sqrt{2D}\\,\\boldsymbol{\\xi}(t)\\]\nWhere \\(\\boldsymbol{\\xi}(t)\\) is Gaussian white noise with a unit variance and \\(D\\) is the diffusion coefficient, the transport coefficient characterizing diffusive transport.\nA key observable in Brownian motion is the mean squared displacement (MSD):\n\\[\\langle (\\Delta r)^2 \\rangle = 2dDt\\]\nwith:\n\n\\(\\langle (\\Delta r)^2 \\rangle\\) is the mean squared displacement\n\\(d\\) is the number of dimensions (2 in our simulation)\n\\(D\\) is the diffusion coefficient\n\\(t\\) is the time elapsed\n\nThe diffusion coefficient \\(D\\) depends on physical properties according to the Einstein-Stokes relation:\n\\[D = \\frac{k_B T}{6\\pi\\eta R}\\]\nWhere \\(k_B\\) is Boltzmann’s constant, \\(T\\) is temperature, \\(\\eta\\) is fluid viscosity, and \\(R\\) is the particle radius.\n\n\nNumerical Implementation\nIn our Colloid class simulation, we implement the discretized version of the overdamped Langevin equation. For each time step \\(\\Delta t\\), the position update is:\n\\[\\Delta x = \\sqrt{2D\\Delta t} \\times \\xi\\]\nWhere \\(\\Delta x\\) is the displacement in one direction, and \\(\\xi\\) is a random number drawn from a normal distribution with mean 0 and variance 1.\nThis is implemented directly in the update() method of our Colloid class:\ndef update(self, dt):\n    self.x.append(self.x[-1] + np.random.normal(0.0, np.sqrt(2*self.D*dt)))\n    self.y.append(self.y[-1] + np.random.normal(0.0, np.sqrt(2*self.D*dt)))\n    return(self.x[-1], self.y[-1])\nIn this implementation: - D is the diffusion coefficient stored as an instance variable - dt is the time step parameter - np.random.normal generates the Gaussian random numbers required for the stochastic process\n\n\n\n\n\n\nTip\n\n\n\nThe choice of time step dt is important in our simulation. If too large, it fails to capture the fine details of the motion. If too small, the simulation becomes computationally expensive. The class design allows us to adjust this parameter easily when calling sim_trajectory() or update().\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdvanced Mathematical Details\n\n\n\n\n\nThe Brownian motion of a colloidal particle results from collisions with surrounding solvent molecules. These collisions lead to a probability distribution described by:\n\\[\np(x,\\Delta t)=\\frac{1}{\\sqrt{4\\pi D \\Delta t}}e^{-\\frac{x^2}{4D \\Delta t}}\n\\]\nwith:\n\n\\(D\\) is the diffusion coefficient\n\\(\\Delta t\\) is the time step\nThe variance is \\(\\sigma^2=2D \\Delta t\\)\n\nThis distribution emerges from the central limit theorem, as shown by Lindenberg and Lévy, when considering many infinitesimally small random steps.\nThe evolution of the probability density function \\(p(x,t)\\) is governed by the diffusion equation:\n\\[\n\\frac{\\partial p}{\\partial t}=D\\frac{\\partial^2 p}{\\partial x^2}\n\\]\nThis partial differential equation, also known as Fick’s second law, describes how the concentration of particles evolves over time due to diffusive processes. The Gaussian distribution above is the fundamental solution (Green’s function) of this diffusion equation, representing how an initially localized distribution spreads out over time.\nThe connection between the microscopic random motion and the macroscopic diffusion equation was first established by Einstein in his 1905 paper on Brownian motion, providing one of the earliest quantitative links between statistical mechanics and thermodynamics.",
    "crumbs": [
      "Python Basics",
      "Lecture 4",
      "Brownian Motion Part I"
    ]
  },
  {
    "objectID": "lectures/lecture04/02-lecture04.html#object-oriented-implementation",
    "href": "lectures/lecture04/02-lecture04.html#object-oriented-implementation",
    "title": "Brownian Motion",
    "section": "Object-Oriented Implementation",
    "text": "Object-Oriented Implementation\n\nWhy Use a Class?\nA class is perfect for this physics simulation because each colloidal particle:\n\nHas specific properties\n\nSize (radius)\nCurrent position\nMovement history\nDiffusion coefficient\n\nFollows certain behaviors\n\nMoves randomly (Brownian motion)\nUpdates its position over time\nKeeps track of where it’s been\n\nCan exist alongside other particles\n\nMany particles can move independently\nEach particle keeps track of its own properties\nParticles can have different sizes\n\nNeeds to track its state over time\n\nRemember previous positions\nCalculate distances moved\nMaintain its own trajectory\n\n\nThis natural mapping between real particles and code objects makes classes an ideal choice for our simulation.\n\n\nClass Design\nWe design a Colloid class to simulate particles undergoing Brownian motion. Using object-oriented programming makes physical sense here - in the real world, each colloidal particle is an independent object with its own properties that follows the same physical laws as other particles.\n\nClass-Level Properties (Shared by All Particles)\nOur Colloid class will store information common to all particles:\n\nnumber: A counter tracking how many particles we’ve created\nf = 2.2×10^{-19}: The physical constant \\(k_B T/(6\\pi\\eta)\\) in m³/s\n\nThis combines Boltzmann’s constant (\\(k_B\\)), temperature (\\(T\\)), and fluid viscosity (\\(\\eta\\))\nUsing this constant simplifies our diffusion calculations\n\n\n\n\nClass Methods (Functions Shared by All Particles)\nThe class provides these shared behaviors:\n\nhow_many(): Returns the total count of particles created\n\nUseful for tracking how many particles exist in our simulation\n\n__str__(): Returns a human-readable description when we print a particle\n\nShows the particle’s radius and current position\n\n\n\n\nInstance Properties (Unique to Each Particle)\nEach individual particle will have its own:\n\nR: Radius in meters\nx, y: Lists storing position history (starting with initial position)\nindex: Unique ID number for each particle\nD: Diffusion coefficient calculated as \\(D = f/R\\)\n\nFrom Einstein-Stokes relation: \\(D = \\frac{k_B T}{6\\pi\\eta R}\\)\nSmaller particles diffuse faster (larger D)\n\n\n\n\nInstance Methods (What Each Particle Can Do)\nEach particle object will have these behaviors:\n\nupdate(dt): Performs a single timestep of Brownian motion\n\nTakes a timestep dt in seconds\nAdds random displacement based on diffusion coefficient\nReturns the new position\n\nsim_trajectory(N, dt): Simulates a complete trajectory\n\nGenerates N steps with timestep dt\nCalls update() repeatedly to build the trajectory\n\nget_trajectory(): Returns the particle’s movement history as a DataFrame\n\nConvenient for analysis and plotting\n\nget_D(): Returns the particle’s diffusion coefficient\n\nUseful for calculations and verification\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that the function sim_trajectory is actually calling the function update of the same object to generate the whole trajectory at once.",
    "crumbs": [
      "Python Basics",
      "Lecture 4",
      "Brownian Motion Part I"
    ]
  },
  {
    "objectID": "lectures/lecture04/02-lecture04.html#simulation-and-analysis",
    "href": "lectures/lecture04/02-lecture04.html#simulation-and-analysis",
    "title": "Brownian Motion",
    "section": "Simulation and Analysis",
    "text": "Simulation and Analysis\n\nSimulating\nWith the help of this Colloid class, we would like to carry out simulations of Brownian motion of multiple particles. The simulations shall\n\ntake n=200 particles\nhave N=200 trajectory points each\nstart all at 0,0\nparticle objects should be stored in a list p_list\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlotting the trajectories\nThe next step is to plot all the trajectories.\n\n\n\n\n\n\n\n\nCharacterizing the Brownian motion\nNow that we have a number of trajectories, we can analyze the motion of our Brownian particles.\n\nCalculate the particle speed\nOne way is to calculate its speed by measuring how far it traveled within a certain time \\(n\\, dt\\), where \\(dt\\) is the timestep of out simulation. We can do that as\n\\[\\begin{equation}\nv(n dt) = \\frac{&lt;\\sqrt{(x_{i+n}-x_{i})^2+(y_{i+n}-y_{i})^2}&gt;}{n\\,dt}\n\\end{equation}\\]\nThe angular brackets on the top take care of the fact that we can measure the distance traveled within a certain time \\(n\\, dt\\) several times along a trajectory.\nThese values can be used to calculate a mean speed. Note that there is not an equal amount of data pairs for all separations available. For \\(n=1\\) there are 5 distances available. For \\(n=5\\), however, only 1. This changes the statistical accuracy of the mean.\n\n\n\n\n\n\nThe result of this analysis shows, that each particle has an apparent speed which seems to increase with decreasing time of observation or which decreases with increasing time. This would mean that there is some friction at work, which slows down the particle in time, but this is apparently not true. Also an infinite speed at zero time appears to be unphysical. The correct answer is just that the speed is no good measure to characterize the motion of a Brownian particle.\n\n\nCalculate the particle mean squared displacement\nA better way to characterize the motion of a Brownian particle is the mean squared displacement, as we have already mentioned it in previous lectures. We may compare our simulation now to the theoretical prediction, which is\n\\[\\begin{equation}\n\\langle \\Delta r^{2}(t)\\rangle=2 d D t\n\\end{equation}\\]\nwhere \\(d\\) is the dimension of the random walk, which is \\(d=2\\) in our case.\n\n\n\n\n\n\nThe results show that the mean squared displacement of the individual particles follows on average the theoretical predictions of a linear growth in time. That means, we are able to read the diffusion coefficient from the slope of the MSD of the individual particles if recorded in a simulation or an experiment.\nYet, each individual MSD is deviating strongly from the theoretical prediction especially at large times. This is due to the fact mentioned earlier that our simulation (or experimental) data only has a limited number of data points, while the theoretical prediction is made for the limit of infinite data points.\n\n\n\n\n\n\nAnalysis of MSD data\n\n\n\nSingle particle tracking, either in the experiment or in numerical simulations can therefore only deliver an estimate of the diffusion coefficient and care should be taken when using the whole MSD to obtain the diffusion coefficient. One typically uses only a short fraction of the whole MSD data at short times.",
    "crumbs": [
      "Python Basics",
      "Lecture 4",
      "Brownian Motion Part I"
    ]
  },
  {
    "objectID": "lectures/lecture04/02-lecture04.html#summary",
    "href": "lectures/lecture04/02-lecture04.html#summary",
    "title": "Brownian Motion",
    "section": "Summary",
    "text": "Summary\nIn this lecture, we have:\n\nExplored the physical principles behind Brownian motion and its mathematical description\nImplemented a computational model using object-oriented programming principles\nCreated a Colloid class with properties and methods that simulate realistic particle behavior\nGenerated and visualized multiple particle trajectories\nAnalyzed the simulation results using mean squared displacement calculations\nCompared our numerical results with theoretical predictions\n\nThis exercise demonstrates how object-oriented programming provides an elegant framework for physics simulations, where the objects in our code naturally represent physical entities in the real world.",
    "crumbs": [
      "Python Basics",
      "Lecture 4",
      "Brownian Motion Part I"
    ]
  },
  {
    "objectID": "lectures/lecture04/02-lecture04.html#further-reading",
    "href": "lectures/lecture04/02-lecture04.html#further-reading",
    "title": "Brownian Motion",
    "section": "Further Reading",
    "text": "Further Reading\n\nEinstein, A. (1905). “On the Movement of Small Particles Suspended in Stationary Liquids Required by the Molecular-Kinetic Theory of Heat”\nBerg, H.C. (1993). “Random Walks in Biology”\nChandrasekhar, S. (1943). “Stochastic Problems in Physics and Astronomy”\nNelson, E. (2001). “Dynamical Theories of Brownian Motion”",
    "crumbs": [
      "Python Basics",
      "Lecture 4",
      "Brownian Motion Part I"
    ]
  },
  {
    "objectID": "seminars/seminar01/00-seminar01.html",
    "href": "seminars/seminar01/00-seminar01.html",
    "title": "CBPM Seminar Questions",
    "section": "",
    "text": "Please answer the following questions to help tailor the seminar to your needs. Your responses are anonymous and will be used only to adapt the teaching to your level of experience.\n\n  Loading…",
    "crumbs": [
      "Seminars",
      "Seminar01",
      "Seminar Questions"
    ]
  },
  {
    "objectID": "seminars/seminar01/MDSimulation.html",
    "href": "seminars/seminar01/MDSimulation.html",
    "title": "Molecular Dynamics Simulations in EMPP 2024",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nplt.rcParams.update({'font.size': 8,\n                     'lines.linewidth': 1,\n                     'lines.markersize': 10,\n                     'axes.labelsize': 10,\n                     'axes.titlesize': 10,\n                     'xtick.labelsize' : 10,\n                     'ytick.labelsize' : 10,\n                     'xtick.top' : True,\n                     'xtick.direction' : 'in',\n                     'ytick.right' : True,\n                     'ytick.direction' : 'in',})\n\ndef get_size(w,h):\n    return((w/2.54,h/2.54))"
  },
  {
    "objectID": "seminars/seminar01/MDSimulation.html#lenard-jones-potential",
    "href": "seminars/seminar01/MDSimulation.html#lenard-jones-potential",
    "title": "Molecular Dynamics Simulations in EMPP 2024",
    "section": "Lenard-Jones Potential",
    "text": "Lenard-Jones Potential\n\ndef lennard_jones(r, epsilon=1, sigma=1):\n    return 4 * epsilon * ((sigma/r)**12 - (sigma/r)**6)\n\nr = np.linspace(0.8, 3, 1000)\nV = lennard_jones(r)\n\nplt.figure(figsize=get_size(8, 6),dpi=150)\nplt.plot(r, V, 'b-', linewidth=2)\nplt.grid(True)\nplt.xlabel('r/σ')\nplt.ylabel('V/ε')\nplt.title('Lennard-Jones Potential')\nplt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\nplt.ylim(-1.5, 3)\nplt.show()"
  },
  {
    "objectID": "seminars/seminar01/MDSimulation.html#taylor-expansion",
    "href": "seminars/seminar01/MDSimulation.html#taylor-expansion",
    "title": "Molecular Dynamics Simulations in EMPP 2024",
    "section": "Taylor Expansion",
    "text": "Taylor Expansion\n\nx = np.linspace(-2*np.pi, 2*np.pi, 1000)\ny = np.sin(x)\ny_taylor = x - 1/6*x**3\ny_taylor1 = x - 1/6*x**3+x**5/120\n\n\nplt.figure(figsize=get_size(8, 6),dpi=150)\nplt.plot(x, y, 'b-', label='sin(x)', linewidth=2)\nplt.plot(x, y_taylor, 'r--', label='O(5)', linewidth=2)\nplt.plot(x, y_taylor1, 'g--', label='O(7)', linewidth=2)\nplt.grid(True)\nplt.xlabel('x')\nplt.ylabel('f(x)')\nplt.xlim(-2,2)\nplt.ylim(-1.3,1.3)\nplt.title('Taylor Expansion of sin(x)')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "seminars/seminar01/MDSimulation.html#velocity-verlet",
    "href": "seminars/seminar01/MDSimulation.html#velocity-verlet",
    "title": "Molecular Dynamics Simulations in EMPP 2024",
    "section": "Velocity Verlet",
    "text": "Velocity Verlet\n\ng = 9.81  # m/s^2\ndt = 0.01  # time step\nt_max = 2.0  # total simulation time\nsteps = int(t_max/dt)\n\n# Initial conditions\ny0 = 20.0  # initial height\nv0 = 0.0   # initial velocity\n\n\n# Arrays to store results\nt = np.zeros(steps)\ny = np.zeros(steps)\nv = np.zeros(steps)\na = np.zeros(steps)\n\n# Initial values\ny[0] = y0\nv[0] = v0\na[0] = -g\n\n# Velocity Verlet integration\nfor i in range(1, steps):\n    t[i] = i * dt\n    y[i] = y[i-1] + v[i-1] * dt + 0.5 * a[i-1] * dt**2  # update position\n    a_new = -g                                          # new acceleration (assuming constant gravity)\n    v[i] = v[i-1] + 0.5 * (a[i-1] + a_new) * dt  # update velocity\n    a[i] = a_new  # store new acceleration\n\nplt.figure(figsize=get_size(8, 6), dpi=150)\nplt.plot(t, y)\nplt.xlabel('Time (s)')\nplt.ylabel('Height (m)')\nplt.title('Free Fall Motion')\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "seminars/seminar01/MDSimulation.html#class-for-an-atom",
    "href": "seminars/seminar01/MDSimulation.html#class-for-an-atom",
    "title": "Molecular Dynamics Simulations in EMPP 2024",
    "section": "Class for an Atom",
    "text": "Class for an Atom"
  },
  {
    "objectID": "seminars/seminar04/md3.html",
    "href": "seminars/seminar04/md3.html",
    "title": "Step-by-Step Development of a Molecular Dynamics Simulation",
    "section": "",
    "text": "We define a class Atom that contains the properties of an atom. The class Atom has the following attributes:\nclass Atom:\n    dimension = 2\n\n    def __init__(self, atom_id, atom_type, position, velocity=None, mass=None):\n        self.id = atom_id\n        self.type = atom_type\n        self.position = position\n        self.velocity = velocity if velocity is not None else np.zeros(dimension)\n        self.mass = mass\n        self.force = np.zeros(dimension)\nThe class Atom has the following attributes:\n\nid: The unique identifier of the atom\ntype: The type of the atom (hydrogen or oxygen or …)\nposition: The position of the atom in 3D space (x, y, z)\nvelocity: The velocity of the atom in 3D space (vx, vy, vz)\nmass: The mass of the atom\nforce: The force acting on the atom in 3D space (fx, fy, fz)\n\nIn addition, we will need some information on the other atoms that are bound to the atom. We will store this information later in a list of atoms called boundto. Since we start with a monoatomic gas, we will not need this information for now. Note that position, velocity, and force are 3D vectors and we store them in numpy arrays. This is a very convenient way to handle vectors and matrices in Python.\nThe class Atom should further implement a number of functions, called methods in object-oriented programming, that allow us to interact with the atom. The following methods are implemented in the Atom class:\n\n\ndef add_force(self, force):\n    \"\"\"Add force contribution to total force on atom\"\"\"\n    self.force += force\n\n\n\ndef reset_force(self):\n    \"\"\"Reset force to zero at start of each step\"\"\"\n    self.force = np.zeros(dimension)\n\n\n\ndef update_position(self, dt):\n    \"\"\"First step of velocity Verlet: update position\"\"\"\n    self.position += self.velocity * dt + 0.5 * (self.force/self.mass) * dt**2\n\n\n\ndef update_velocity(self, dt, new_force):\n    \"\"\"Second step of velocity Verlet: update velocity using average force\"\"\"\n    self.velocity += 0.5 * (new_force + self.force)/self.mass * dt\n    self.force = new_force\n\n\n\ndef apply_periodic_boundaries(self, box_size):\n        \"\"\"Apply periodic boundary conditions\"\"\"\n        self.position = self.position % box_size\n\n\n\n\n\n\nComplete Atom class\n\n\n\n\n\nclass Atom:\n    def __init__(self, atom_id, atom_type, position, velocity=None, mass=None):\n        self.id = atom_id\n        self.type = atom_type\n        self.position = position\n        self.velocity = velocity if velocity is not None else np.random.randn(2)*20\n        self.mass = mass\n        self.force = np.zeros(2)\n\n\n    def add_force(self, force):\n        \"\"\"Add force contribution to total force on atom\"\"\"\n        self.force += force\n\n    def reset_force(self):\n        \"\"\"Reset force to zero at start of each step\"\"\"\n        self.force = np.zeros(2)\n\n    def update_position(self, dt):\n        \"\"\"First step of velocity Verlet: update position\"\"\"\n        self.position += self.velocity * dt + 0.5 * (self.force/self.mass) * dt**2\n\n    def update_velocity(self, dt, new_force):\n        \"\"\"Second step of velocity Verlet: update velocity using average force\"\"\"\n        self.velocity += 0.5 * (new_force + self.force)/self.mass * dt\n        self.force = new_force\n\n    def apply_periodic_boundaries(self, box_size):\n            \"\"\"Apply periodic boundary conditions\"\"\"\n            self.position = self.position % box_size\n\n\n\nThis would be a good time to do something simple with the atom class. Let’s create a bunch of atoms and plot them in a 2D space.",
    "crumbs": [
      "Seminars",
      "Seminar04",
      "MD Simulation 3"
    ]
  },
  {
    "objectID": "seminars/seminar04/md3.html#implementations",
    "href": "seminars/seminar04/md3.html#implementations",
    "title": "Step-by-Step Development of a Molecular Dynamics Simulation",
    "section": "",
    "text": "We define a class Atom that contains the properties of an atom. The class Atom has the following attributes:\nclass Atom:\n    dimension = 2\n\n    def __init__(self, atom_id, atom_type, position, velocity=None, mass=None):\n        self.id = atom_id\n        self.type = atom_type\n        self.position = position\n        self.velocity = velocity if velocity is not None else np.zeros(dimension)\n        self.mass = mass\n        self.force = np.zeros(dimension)\nThe class Atom has the following attributes:\n\nid: The unique identifier of the atom\ntype: The type of the atom (hydrogen or oxygen or …)\nposition: The position of the atom in 3D space (x, y, z)\nvelocity: The velocity of the atom in 3D space (vx, vy, vz)\nmass: The mass of the atom\nforce: The force acting on the atom in 3D space (fx, fy, fz)\n\nIn addition, we will need some information on the other atoms that are bound to the atom. We will store this information later in a list of atoms called boundto. Since we start with a monoatomic gas, we will not need this information for now. Note that position, velocity, and force are 3D vectors and we store them in numpy arrays. This is a very convenient way to handle vectors and matrices in Python.\nThe class Atom should further implement a number of functions, called methods in object-oriented programming, that allow us to interact with the atom. The following methods are implemented in the Atom class:\n\n\ndef add_force(self, force):\n    \"\"\"Add force contribution to total force on atom\"\"\"\n    self.force += force\n\n\n\ndef reset_force(self):\n    \"\"\"Reset force to zero at start of each step\"\"\"\n    self.force = np.zeros(dimension)\n\n\n\ndef update_position(self, dt):\n    \"\"\"First step of velocity Verlet: update position\"\"\"\n    self.position += self.velocity * dt + 0.5 * (self.force/self.mass) * dt**2\n\n\n\ndef update_velocity(self, dt, new_force):\n    \"\"\"Second step of velocity Verlet: update velocity using average force\"\"\"\n    self.velocity += 0.5 * (new_force + self.force)/self.mass * dt\n    self.force = new_force\n\n\n\ndef apply_periodic_boundaries(self, box_size):\n        \"\"\"Apply periodic boundary conditions\"\"\"\n        self.position = self.position % box_size\n\n\n\n\n\n\nComplete Atom class\n\n\n\n\n\nclass Atom:\n    def __init__(self, atom_id, atom_type, position, velocity=None, mass=None):\n        self.id = atom_id\n        self.type = atom_type\n        self.position = position\n        self.velocity = velocity if velocity is not None else np.random.randn(2)*20\n        self.mass = mass\n        self.force = np.zeros(2)\n\n\n    def add_force(self, force):\n        \"\"\"Add force contribution to total force on atom\"\"\"\n        self.force += force\n\n    def reset_force(self):\n        \"\"\"Reset force to zero at start of each step\"\"\"\n        self.force = np.zeros(2)\n\n    def update_position(self, dt):\n        \"\"\"First step of velocity Verlet: update position\"\"\"\n        self.position += self.velocity * dt + 0.5 * (self.force/self.mass) * dt**2\n\n    def update_velocity(self, dt, new_force):\n        \"\"\"Second step of velocity Verlet: update velocity using average force\"\"\"\n        self.velocity += 0.5 * (new_force + self.force)/self.mass * dt\n        self.force = new_force\n\n    def apply_periodic_boundaries(self, box_size):\n            \"\"\"Apply periodic boundary conditions\"\"\"\n            self.position = self.position % box_size\n\n\n\nThis would be a good time to do something simple with the atom class. Let’s create a bunch of atoms and plot them in a 2D space.",
    "crumbs": [
      "Seminars",
      "Seminar04",
      "MD Simulation 3"
    ]
  },
  {
    "objectID": "seminars/seminar02/md1.html",
    "href": "seminars/seminar02/md1.html",
    "title": "Step-by-Step Development of a Molecular Dynamics Simulation",
    "section": "",
    "text": "Real molecular dynamics (MD) simulations are complex and computationally expensive but very cool, as they give you a glimpse into the world of atoms and molecules. Here, we will develop a simple MD simulation from scratch in Python. The goal is to understand the basic concepts and algorithms behind MD simulations and get something running which can be extended later but also what we are proud of at the end of the course.\nBefore we can start with implementing a simulation, we need to understand the basic concepts and algorithms behind MD simulations. The following sections will guide you through the development of a simple MD simulation. The Jupyter Notebook below will help you to copy and paste the code to test the snippets presented.\nOpen in New Window",
    "crumbs": [
      "Seminars",
      "Seminar02",
      "MD Simulation 1"
    ]
  },
  {
    "objectID": "seminars/seminar02/md1.html#molecular-dynamics-simulations",
    "href": "seminars/seminar02/md1.html#molecular-dynamics-simulations",
    "title": "Step-by-Step Development of a Molecular Dynamics Simulation",
    "section": "",
    "text": "Real molecular dynamics (MD) simulations are complex and computationally expensive but very cool, as they give you a glimpse into the world of atoms and molecules. Here, we will develop a simple MD simulation from scratch in Python. The goal is to understand the basic concepts and algorithms behind MD simulations and get something running which can be extended later but also what we are proud of at the end of the course.\nBefore we can start with implementing a simulation, we need to understand the basic concepts and algorithms behind MD simulations. The following sections will guide you through the development of a simple MD simulation. The Jupyter Notebook below will help you to copy and paste the code to test the snippets presented.\nOpen in New Window",
    "crumbs": [
      "Seminars",
      "Seminar02",
      "MD Simulation 1"
    ]
  },
  {
    "objectID": "seminars/seminar02/md1.html#basic-physical-concepts",
    "href": "seminars/seminar02/md1.html#basic-physical-concepts",
    "title": "Step-by-Step Development of a Molecular Dynamics Simulation",
    "section": "Basic Physical Concepts",
    "text": "Basic Physical Concepts\n\nNewton’s Equations of Motion\nThe motion of particles in a molecular dynamics simulation is governed by Newton’s equations of motion:\n\\[m_i \\frac{d^2\\vec{r}_i}{dt^2} = \\vec{F}_i\\]\nwhere \\(m_i\\) is the mass of particle \\(i\\), \\(\\vec{r}_i\\) is the position of particle \\(i\\), and \\(\\vec{F}_i\\) is the force acting on particle \\(i\\).\nThe force acting on a particle is the sum of all forces acting on it:\n\\[\\vec{F}_i = \\sum_{j \\neq i} \\vec{F}_{ij}\\]\nwhere \\(\\vec{F}_{ij}\\) is the force acting on particle \\(i\\) due to particle \\(j\\).\n\n\nPotential Energy Functions and Forces\nThe force \\(\\vec{F}_{ij}\\) is usually derived from a potential energy function and may result from a variety of interactions, such as:\n\n\n\n\n\n\n\n\nInteraction Type\nSubtype\nIllustration\n\n\n\n\nBonded interactions\nBond stretching\n\n\n\n\nBond angle bending\n\n\n\n\nTorsional interactions\n\n\n\nNon-bonded interactions\nElectrostatic interactions\n\n\n\n\nVan der Waals interactions\n\n\n\nExternal forces\n\n\n\n\n\nWe will implement some of them but not all of them.\n\nLennard-Jones Potential\nThe most common potential energy function used in MD simulations is the Lennard-Jones potential. It is belonging to the class of non-bonded interactions. The force and the potential energy of the Lennard-Jones potential are given by:\n\\[V_{LJ}(r) = 4\\epsilon \\left[\\left(\\frac{\\sigma}{r}\\right)^{12} - \\left(\\frac{\\sigma}{r}\\right)^6\\right]\\]\nand\n\\[F_{LJ}(r) = -\\frac{dV_{LJ}}{dr} = 24\\epsilon \\left[2\\left(\\frac{\\sigma}{r}\\right)^{12} - \\left(\\frac{\\sigma}{r}\\right)^6\\right]\\frac{\\vec{r}}{r^2}\\]\nwhere \\(\\frac{\\vec{r}}{r^2}\\) represents the direction of the force (the unit vector \\(\\hat{r} = \\frac{\\vec{r}}{r}\\)) multiplied by \\(\\frac{1}{r}\\), and \\(\\epsilon\\) is the depth of the potential well, \\(\\sigma\\) is the distance at which the potential is zero, and \\(r\\) is the distance between particles.\nThe Lenard Jones potential is good for describing the interaction of non-bonded atoms in a molecular system e.g. in a gas or a liquid and is therefore well suited if we first want to simulate a gas or a liquid.\n\n\nCode\ndef lennard_jones(r, epsilon=1, sigma=1):\n    return 4 * epsilon * ((sigma/r)**12 - (sigma/r)**6)\n\nr = np.linspace(0.8, 3, 1000)\nV = lennard_jones(r)\n\nplt.figure(figsize=get_size(8, 6),dpi=150)\nplt.plot(r, V, 'b-', linewidth=2)\nplt.grid(True)\nplt.xlabel('r/σ')\nplt.ylabel('V/ε')\nplt.title('Lennard-Jones Potential')\nplt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\nplt.ylim(-1.5, 3)\nplt.show()\n\n\n\n\n\n\n\n\n\nThe figure above shows the Lennard-Jones potential as a function of the distance between particles. The potential energy is zero at the equilibrium distance \\(r = \\sigma\\) and has a minimum at \\(r = 2^{1/6}\\sigma\\). The potential energy is positive for \\(r &lt; \\sigma\\) and negative for \\(r &gt; \\sigma\\).\n\n\n\n\n\n\nValues for atomic hydrogen\n\n\n\nFor atomic hydrogen (H), typical Lennard-Jones parameters are:\n\n\\(\\sigma \\approx 2.38\\) Å = \\(2.38 \\times 10^{-10}\\) meters\n\\(\\epsilon \\approx 0.0167\\) kcal/mol = \\(1.16 \\times 10^{-21}\\) joules\n\n\n\nLater, if we manage to advance to some more complicated systems, we may want to introduce:\n\nforce in bonds between two atoms\nforce in bond angles between three atoms\nforce in dihedral angles between four atoms\n\nBut for now, we will stick to the Lennard-Jones potential.",
    "crumbs": [
      "Seminars",
      "Seminar02",
      "MD Simulation 1"
    ]
  },
  {
    "objectID": "seminars/seminar02/md1.html#integrating-newtons-equation-of-motion",
    "href": "seminars/seminar02/md1.html#integrating-newtons-equation-of-motion",
    "title": "Step-by-Step Development of a Molecular Dynamics Simulation",
    "section": "Integrating Newton’s Equation of Motion",
    "text": "Integrating Newton’s Equation of Motion\nWhen we have the forces on a particle we have in principle its acceleration. To get the velocity and the position of the particle we need to integrate the equations of motion. There are several methods to do this, but we will start with the simplest one, the Euler method.\n\nEuler Method\nTo obtain this one first needs to know about the Taylor expansion of a function in general. The Taylor expansion of a function \\(f(x)\\) around a point \\(x_0\\) is providing an approximation of the function in the vicinity of \\(x_0\\). It is given by:\n\\[f(x) = f(x_0) + f'(x_0)(x - x_0) + \\frac{1}{2}f''(x_0)(x - x_0)^2 + \\cdots\\]\nwhere \\(f'(x_0)\\) is the first derivative of \\(f(x)\\) at \\(x_0\\), \\(f''(x_0)\\) is the second derivative of \\(f(x)\\) at \\(x_0\\), and so on. We can demonstrate that by expanding a sine function around \\(x_0 = 0\\):\n\\[\\sin(x) = \\sin(0) + \\cos(0)x - \\frac{1}{2}\\sin(0)x^2 + \\cdots = x - \\frac{1}{6}x^3 + \\cdots\\]\nPlotting this yields:\n\n\nCode\nx = np.linspace(-2*np.pi, 2*np.pi, 1000)\ny = np.sin(x)\ny_taylor = x - 1/6*x**3\n\nplt.figure(figsize=get_size(8, 6),dpi=150)\nplt.plot(x, y, 'b-', label='sin(x)', linewidth=2)\nplt.plot(x, y_taylor, 'r--', label='Taylor expansion', linewidth=2)\nplt.grid(True)\nplt.xlabel('x')\nplt.ylabel('f(x)')\nplt.xlim(-2,2)\nplt.ylim(-2,2)\nplt.title('Taylor Expansion of sin(x)')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe expansion is therefore a good approximation in a region close to \\(x_0\\).\n\n\nVelocity Verlet Algorithm\nThe velocity Verlet algorithm is a second-order algorithm that offers greater accuracy than the Euler method. It can be derived from the Taylor expansion of the position and velocity vectors:\n\\[\\mathbf{r}(t + \\Delta t) = \\mathbf{r}(t) + \\mathbf{v}(t)\\Delta t + \\frac{1}{2}\\frac{\\mathbf{F}(t)}{m}\\Delta t^2+ O(\\Delta t^3)\\]\nThe higher order terms in the Taylor expansion are neglected, which results in an error of order \\(\\Delta t^3\\). In contrast, the Euler method is obtained by neglecting the higher order terms in the Taylor expansion of the velocity vector:\n\\[\\mathbf{v}(t + \\Delta t) = \\mathbf{v}(t) + \\frac{\\mathbf{F}(t)}{m}\\Delta t + O(\\Delta t^2)\\]\nThis makes the Euler method only first order accurate with an error of order \\(\\Delta t^2\\).\nThe Velocity Verlet algorithm is particularly valuable for molecular dynamics simulations because it offers several advantages over the Euler method. It does a much better job preserving the total energy of the system over long simulation times. The algorithm is also time-reversible, which is a property of the exact equations of motion. Furthermore, it provides symplectic integration, preserving the phase space volume, another important property for physical simulations. These properties make the Velocity Verlet algorithm much more stable for long simulations, which is crucial when modeling molecular systems over meaningful timescales.\nThe velocity Verlet algorithm provides a stable and accurate way to integrate the equations of motion through a three-stage process. First, we update positions using current velocities and forces: \\[\\mathbf{r}(t + \\Delta t) = \\mathbf{r}(t) + \\mathbf{v}(t)\\Delta t + \\frac{1}{2}\\frac{\\mathbf{F}(t)}{m}\\Delta t^2\\]\nNext, we calculate new forces based on these updated positions: \\[\\mathbf{F}(t + \\Delta t) = \\mathbf{F}(\\mathbf{r}(t + \\Delta t))\\]\nFinally, we update velocities using an average of old and new forces: \\[\\mathbf{v}(t + \\Delta t) = \\mathbf{v}(t) + \\frac{1}{2}\\frac{\\mathbf{F}(t) + \\mathbf{F}(t + \\Delta t)}{m}\\Delta t\\]\nIn these equations, \\(\\mathbf{r}\\) represents the position vector, \\(\\mathbf{v}\\) is the velocity vector, \\(\\mathbf{F}\\) is the force vector, \\(m\\) stands for mass, and \\(\\Delta t\\) is the timestep. This approach ensures greater accuracy and stability in our molecular dynamics simulations compared to simpler methods.\n\n\nSimple Integration Example: Free Fall\nLet’s start by integrating the equation of motion for a particle in free fall using the Velocity Verlet algorithm. This is an ideal starting example since the physics is straightforward, with gravity being the only force acting on the particle, and we can compare our numerical solution to the well-known analytical one.\nNewton’s equation of motion: \\[\\mathbf{F} = m\\mathbf{a}\\]\nFor gravity, the only force acting on our particle is the gravitational force pointing downward: \\[\\mathbf{F} = -mg\\hat{\\mathbf{y}}\\]\nTherefore, the acceleration in the y-direction is constant: \\[\\ddot{y} = -g\\]\nThe analytical solution to this differential equation gives us the position and velocity at any time \\(t\\). The position is given by \\[y(t) = y_0 + v_0t - \\frac{1}{2}gt^2\\], and the velocity is expressed as \\[v(t) = v_0 - gt\\]. Here, \\(y_0\\) is the initial height, \\(v_0\\) is the initial velocity, and \\(g\\) is the acceleration due to gravity. We can use this exact solution to verify our numerical integration method.\n\n\nCode\n# Parameters\n\ng = 9.81  # m/s^2\ndt = 0.01  # time step\nt_max = 2.0  # total simulation time\nsteps = int(t_max/dt)\n\n# Initial conditions\ny0 = 20.0  # initial height\nv0 = 0.0   # initial velocity\n\n\n# Arrays to store results\nt = np.zeros(steps)\ny = np.zeros(steps)\nv = np.zeros(steps)\na = np.zeros(steps)\n\n# Initial values\ny[0] = y0\nv[0] = v0\na[0] = -g\n\n# Velocity Verlet integration\nfor i in range(1, steps):\n    t[i] = i * dt\n    y[i] = y[i-1] + v[i-1] * dt + 0.5 * a[i-1] * dt**2  # update position\n    a_new = -g                                          # new acceleration (assuming constant gravity)\n    v[i] = v[i-1] + 0.5 * (a[i-1] + a_new) * dt         # update velocity\n    a[i] = a_new                                        # store new acceleration\n\ny_analytical = y0 + v0*t - 0.5*g*t**2\nplt.figure(figsize=get_size(8, 6), dpi=150)\nplt.plot(t, y)\nplt.plot(t, y_analytical, 'r--')\n\nplt.xlabel('Time (s)')\nplt.ylabel('Height (m)')\nplt.title('Free Fall Motion')\nplt.grid(True)\nplt.show()",
    "crumbs": [
      "Seminars",
      "Seminar02",
      "MD Simulation 1"
    ]
  },
  {
    "objectID": "seminars/seminar05/seminar05.html",
    "href": "seminars/seminar05/seminar05.html",
    "title": "Free coding Seminar",
    "section": "",
    "text": "Today’s seminar will be a free coding session. You’ll have 30 minutes to work on one of the following physics problems. After the coding time, we’ll discuss your approaches, solutions, and any challenges you encountered.\nOpen in New Window\n\n\n\nChoose one of the following problems:\n\n\nPhysics Background: In real-world scenarios, projectile motion is affected by air resistance. This resistance force is often modeled as proportional to the velocity (for low speeds) or to the square of the velocity (for higher speeds).\nProgramming Task: 1. Write a program to simulate the trajectory of a projectile with air resistance. 2. The equation of motion is: \\[m\\vec{a} = m\\vec{g} - b\\vec{v}\\] (linear drag) or \\[m\\vec{a} = m\\vec{g} - b|\\vec{v}|\\vec{v}\\] (quadratic drag) 3. Compare trajectories with different drag coefficients. 4. Visualize the trajectory and calculate the range for different initial angles. 5. Advanced: Find the optimal launch angle for maximum range when air resistance is present.\n\n\n\nPhysics Background: In quantum mechanics, particles are described by wave functions. A wave packet is a localized superposition of plane waves that can represent a particle with a reasonably well-defined position and momentum.\nProgramming Task: 1. Simulate the time evolution of a 1D Gaussian wave packet in free space: \\[\\psi(x,0) = A e^{-(x-x_0)^2/(4\\sigma^2)} e^{ik_0x}\\] 2. The time evolution is given by the time-dependent Schrödinger equation. 3. Calculate and plot the probability density \\(|\\psi(x,t)|^2\\) at different times. 4. Observe and explain the spreading of the wave packet. 5. Calculate the expectation values of position and momentum over time. 6. Advanced: Add a potential barrier and observe reflection/transmission.\nChoose the problem that interests you most or matches your skill level. You’ll have 30 minutes to work on your solution. Focus on getting a working implementation first, then refine and add features if time permits.",
    "crumbs": [
      "Seminars",
      "Seminar05",
      "Free Coding Seminar"
    ]
  },
  {
    "objectID": "seminars/seminar05/seminar05.html#free-coding-seminar---physics-problems",
    "href": "seminars/seminar05/seminar05.html#free-coding-seminar---physics-problems",
    "title": "Free coding Seminar",
    "section": "",
    "text": "Today’s seminar will be a free coding session. You’ll have 30 minutes to work on one of the following physics problems. After the coding time, we’ll discuss your approaches, solutions, and any challenges you encountered.\nOpen in New Window\n\n\n\nChoose one of the following problems:\n\n\nPhysics Background: In real-world scenarios, projectile motion is affected by air resistance. This resistance force is often modeled as proportional to the velocity (for low speeds) or to the square of the velocity (for higher speeds).\nProgramming Task: 1. Write a program to simulate the trajectory of a projectile with air resistance. 2. The equation of motion is: \\[m\\vec{a} = m\\vec{g} - b\\vec{v}\\] (linear drag) or \\[m\\vec{a} = m\\vec{g} - b|\\vec{v}|\\vec{v}\\] (quadratic drag) 3. Compare trajectories with different drag coefficients. 4. Visualize the trajectory and calculate the range for different initial angles. 5. Advanced: Find the optimal launch angle for maximum range when air resistance is present.\n\n\n\nPhysics Background: In quantum mechanics, particles are described by wave functions. A wave packet is a localized superposition of plane waves that can represent a particle with a reasonably well-defined position and momentum.\nProgramming Task: 1. Simulate the time evolution of a 1D Gaussian wave packet in free space: \\[\\psi(x,0) = A e^{-(x-x_0)^2/(4\\sigma^2)} e^{ik_0x}\\] 2. The time evolution is given by the time-dependent Schrödinger equation. 3. Calculate and plot the probability density \\(|\\psi(x,t)|^2\\) at different times. 4. Observe and explain the spreading of the wave packet. 5. Calculate the expectation values of position and momentum over time. 6. Advanced: Add a potential barrier and observe reflection/transmission.\nChoose the problem that interests you most or matches your skill level. You’ll have 30 minutes to work on your solution. Focus on getting a working implementation first, then refine and add features if time permits.",
    "crumbs": [
      "Seminars",
      "Seminar05",
      "Free Coding Seminar"
    ]
  },
  {
    "objectID": "course-info/schedule.html",
    "href": "course-info/schedule.html",
    "title": "Course Schedule",
    "section": "",
    "text": "The course will be updated weekly with the lecture schedule. Therefore, expect a new lecture every\nTuesday from April 8, 2025, at 13:15, Thoeretical Lecture Hall\nand a new assignment from 1:00 pm (starting the second week).\nExperience has shown that the best results are achieved when you are present in the lecture hall for the lectures. However, all material will also be available online, so you can access it at any time to learn whenever it suits you.",
    "crumbs": [
      "Course Info",
      "Schedule"
    ]
  },
  {
    "objectID": "course-info/schedule.html#lectures",
    "href": "course-info/schedule.html#lectures",
    "title": "Course Schedule",
    "section": "",
    "text": "The course will be updated weekly with the lecture schedule. Therefore, expect a new lecture every\nTuesday from April 8, 2025, at 13:15, Thoeretical Lecture Hall\nand a new assignment from 1:00 pm (starting the second week).\nExperience has shown that the best results are achieved when you are present in the lecture hall for the lectures. However, all material will also be available online, so you can access it at any time to learn whenever it suits you.",
    "crumbs": [
      "Course Info",
      "Schedule"
    ]
  },
  {
    "objectID": "course-info/schedule.html#seminars",
    "href": "course-info/schedule.html#seminars",
    "title": "Course Schedule",
    "section": "Seminars",
    "text": "Seminars\nSeminars will be held on Tuesday from April 15, 2025, at 15:00, , Thoeretical Lecture Hall\nIn the seminars, we will discuss the solutions to the exercises from the previous week and provide advanced insights and examples as well as discuss the topics covered in the lectures.\nThe topics covered in the seminars depend on YOU and your interests! We will tailor the content to your needs and preferences!",
    "crumbs": [
      "Course Info",
      "Schedule"
    ]
  },
  {
    "objectID": "course-info/resources.html",
    "href": "course-info/resources.html",
    "title": "Resources",
    "section": "",
    "text": "Above all else, it is important for this course that you have internet access during the lectures. We will conduct many examples and exercises that refer to online resources.\nWithin the university, the Eduroam networks can be used. You can find the necessary profile data here.\nFurthermore, there are many other well-structured resources about Python available online. Below you will find only a very small selection.",
    "crumbs": [
      "Course Info",
      "Ressources"
    ]
  },
  {
    "objectID": "course-info/resources.html#molecular-nanophotonics-group",
    "href": "course-info/resources.html#molecular-nanophotonics-group",
    "title": "Resources",
    "section": "Molecular Nanophotonics Group",
    "text": "Molecular Nanophotonics Group\n\nMolecular Nanophotonics Group Website",
    "crumbs": [
      "Course Info",
      "Ressources"
    ]
  },
  {
    "objectID": "course-info/resources.html#additional-advanced-courses",
    "href": "course-info/resources.html#additional-advanced-courses",
    "title": "Resources",
    "section": "Additional Advanced Courses",
    "text": "Additional Advanced Courses\n\nRosenow Group (Theory), Master Course on Statistical Mechanics of Deep Learning",
    "crumbs": [
      "Course Info",
      "Ressources"
    ]
  },
  {
    "objectID": "course-info/resources.html#python-documentation",
    "href": "course-info/resources.html#python-documentation",
    "title": "Resources",
    "section": "Python Documentation",
    "text": "Python Documentation\n\nPython\nMatplotlib\nPandas",
    "crumbs": [
      "Course Info",
      "Ressources"
    ]
  },
  {
    "objectID": "course-info/resources.html#python-tutorials",
    "href": "course-info/resources.html#python-tutorials",
    "title": "Resources",
    "section": "Python Tutorials",
    "text": "Python Tutorials\n\nIntroduction to Python for Science\nNice MatPlotLib tutorial",
    "crumbs": [
      "Course Info",
      "Ressources"
    ]
  },
  {
    "objectID": "course-info/resources.html#further-reading",
    "href": "course-info/resources.html#further-reading",
    "title": "Resources",
    "section": "Further Reading",
    "text": "Further Reading\n\nPython Official Documentation on Functions\nReal Python: Python Loops and Iterators\nPython Conditional Statements\n\nFor interactive practice:\n\nCodecademy Python Course\nHackerRank Python Practice",
    "crumbs": [
      "Course Info",
      "Ressources"
    ]
  },
  {
    "objectID": "course-info/resources.html#marimo-reactive-notebook",
    "href": "course-info/resources.html#marimo-reactive-notebook",
    "title": "Resources",
    "section": "Marimo Reactive Notebook",
    "text": "Marimo Reactive Notebook\n\nMarimo Webpage",
    "crumbs": [
      "Course Info",
      "Ressources"
    ]
  },
  {
    "objectID": "course-info/resources.html#julia-tutorial",
    "href": "course-info/resources.html#julia-tutorial",
    "title": "Resources",
    "section": "Julia Tutorial",
    "text": "Julia Tutorial\n\nJulia Programming Language",
    "crumbs": [
      "Course Info",
      "Ressources"
    ]
  },
  {
    "objectID": "course-info/resources.html#pluto-notebook",
    "href": "course-info/resources.html#pluto-notebook",
    "title": "Resources",
    "section": "Pluto NoteBook",
    "text": "Pluto NoteBook\n\nPluto GitHub Webpage",
    "crumbs": [
      "Course Info",
      "Ressources"
    ]
  },
  {
    "objectID": "course-info/how_to_quiz.html",
    "href": "course-info/how_to_quiz.html",
    "title": "Interactive Python Quiz",
    "section": "",
    "text": "In this quiz, you can write and execute Python code directly in your browser.\n\n\nWrite a function square(n) that returns the square of a number.\n\n\n\n\n\n# Write your Python code here\ndef square(n):\n    return n * n\n\nprint(square(5))\n\n\nRun Code"
  },
  {
    "objectID": "course-info/how_to_quiz.html#question-1",
    "href": "course-info/how_to_quiz.html#question-1",
    "title": "Interactive Python Quiz",
    "section": "",
    "text": "Write a function square(n) that returns the square of a number.\n\n\n\n\n\n# Write your Python code here\ndef square(n):\n    return n * n\n\nprint(square(5))\n\n\nRun Code"
  },
  {
    "objectID": "course-info/intructors.html",
    "href": "course-info/intructors.html",
    "title": "Instructor",
    "section": "",
    "text": "Linnéstr. 5, 04103 Leipzig\nOffice: 322\nPhone: +49 341 97 32571\nEmail: lastname@physik.uni-leipzig.de",
    "crumbs": [
      "Course Info",
      "Instructor"
    ]
  },
  {
    "objectID": "course-info/intructors.html#prof.-dr.-frank-cichos",
    "href": "course-info/intructors.html#prof.-dr.-frank-cichos",
    "title": "Instructor",
    "section": "",
    "text": "Linnéstr. 5, 04103 Leipzig\nOffice: 322\nPhone: +49 341 97 32571\nEmail: lastname@physik.uni-leipzig.de",
    "crumbs": [
      "Course Info",
      "Instructor"
    ]
  },
  {
    "objectID": "course-info/assignments.html",
    "href": "course-info/assignments.html",
    "title": "Assignments",
    "section": "",
    "text": "A total of 6 exercise sheets will be provided. The exercise sheets are part of the examination! The exercise sheets are not graded, but completing them is required for the successful completion of the module. More detailed information on the evaluation of the exercise sheets can be found on the Exam page.\n\nProvision and Submission of Exercises\n\nPublication: Every Tuesday at 1:00 PM (starting second week)\nSubmission deadline: By the following Tuesday at 12:00 PM\nProcessing period: One week (minus one hour)\nPlatform: University Moodle\n\nBoth for working on and submitting assignments\n\nImportant:\n\nMake sure to submit on time within the specified deadline.\nAfter the deadline expires, the assignment is no longer available and cannot be submitted.\nPlease note that submitting the same assignment multiple times reduces the score by 10% with each submission\n\n\n\n\nMoodle page with Assignments\n\n\n\nMoodle Link",
    "crumbs": [
      "Course Info",
      "Assignments"
    ]
  },
  {
    "objectID": "course-info/website.html",
    "href": "course-info/website.html",
    "title": "Webpages",
    "section": "",
    "text": "These Webpages\nThis website contains all the information required for our course Computer Based Physical Modelling. You will find a new lecture and a new assignment here each week. The lecture notes are accompanied by videos that explain the content of the lecture in English, but you can also get by with just reading. From these web pages, you will be guided to various resources that you can use to learn programming in Python. We will use some great tools from the internet, such as:\n\nGoogle Colab service, to host Jupyter Notebooks (https://colab.research.google.com). The Google Colab project provides a useful environment for sharing notebooks.\n\n\n\ngoogle colab screen\n\n\nWhile we don’t use direct Colab links in this course, you can use this service on your own to work with the Jupyter Notebooks. Google Colab provides a free, cloud-based environment that allows you to run Python code without installing anything on your local machine. If you prefer to use Colab, you can upload the notebook files to your Google Drive and open them with Colab.\nGitHub and GitHub Pages service for hosting websites (https://github.com). GitHub is a great place to host your collaborative coding projects including version control. In the upper right corner, you will also find a link to the GitHub repository where the notebooks are hosted.\n\n\n\ngithub screen\n\n\nAnaconda Jupyter package for notebooks on your own computer (https://www.anaconda.com/distribution/). The anaconda package provides you with the Jupyter Notebook environment including Python. If you want to use Jupyter at home without online access, this is a good package to install.\n\n\n\nanaconda screen"
  },
  {
    "objectID": "course-info/exam.html",
    "href": "course-info/exam.html",
    "title": "Exams",
    "section": "",
    "text": "The examination in this case consists of the following parts:\n\nExercises:\n\n6 series of assignments that are not graded\nAt least 50% of the total points for all assignments must be achieved.\n\nTwo Tests:\n\nEach test lasts 45 minutes.\nThe test will take place in person.\nThe tests will be announced in advance.\nThe scores from both tests are added together to give an overall grade.\n\n\nBoth parts (exercises and tests) must be passed to successfully complete the module.",
    "crumbs": [
      "Course Info",
      "Exams"
    ]
  },
  {
    "objectID": "course-info/exam.html#first-time-participants",
    "href": "course-info/exam.html#first-time-participants",
    "title": "Exams",
    "section": "",
    "text": "The examination in this case consists of the following parts:\n\nExercises:\n\n6 series of assignments that are not graded\nAt least 50% of the total points for all assignments must be achieved.\n\nTwo Tests:\n\nEach test lasts 45 minutes.\nThe test will take place in person.\nThe tests will be announced in advance.\nThe scores from both tests are added together to give an overall grade.\n\n\nBoth parts (exercises and tests) must be passed to successfully complete the module.",
    "crumbs": [
      "Course Info",
      "Exams"
    ]
  },
  {
    "objectID": "course-info/exam.html#students-that-already-did-the-module",
    "href": "course-info/exam.html#students-that-already-did-the-module",
    "title": "Exams",
    "section": "Students that already did the module",
    "text": "Students that already did the module\n\nStudents that did the module in SS 2024: The examination consists of exercises and project submission.\n\nIf you have achieved ≥ 50% in the summer semester 2024: Only project submission required\nIf you have achieved &lt; 50% in the summer semester 2024: Exercises and project submission required\n\nStudents that did the module in SS 2023: The examination consists of the submission of a project and a 5-minute video about your project instead of the exercises.\nEnrollment:\n\nNew registration for the module not necessary when you are repeating the module.\nFor Moodle access: Contact Andrea Kramer (firstname.surname@uni-leipzig.de) via university email.\n\nProject Submission:\n\nDeadline: September 3, 2025, 1:00 pm sharp.\nLater Submissions will not be considered.",
    "crumbs": [
      "Course Info",
      "Exams"
    ]
  },
  {
    "objectID": "course-info/course_info.html",
    "href": "course-info/course_info.html",
    "title": "Course Information",
    "section": "",
    "text": "Instructor\nEmail: lastname@physik.uni-leipzig.de\n\nProf. Dr. Frank Cichos\n\nLinnéstr. 5, 04103 Leipzig\nOffice: 322\nPhone: +0341 97 32571"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CBPM 2025",
    "section": "",
    "text": "Welcome to Computer-Based Physical Modelling!\nThe programming language Python is useful for all kinds of scientific and technical tasks. You can use it to analyze and visualize data. You can also use it to numerically solve scientific problems that are difficult or impossible to solve analytically. Python is freely available and, due to its modular structure, has been expanded with an almost infinite number of modules for various purposes.\nThis course aims to introduce you to programming with Python. It is primarily aimed at beginners, but we hope it will also be interesting for advanced users. We begin the course with an introduction to the Jupyter Notebook environment, which we will use throughout the entire course. Afterward, we will provide an introduction to Python and show you some basic functions, such as plotting and analyzing data through curve fitting, reading and writing files, which are some of the tasks you will encounter during your physics studies. We will also show you some advanced topics such as animation in Jupyter and the simulation of physical processes in\n\nMechanics\nElectrostatics\nWaves\nOptics\n\nIf there is time left at the end of the course, we will also take a look at machine learning methods, which have become an important tool in physics as well.\nWe will not present a comprehensive list of numerical simulation schemes, but rather use the examples to spark your curiosity. Since there are slight differences in the syntax of the various Python versions, we will always refer to the Python 3 standard in the following.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "seminars/seminar05/md4.html",
    "href": "seminars/seminar05/md4.html",
    "title": "Step-by-Step Development of a Molecular Dynamics Simulation",
    "section": "",
    "text": "In the last seminar we have defined the class Atom that represents an atom in the simulation. This time, we would like to a force field to the simulation. We will use for out simulations the Lennard-Jones potential that we have had a look at initiall. We will implement this force field in a class ForceField that will contain the parameters of the force field and the methods to calculate the forces between the atoms."
  },
  {
    "objectID": "seminars/seminar05/md4.html#the-forcefield-class",
    "href": "seminars/seminar05/md4.html#the-forcefield-class",
    "title": "Step-by-Step Development of a Molecular Dynamics Simulation",
    "section": "The ForceField Class",
    "text": "The ForceField Class\nThe force field is a class that contains the parameters of the force field and the methods to calculate the forces between the atoms. The class ForceField has the following attributes:\n\nsigma: The parameter sigma of the Lennard-Jones potential\nepsilon: The parameter epsilon of the Lennard-Jones potential\n\nThese parameters are specific for each atom type. We will store these parameters in a dictionary where the keys are the atom types and the values are dictionaries containing the parameters sigma and epsilon. The class ForceField also contains the box size of the simulation. This is needed to apply periodic boundary conditions.\nclass ForceField:\n    def __init__(self):\n        self.parameters = {\n            'C': {'epsilon': 1.615, 'sigma': 1.36},\n            'H': {'epsilon': 1.0, 'sigma': 1.0 },\n            'O': {'epsilon': 1.846, 'sigma': 3.0},\n        }\n        self.box_size = None  # Will be set when initializing the simulation\nYou will have certainly noticed that the parameters I defined do not correspond to the real values of the Lennard-Jones potential. Remember that the values for the hydrogen atom are typically\n\n\\(\\sigma \\approx 2.38\\) Å = \\(2.38 \\times 10^{-10}\\) meters\n\\(\\epsilon \\approx 0.0167\\) kcal/mol = \\(1.16 \\times 10^{-21}\\) joules\n\nThese are all small numbers and we will use larger values to make the simulation more stable. Actually, the Lenard-Jones potential provides a natural length and energy scale for the simulation. The length scale is the parameter \\(\\sigma\\) and the energy scale is the parameter \\(\\epsilon\\). We can therefore set \\(\\sigma_{LJ}=1\\) and \\(\\epsilon_{LJ}=1\\) and scale all other parameters accordingly. This is a common practice in molecular dynamics simulations.\nDue to this rescaling energy, temperature and time units are also not the same as in the real world. We will use the following units:\n\nEnergy: \\(\\epsilon_{LJ} = \\epsilon_{H}/\\epsilon_{H} = 1\\)\nLength: \\(\\sigma_{LJ} = 1\\)\nMass: \\(m_{LJ} = 1\\)\n\nThis means now that all energies, for example, have to be scales by _{H} also the thermal energy. As thermal energy is related to temperature, then the temperature of the Lennard-Jones system\n\\[\nT_{LJ}=\\frac{k_B T}{\\epsilon_{LJ}}\n\\]\nwhich is, in the case of using the hydrogen energy scale, \\(T_{LJ}=3.571\\). for \\(T=300\\, K\\). For the time scale, we have to consider the mass of the hydrogen atom. The time scale is given by\n\\[\nt_{LJ}=\\frac{t}{\\sigma}\\sqrt{\\frac{\\epsilon}{m_{H}}}\n\\]\nThus a time unit of \\(1\\, fs\\) corresponds to \\(t_{LJ}=0.099\\). Thus using a timestep of 0.01 in reduced units would correspond to a real world timestep of just 1 fs. The table below shows the conversion factors for the different units. Simulating a Lennard-Jones system in reduced units therefore allows you to rescale to a real systems with the help of these conversion factors.\n\\[\n\\begin{array}{c|c}\n\\hline \\mathrm{r}^* & \\mathrm{r} \\sigma^{-1} \\\\\n\\hline \\mathrm{~m}^* & \\mathrm{mM}^{-1} \\\\\n\\hline \\mathrm{t}^* & \\mathrm{t} \\sigma^{-1} \\sqrt{\\epsilon / M} \\\\\n\\hline \\mathrm{~T}^* & \\mathrm{k}_B T \\epsilon^{-1} \\\\\n\\hline \\mathrm{E}^* & \\mathrm{E} \\epsilon^{-1} \\\\\n\\hline \\mathrm{~F}^* & \\mathrm{~F} \\sigma \\epsilon^{-1} \\\\\n\\hline \\mathrm{P}^* & \\mathrm{P} \\sigma^3 \\epsilon^{-1} \\\\\n\\hline \\mathrm{v}^* & \\mathrm{v} \\sqrt{M / \\epsilon} \\\\\n\\hline \\rho^* & \\mathrm{~N} \\sigma^3 V^{-1} \\\\\n\\hline\n\\end{array}\n\\]\n\nApply mixing rules when needed\n\nget_pair_parameters\nWhen we looked at the Lennard-Jones potential we realized that it reflects the pair interaction between the same atoms. However, in a molecular dynamics simulation, we have different atoms interacting with each other. We need to define the parameters of the interaction between different atoms. This is done using mixing rules. The most common mixing rule is the Lorentz-Berthelot mixing rule. The parameters of the interaction between two different atoms are calculated as follows:\ndef get_pair_parameters(self, type1, type2):\n    # Apply mixing rules when needed\n    eps1 = self.parameters[type1]['epsilon']\n    eps2 = self.parameters[type2]['epsilon']\n    sig1 = self.parameters[type1]['sigma']\n    sig2 = self.parameters[type2]['sigma']\n\n    # Lorentz-Berthelot mixing rules\n    epsilon = np.sqrt(eps1 * eps2)\n    sigma = (sig1 + sig2) / 2\n\n    return epsilon, sigma\nWe therefore introduce the method get_pair_parameters that calculates the parameters of the Lennard-Jones potential between two different atoms. The method takes the atom types as arguments and returns the parameters epsilon and sigma of the Lennard-Jones potential between these two atoms. The method applies the Lorentz-Berthelot mixing rules to calculate the parameters. The method returns the parameters epsilon and sigma of the Lennard-Jones potential between the two atoms.\n\n\n\nApply minimum image convention\n\nminimum_image_distance\nSimilarly, we already realized that using a finite box size requires to introduce boundary conditions. We decided that periodic boundary conditions are actually most convinient. However, this is introducing a new problem. When we calculate the distance between two atoms, we have to consider the minimum image distance. This means that we have to consider the distance between two atoms in the nearest image. This is done by applying the minimum image convention. The method minimum_image_distance calculates the minimum image distance between two positions. The method takes the positions of the two atoms as arguments and returns the minimum image distance between the two positions. The method applies the minimum image convention to calculate the minimum image distance.\ndef minimum_image_distance(self, pos1, pos2):\n    \"\"\"Calculate minimum image distance between two positions\"\"\"\n    delta = pos1 - pos2\n    # Apply minimum image convention\n    delta = delta - self.box_size * np.round(delta / self.box_size)\n    return delta\n\n\n\nCalculate the Lennard-Jones force between two atoms\n\ncalculate_lj_force\nFinally we can calculate the Lennard-Jones force between two atoms. The method calculate_lj_force calculates the Lennard-Jones force between two atoms. The method takes the two atoms as arguments and returns the force between the two atoms. The method calculates the Lennard-Jones force between the two atoms using the Lennard-Jones potential. The method returns the force between the two atoms.\ndef calculate_lj_force(self, atom1, atom2):\n    epsilon, sigma = self.get_pair_parameters(atom1.type, atom2.type)\n    r = self.minimum_image_distance(atom1.position, atom2.position)\n    r_mag = np.linalg.norm(r)\n\n    # Add cutoff distance for stability\n    if r_mag &gt; 2.5*sigma:\n        return np.zeros(2)\n\n    force_mag = 24 * epsilon * (\n        2 * (sigma/r_mag)**13\n        - (sigma/r_mag)**7\n    )\n    force = force_mag * r/r_mag\n    return force\nWith these parts we have now a complete force field class which we can add to our simulation code.\n\n\n\n\n\n\nComplete ForceField class\n\n\n\n\n\nclass ForceField:\n    def __init__(self):\n        self.parameters = {\n            'C': {'epsilon': 1.615, 'sigma': 1.36},\n            'H': {'epsilon': 1.0, 'sigma': 1.0 },\n            'O': {'epsilon': 1.846, 'sigma': 3.0},\n        }\n        self.box_size = None  # Will be set when initializing the simulation\n\n    def get_pair_parameters(self, type1, type2):\n        # Apply mixing rules when needed\n        eps1 = self.parameters[type1]['epsilon']\n        eps2 = self.parameters[type2]['epsilon']\n        sig1 = self.parameters[type1]['sigma']\n        sig2 = self.parameters[type2]['sigma']\n\n        # Lorentz-Berthelot mixing rules\n        epsilon = np.sqrt(eps1 * eps2)\n        sigma = (sig1 + sig2) / 2\n\n        return epsilon, sigma\n\n    def minimum_image_distance(self, pos1, pos2):\n        \"\"\"Calculate minimum image distance between two positions\"\"\"\n        delta = pos1 - pos2\n        # Apply minimum image convention\n        delta = delta - self.box_size * np.round(delta / self.box_size)\n        return delta\n\n    def calculate_lj_force(self, atom1, atom2):\n        epsilon, sigma = self.get_pair_parameters(atom1.type, atom2.type)\n        r = self.minimum_image_distance(atom1.position, atom2.position)\n        r_mag = np.linalg.norm(r)\n\n        # Add cutoff distance for stability\n        if r_mag &gt; 2.5*sigma:\n            return np.zeros(2)\n\n        force_mag = 24 * epsilon * (\n            2 * (sigma/r_mag)**13\n            - (sigma/r_mag)**7\n        )\n        force = force_mag * r/r_mag\n        return force"
  },
  {
    "objectID": "seminars/seminar05/md4.html#md-simulation-class",
    "href": "seminars/seminar05/md4.html#md-simulation-class",
    "title": "Step-by-Step Development of a Molecular Dynamics Simulation",
    "section": "MD Simulation Class",
    "text": "MD Simulation Class\nThe last thing we need to do is to implement the MD simulation class. This class will be responsible for running the simulation. It is the controller of the simulation, who coordinates everything. By keeping this in a class you may even run several simulations simultaneously. This is not the case here, but it is a good practice to keep the simulation in a class.\n\nMDSimulation class constructor\nThis is just the constructor of the MD Simulation class. It takes the atoms, the force field, the timestep, and the box size as input. It initializes the simulation with the given parameters and sets the initial energy of the system to None. It also initializes an empty list to store the energy history of the system. The latter ones are not used for the moment but could be important later.\nclass MDSimulation:\n    def __init__(self, atoms, forcefield, timestep, box_size):\n        self.atoms = atoms\n        self.forcefield = forcefield\n        self.forcefield.box_size = box_size  # Set box size in forcefield\n        self.timestep = timestep\n        self.box_size = np.array(box_size)\n        self.initial_energy = None\n        self.energy_history = []\n\n\ncalculate_forces method\nThe calculate_forces method calculates the forces between all pairs of atoms in the system. It first resets all forces on the atoms to zero. Then, it calculates the forces between all pairs of atoms using the Lennard-Jones force calculation from the force field class. The method updates the forces on the atoms accordingly. The method does not return anything.\n    def calculate_forces(self):\n        # Reset all forces\n        for atom in self.atoms:\n            atom.reset_force()\n\n        # Calculate forces between all pairs\n        for i, atom1 in enumerate(self.atoms):\n            for atom2 in self.atoms[i+1:]:\n                force = self.forcefield.calculate_lj_force(atom1, atom2)\n                atom1.add_force(force)\n                atom2.add_force(-force)  # Newton's third law\n\n\nupdate_positions_and_velocities method\nThe update_positions_and_velocities method does exactly what its name says. It first of all updates the positions by calling the specific method of the atom. Then it is applying periodic boundary conditions. After that, it stores the current forces for the velocity update. Then it recalculates the forces with the new positions. Finally, it updates the velocities using the average of the old and new forces. The method does not return anything.\n    def update_positions_and_velocities(self):\n        # First step: Update positions using current forces\n        for atom in self.atoms:\n            atom.update_position(self.timestep)\n            # Apply periodic boundary conditions\n            atom.apply_periodic_boundaries(self.box_size)\n\n        # Store current forces for velocity update\n        old_forces = {atom.id: atom.force.copy() for atom in self.atoms}\n\n        # Recalculate forces with new positions\n        self.calculate_forces()\n\n        # Second step: Update velocities using average of old and new forces\n        for atom in self.atoms:\n            atom.update_velocity(self.timestep, atom.force)\nWith these methods, we have a complete simulation class that can run a molecular dynamics simulation for a given number of steps. The simulation class will keep track of the energy of the system at each step, which can be used to analyze the behavior of the system over time.\n\n\n\n\n\n\nComplete MDSimulation class\n\n\n\n\n\n\nclass MDSimulation:\n    def __init__(self, atoms, forcefield, timestep, box_size):\n        self.atoms = atoms\n        self.forcefield = forcefield\n        self.forcefield.box_size = box_size  # Set box size in forcefield\n        self.timestep = timestep\n        self.box_size = np.array(box_size)\n        self.initial_energy = None\n        self.energy_history = []\n\n\n    def calculate_forces(self):\n        # Reset all forces\n        for atom in self.atoms:\n            atom.reset_force()\n\n        # Calculate forces between all pairs\n        for i, atom1 in enumerate(self.atoms):\n            for atom2 in self.atoms[i+1:]:\n                force = self.forcefield.calculate_lj_force(atom1, atom2)\n                atom1.add_force(force)\n                atom2.add_force(-force)  # Newton's third law\n\n    def update_positions_and_velocities(self):\n        # First step: Update positions using current forces\n        for atom in self.atoms:\n            atom.update_position(self.timestep)\n            # Apply periodic boundary conditions\n            atom.apply_periodic_boundaries(self.box_size)\n\n        # Store current forces for velocity update\n        old_forces = {atom.id: atom.force.copy() for atom in self.atoms}\n\n        # Recalculate forces with new positions\n        self.calculate_forces()\n\n        # Second step: Update velocities using average of old and new forces\n        for atom in self.atoms:\n            atom.update_velocity(self.timestep, atom.force)\n\n\n\n\nNow we have the atom class, the force field class, and the simulation class. We can use these classes to run a molecular dynamics simulation of a simple Lennard-Jones system. In the next seminar, we still have to find a way to\n\ninitialize the positions of the atoms in an appropriate way\nto provide them with a velocity distribution that matches the temperature of the system\nto run the simulation and keep the temperature constant\nto trace the energy in the system over time"
  },
  {
    "objectID": "seminars/seminar03/md2.html",
    "href": "seminars/seminar03/md2.html",
    "title": "Step-by-Step Development of a Molecular Dynamics Simulation",
    "section": "",
    "text": "In the previous seminar, we learned about the key components of a molecular dynamics simulation:\n\nThe Lennard-Jones potential describing forces between atoms\nThe Velocity Verlet algorithm for updating positions and velocities\n\nNow we’ll implement these concepts in code. To organize our simulation, we’ll have to think about some issues:\n\n\nIn the previous example, we have assumed that the particle is in free fall. That means eventually it would bounce against the floor. In a real simulation, we need to consider boundary conditions as well. For example, if the particle hits the ground we could implement a simple reflection rule. This is called reflecting boundary conditions and would introduce some additional effects to the simulation. On the other side, one could make the system “kind of” infinitely large by introducing periodic boundary conditions. This means that if a particle leaves the simulation box on one side, it re-enters on the opposite side. This is a common approach in molecular dynamics simulations.\n\n\n\nPerdiodic Boundary Conditions\n\n\n\n\n\n\n\n\nThe Minimum Image Convention in Molecular Dynamics\n\n\n\n\n\nWhen we simulate particles in a box with periodic boundary conditions (meaning particles that leave on one side reappear on the opposite side), we need to calculate the forces between them correctly. Imagine two particles near opposite edges of the box: one at position x=1 and another at x=9 in a box of length 10. Without the minimum image convention, we would calculate their distance as 8 units (9-1). However, due to the periodic boundaries, these particles could actually interact across the boundary, with a shorter distance of just 2 units! The minimum image convention automatically finds this shortest distance, ensuring that we calculate the forces between particles correctly. It’s like taking a shortcut across the periodic boundary instead of walking the longer path through the box.\n\n\n\n\n\n\nThe question we have to think about now is how to implement these formulas in a numerical simulation. The goal is to simulate the motion of many atoms in a box. Each atom is different and has its own position, velocity, and force. Consequently we need to store these quantities for each atom, though the structure in which we store them is the same for each atom. All atoms with their properties actually belong to the same class of objects. We can therefore use a very suitable concept of object-oriented programming, the class.\nA class in object-oriented programming is a blueprint for creating objects (a particular data structure), providing initial values for state (member variables or attributes), and implementations of behavior (member functions or methods). The class is a template for objects, and an object is an instance of a class. The class defines the properties and behavior common to all objects of the class. The objects are the instances of the class that contain the actual data.\nThink of the Atom class as a container for everything we need to know about a single atom:\n\nIts position (where is it?)\nIts velocity (how fast is it moving?)\nThe forces acting on it (what’s pushing or pulling it?)\nIts type (is it hydrogen, oxygen, etc.?)\nIts mass (how heavy is it?)\n\n\n\n\nWe also have a set of forces, that is acting between the atoms. These forces are calculated based on the positions of the atoms. The force fields are all the same for the atoms only the parameters are different. We can represent the force field as a class as well. We will first implement the Lennard-Jones potential in the class. Later we will implement more complex force fields. We will realize that we will later have to introduce different parameters for the Lenard Jones potential for different atom types. We will store these parameters in a dictionary. This dictionary will be part of the force field class and actually represent the Force Field.\nIf atoms are of the same type, they will have the same parameters. However, if they are of different types we will have to mix the parameters. This is done by the mixing rules. We will implement the Lorentz-Berthelot mixing rules. These rules are used to calculate the parameters for the interaction between two different atom types.\n\n\nFor two different atoms (A and B), the Lennard-Jones parameters \\(\\sigma\\) and \\(\\epsilon\\) are calculated using:\n\nArithmetic mean for \\(\\sigma\\) (Lorentz rule):\n\\[\\sigma_{AB} = \\frac{\\sigma_A + \\sigma_B}{2}\\]\nGeometric mean for \\(\\epsilon\\) (Berthelot rule):\n\\[\\epsilon_{AB} = \\sqrt{\\epsilon_A \\epsilon_B}\\]\n\nThese parameters are then used in the Lennard-Jones potential:\n\\[V_{LJ}(r) = 4\\epsilon_{AB}\\left[\\left(\\frac{\\sigma_{AB}}{r}\\right)^{12} - \\left(\\frac{\\sigma_{AB}}{r}\\right)^6\\right]\\]\n\n\n\n\nIn the previous example, we have started with a particle at rest. In a real simulation, we would like to start with a certain temperature. This means that the particles have a certain velocity distribution. We can introduce this by assigning random velocities to the particles. The velocities should be drawn from a Maxwell-Boltzmann distribution. This is a distribution that describes the velocity distribution of particles in at a certain temperature. The distribution is given by:\n\\[\nf_v\\left(v_x\\right)=\\sqrt{\\frac{m}{2 \\pi k_B T}} \\exp \\left[\\frac{-m v_x^2}{2 k_B T}\\right]\n\\]\nwhere \\(m\\) is the mass of the particle, \\(k_B\\) is Boltzmann’s constant, and \\(T\\) is the temperature. \\(v_x\\) is the velocity in the x-direction. The velocities in the y and z directions are drawn in the same way. The temperature of the system is related to the kinetic energy of the particles.\n\n\n\n\n\n\nMaxwell-Boltzmann Velocities in 3D\n\n\n\n\n\nThe probability distribution for the velocity magnitude in 3D is:\n\\[f(v) = 4\\pi v^2 \\left(\\frac{m}{2\\pi k_BT}\\right)^{3/2} \\exp\\left(-\\frac{mv^2}{2k_BT}\\right)\\]\n\nMean velocity magnitude:\n\\[\\langle v \\rangle = \\sqrt{\\frac{8k_BT}{\\pi m}}\\]\nMost probable velocity (peak of distribution):\n\\[v_{mp} = \\sqrt{\\frac{2k_BT}{m}}\\]\nRoot mean square velocity:\n\\[v_{rms} = \\sqrt{\\frac{3k_BT}{m}}\\]\n\nThese velocities can also be expressed in terms of the kinetic energy of the particles. The average kinetic energy per particle is:\n\\[\\langle E_{kin} \\rangle = \\frac{3}{2}k_BT\\]\nThen we can express the velocities as:\n\nMean velocity magnitude:\n\\[\\langle v \\rangle = \\sqrt{\\frac{8\\langle E_{kin} \\rangle}{3\\pi m}}\\]\nMost probable velocity:\n\\[v_{mp} = \\sqrt{\\frac{4\\langle E_{kin} \\rangle}{3m}}\\]\nRoot mean square velocity:\n\\[v_{rms} = \\sqrt{\\frac{2\\langle E_{kin} \\rangle}{m}}\\]\n\n\n\n\n\n\nCode\n# Constants\nkb = 1.380649e-23  # Boltzmann constant in J/K\nm_H = 1.6735575e-27  # Mass of hydrogen atom in kg\nT = 300  # Temperature in K\n\n# Velocity range for plotting\nv = np.linspace(-10000, 10000, 1000)  # m/s\nv_mag = np.linspace(0, 10000, 1000)  # m/s\n\n# Maxwell-Boltzmann distribution for x-component\ndef MB_1D(v, m, T):\n    return np.sqrt(m/(2*np.pi*kb*T)) * np.exp(-m*v**2/(2*kb*T))\n\n# Maxwell-Boltzmann distribution for velocity magnitude in 2D\ndef MB_2D_mag(v, m, T):\n    return v * m/(kb*T) * np.exp(-m*v**2/(2*kb*T))\n\n# Create figure\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=get_size(16, 8))\n\n# Plot x-component distribution\nax1.plot(v, MB_1D(v, m_H, T))\nax1.set_xlabel('Velocity [m/s]')\nax1.set_ylabel('Probability density')\n\n# Plot magnitude distribution\nax2.plot(v_mag, MB_2D_mag(v_mag, m_H, T))\nax2.set_xlabel('Velocity magnitude [m/s]')\nax2.set_ylabel('Probability density')\nax2.axvline(np.sqrt(kb*T/m_H), color='r', linestyle='--', label='Most probable velocity')\nax2.axvline(np.sqrt(2)*np.sqrt(kb*T/m_H), color='g', linestyle='--', label='Mean velocity')\n\nplt.tight_layout()\nplt.show()\n\n# Print most probable velocity\nv_mp_1D = 0  # Most probable velocity for 1D is zero\nv_mp_2D = np.sqrt(kb*T/m_H)  # Most probable velocity magnitude in 2D\nprint(f\"Most probable velocity magnitude in 2D: {v_mp_2D:.1f} m/s\")\nprint(f\"Mean velocity magnitude in 2D: {np.sqrt(2)*v_mp_2D:.1f} m/s\")\n\n\n\n\n\n\n\n\nFigure 1: Maxwell Boltzmann distribution of speeds for one component of the velocity and the magnitude of the velocity vector.\n\n\n\n\n\nMost probable velocity magnitude in 2D: 1573.2 m/s\nMean velocity magnitude in 2D: 2224.8 m/s\n\n\nThe temperature T in a 2D system is related to the kinetic energy by:\n\\[T = \\frac{2K}{N_f k_B}\\]\nwhere:\n\nK is the total kinetic energy: \\(K = \\sum_i \\frac{1}{2}m_i v_i^2\\)\n\\(N_f\\) is the number of degrees of freedom (2N in 2D, where N is number of particles)\n\\(k_B\\) is Boltzmann’s constant (often set to 1 in reduced units)\n\nTo scale to a target temperature \\(T_{target}\\), we multiply velocities by \\(\\sqrt{\\frac{T_{target}}{T_{current}}}\\)\n\n\n\nFinally, we need a class that controls the simulation. This class will contain the main loop of the simulation, where the integration algorithm is called in each time step. It will also contain the methods to calculate the forces between the atoms.\n\n\n\nBefore we implement all classes, we will first visualize the particles moving in a 2D box. We will use the matplotlib library to create an animation of the particles moving in the box. We will also implement periodic boundary conditions, so that particles that leave the box on one side re-enter on the opposite side.\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom IPython.display import clear_output\nfrom scipy.spatial.distance import cdist\n\nn_side =2\n\n1x = np.linspace(0.05, 0.95, n_side)\ny = np.linspace(0.05, 0.95, n_side)\n2xx, yy = np.meshgrid(x, y)\n3particles = np.vstack([xx.ravel(), yy.ravel()]).T\n\nvelocities = np.random.normal(scale=0.005, size=(n_side**2, 2))\n\nradius = 0.0177\nfig, ax = plt.subplots(figsize=(9,9))\n\nn_steps = 200\n\n4for _ in range(n_steps):\n5    clear_output(wait=True)\n\n    # Update particle positions based on their velocities\n    particles += velocities\n    # Apply periodic boundary conditions in x direction (wrap around at 0 and 1)\n    particles[:, 0] = particles[:, 0] % 1\n    # Apply periodic boundary conditions in y direction (wrap around at 0 and 1)\n    particles[:, 1] = particles[:, 1] % 1\n    # Calculate distances between all pairs of particles\n    distances = cdist(particles, particles)\n\n    # Calculate collisions using the upper triangle of the distance matrix\n    # distances &lt; 2*radius gives a boolean matrix where True means collision\n    # np.triu takes only the upper triangle to avoid counting collisions twice\n    collisions = np.triu(distances &lt; 2*radius, 1)\n\n    # Handle collisions between particles\n    for i, j in zip(*np.nonzero(collisions)):\n        # Exchange velocities between colliding particles (elastic collision)\n6        velocities[i], velocities[j] = velocities[j], velocities[i].copy()\n\n        # Calculate how much particles overlap\n        overlap = 2*radius - distances[i, j]\n\n        # Calculate unit vector pointing from j to i\n        direction = particles[i] - particles[j]\n        direction /= np.linalg.norm(direction)\n\n        # Move particles apart to prevent overlap\n        particles[i] += 0.5 * overlap * direction\n        particles[j] -= 0.5 * overlap * direction\n\n    ax.scatter(particles[:, 0], particles[:, 1], s=100, edgecolors='r', facecolors='none')\n\n\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    ax.axis(\"off\")\n\n    display(fig)\n    plt.pause(0.01)\n\n    # Clear the current plot to prepare for next frame\n    ax.clear()\n\n1\n\nCreate a 1D array of x and y-coordinates for the particles.\n\n2\n\nCreate a meshgrid of x and y-coordinates.\n\n3\n\nFlatten the meshgrid to get a 2D array of particle positions.\n\n4\n\nSimulation loop\n\n5\n\nClear the output to display the animation in a single cell.\n\n6\n\nHandle collisions between particles by exchanging velocities and moving particles apart to prevent overlap. The exchange of velocities in your code works because of the conservation of momentum and energy:\n\n\n\nFor two particles of equal mass m in a head-on elastic collision: Before collision:\n\nMomentum: \\(p = mv_1 + mv_2\\)\nEnergy: \\(E = \\frac{1}{2}mv_1^2 + \\frac{1}{2}mv_2^2\\)\n\nAfter collision (with velocity exchange): - Momentum: \\(p = mv_2 + mv_1\\) (same as before!) - Energy: \\(E = \\frac{1}{2}mv_2^2 + \\frac{1}{2}mv_1^2\\) (same as before!)\n\n\n\nCode\nfrom IPython.display import clear_output\nfrom scipy.spatial.distance import cdist\nfrom scipy.stats import maxwell\n\n# Increase number of particles for better statistics\nn_side = 10  # Creates 100 particles\nx = np.linspace(0.05, 0.95, n_side)\ny = np.linspace(0.05, 0.95, n_side)\nxx, yy = np.meshgrid(x, y)\nparticles = np.vstack([xx.ravel(), yy.ravel()]).T\n\n# Initialize with normal distribution\nvelocities = np.random.normal(scale=0.005, size=(n_side**2, 2))\nradius = 0.0177\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=get_size(15, 8))\nn_steps = 500\n\n# Store velocity magnitudes for analysis\nvelocity_history = []\n\nfor step in range(n_steps):\n    clear_output(wait=True)\n    # Update particle positions based on their velocities\n    particles += velocities\n\n    # Apply periodic boundary conditions\n    particles = particles % 1\n\n    # Calculate distances between all pairs of particles\n    distances = cdist(particles, particles)\n\n    # Calculate collisions using the upper triangle of the distance matrix\n    collisions = np.triu(distances &lt; 2*radius, 1)\n\n    # Handle collisions between particles\n    for i, j in zip(*np.nonzero(collisions)):\n        # Get particle positions and velocities\n        pos_i, pos_j = particles[i], particles[j]\n        vel_i, vel_j = velocities[i], velocities[j]\n\n        # Calculate relative position vector (line of centers)\n        r_ij = pos_i - pos_j\n        dist = np.linalg.norm(r_ij)\n        r_ij_normalized = r_ij / dist if dist &gt; 0 else np.array([1, 0])\n\n        # *** CORRECTED COLLISION PHYSICS ***\n        # Split velocities into components parallel and perpendicular to collision axis\n        v_i_parallel = np.dot(vel_i, r_ij_normalized) * r_ij_normalized\n        v_i_perp = vel_i - v_i_parallel\n\n        v_j_parallel = np.dot(vel_j, r_ij_normalized) * r_ij_normalized\n        v_j_perp = vel_j - v_j_parallel\n\n        # Exchange parallel components (proper elastic collision)\n        velocities[i] = v_i_perp + v_j_parallel\n        velocities[j] = v_j_perp + v_i_parallel\n\n        # Move particles apart to prevent overlap\n        overlap = 2*radius - dist\n        particles[i] += 0.5 * overlap * r_ij_normalized\n        particles[j] -= 0.5 * overlap * r_ij_normalized\n\n    # Every 5 steps, record velocity magnitudes\n    if step % 5 == 0:\n        speeds = np.sqrt(np.sum(velocities**2, axis=1))\n        velocity_history.extend(speeds)\n\n    # Every 20 steps, update the visualization\n    if step % 20 == 0:\n        # Clear the current plot to prepare for next frame\n        ax1.clear()\n        ax2.clear()\n\n        # Plot particles\n        ax1.scatter(particles[:, 0], particles[:, 1], s=100, edgecolors='r', facecolors='none')\n        ax1.set_xlim(0, 1)\n        ax1.set_ylim(0, 1)\n        ax1.set_title(f\"Step {step}\")\n        ax1.axis(\"off\")\n\n        # Plot velocity distribution\n        if velocity_history:\n            # Plot histogram of speeds\n            hist, bins = np.histogram(velocity_history, bins=30, density=True)\n            bin_centers = 0.5 * (bins[1:] + bins[:-1])\n            ax2.bar(bin_centers, hist, width=bins[1]-bins[0], alpha=0.7, label='Simulation')\n\n            # Plot Maxwell-Boltzmann distribution for comparison\n            # For 2D Maxwell-Boltzmann, use Rayleigh distribution parameters\n            scale = np.std(velocity_history) / np.sqrt(1 - 2/np.pi)\n            x = np.linspace(0, max(velocity_history), 100)\n\n            # In 2D, speed follows Rayleigh distribution\n            rayleigh_pdf = (x/scale**2) * np.exp(-x**2/(2*scale**2))\n            ax2.plot(x, rayleigh_pdf, 'r-', lw=2, label='Maxwell-Boltzmann (2D)')\n\n            ax2.set_title(\"Speed Distribution\")\n            ax2.set_xlabel(\"Speed\")\n            ax2.set_ylabel(\"Probability Density\")\n            ax2.legend()\n\n        display(fig)\n        plt.pause(0.01)\n\n# Final velocity distribution\nplt.figure(figsize=get_size(12, 8))\nhist, bins = np.histogram(velocity_history, bins=30, density=True)\nbin_centers = 0.5 * (bins[1:] + bins[:-1])\nplt.bar(bin_centers, hist, width=bins[1]-bins[0], alpha=0.7, label='Simulation')\n\n# Plot ideal Maxwell-Boltzmann distribution for 2D (Rayleigh)\nscale = np.std(velocity_history) / np.sqrt(1 - 2/np.pi)\nx = np.linspace(0, max(velocity_history), 100)\nrayleigh_pdf = (x/scale**2) * np.exp(-x**2/(2*scale**2))\nplt.plot(x, rayleigh_pdf, 'r-', lw=2, label='MB (2D)')\n\n\nplt.xlabel(r\"speed $v$\")\nplt.ylabel(\"probability density\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2: Simulated and analytica for the Maxwell-Boltzmann distribution.",
    "crumbs": [
      "Seminars",
      "Seminar03",
      "MD Simulation 2"
    ]
  },
  {
    "objectID": "seminars/seminar03/md2.html#from-theory-to-code",
    "href": "seminars/seminar03/md2.html#from-theory-to-code",
    "title": "Step-by-Step Development of a Molecular Dynamics Simulation",
    "section": "",
    "text": "In the previous seminar, we learned about the key components of a molecular dynamics simulation:\n\nThe Lennard-Jones potential describing forces between atoms\nThe Velocity Verlet algorithm for updating positions and velocities\n\nNow we’ll implement these concepts in code. To organize our simulation, we’ll have to think about some issues:\n\n\nIn the previous example, we have assumed that the particle is in free fall. That means eventually it would bounce against the floor. In a real simulation, we need to consider boundary conditions as well. For example, if the particle hits the ground we could implement a simple reflection rule. This is called reflecting boundary conditions and would introduce some additional effects to the simulation. On the other side, one could make the system “kind of” infinitely large by introducing periodic boundary conditions. This means that if a particle leaves the simulation box on one side, it re-enters on the opposite side. This is a common approach in molecular dynamics simulations.\n\n\n\nPerdiodic Boundary Conditions\n\n\n\n\n\n\n\n\nThe Minimum Image Convention in Molecular Dynamics\n\n\n\n\n\nWhen we simulate particles in a box with periodic boundary conditions (meaning particles that leave on one side reappear on the opposite side), we need to calculate the forces between them correctly. Imagine two particles near opposite edges of the box: one at position x=1 and another at x=9 in a box of length 10. Without the minimum image convention, we would calculate their distance as 8 units (9-1). However, due to the periodic boundaries, these particles could actually interact across the boundary, with a shorter distance of just 2 units! The minimum image convention automatically finds this shortest distance, ensuring that we calculate the forces between particles correctly. It’s like taking a shortcut across the periodic boundary instead of walking the longer path through the box.\n\n\n\n\n\n\nThe question we have to think about now is how to implement these formulas in a numerical simulation. The goal is to simulate the motion of many atoms in a box. Each atom is different and has its own position, velocity, and force. Consequently we need to store these quantities for each atom, though the structure in which we store them is the same for each atom. All atoms with their properties actually belong to the same class of objects. We can therefore use a very suitable concept of object-oriented programming, the class.\nA class in object-oriented programming is a blueprint for creating objects (a particular data structure), providing initial values for state (member variables or attributes), and implementations of behavior (member functions or methods). The class is a template for objects, and an object is an instance of a class. The class defines the properties and behavior common to all objects of the class. The objects are the instances of the class that contain the actual data.\nThink of the Atom class as a container for everything we need to know about a single atom:\n\nIts position (where is it?)\nIts velocity (how fast is it moving?)\nThe forces acting on it (what’s pushing or pulling it?)\nIts type (is it hydrogen, oxygen, etc.?)\nIts mass (how heavy is it?)\n\n\n\n\nWe also have a set of forces, that is acting between the atoms. These forces are calculated based on the positions of the atoms. The force fields are all the same for the atoms only the parameters are different. We can represent the force field as a class as well. We will first implement the Lennard-Jones potential in the class. Later we will implement more complex force fields. We will realize that we will later have to introduce different parameters for the Lenard Jones potential for different atom types. We will store these parameters in a dictionary. This dictionary will be part of the force field class and actually represent the Force Field.\nIf atoms are of the same type, they will have the same parameters. However, if they are of different types we will have to mix the parameters. This is done by the mixing rules. We will implement the Lorentz-Berthelot mixing rules. These rules are used to calculate the parameters for the interaction between two different atom types.\n\n\nFor two different atoms (A and B), the Lennard-Jones parameters \\(\\sigma\\) and \\(\\epsilon\\) are calculated using:\n\nArithmetic mean for \\(\\sigma\\) (Lorentz rule):\n\\[\\sigma_{AB} = \\frac{\\sigma_A + \\sigma_B}{2}\\]\nGeometric mean for \\(\\epsilon\\) (Berthelot rule):\n\\[\\epsilon_{AB} = \\sqrt{\\epsilon_A \\epsilon_B}\\]\n\nThese parameters are then used in the Lennard-Jones potential:\n\\[V_{LJ}(r) = 4\\epsilon_{AB}\\left[\\left(\\frac{\\sigma_{AB}}{r}\\right)^{12} - \\left(\\frac{\\sigma_{AB}}{r}\\right)^6\\right]\\]\n\n\n\n\nIn the previous example, we have started with a particle at rest. In a real simulation, we would like to start with a certain temperature. This means that the particles have a certain velocity distribution. We can introduce this by assigning random velocities to the particles. The velocities should be drawn from a Maxwell-Boltzmann distribution. This is a distribution that describes the velocity distribution of particles in at a certain temperature. The distribution is given by:\n\\[\nf_v\\left(v_x\\right)=\\sqrt{\\frac{m}{2 \\pi k_B T}} \\exp \\left[\\frac{-m v_x^2}{2 k_B T}\\right]\n\\]\nwhere \\(m\\) is the mass of the particle, \\(k_B\\) is Boltzmann’s constant, and \\(T\\) is the temperature. \\(v_x\\) is the velocity in the x-direction. The velocities in the y and z directions are drawn in the same way. The temperature of the system is related to the kinetic energy of the particles.\n\n\n\n\n\n\nMaxwell-Boltzmann Velocities in 3D\n\n\n\n\n\nThe probability distribution for the velocity magnitude in 3D is:\n\\[f(v) = 4\\pi v^2 \\left(\\frac{m}{2\\pi k_BT}\\right)^{3/2} \\exp\\left(-\\frac{mv^2}{2k_BT}\\right)\\]\n\nMean velocity magnitude:\n\\[\\langle v \\rangle = \\sqrt{\\frac{8k_BT}{\\pi m}}\\]\nMost probable velocity (peak of distribution):\n\\[v_{mp} = \\sqrt{\\frac{2k_BT}{m}}\\]\nRoot mean square velocity:\n\\[v_{rms} = \\sqrt{\\frac{3k_BT}{m}}\\]\n\nThese velocities can also be expressed in terms of the kinetic energy of the particles. The average kinetic energy per particle is:\n\\[\\langle E_{kin} \\rangle = \\frac{3}{2}k_BT\\]\nThen we can express the velocities as:\n\nMean velocity magnitude:\n\\[\\langle v \\rangle = \\sqrt{\\frac{8\\langle E_{kin} \\rangle}{3\\pi m}}\\]\nMost probable velocity:\n\\[v_{mp} = \\sqrt{\\frac{4\\langle E_{kin} \\rangle}{3m}}\\]\nRoot mean square velocity:\n\\[v_{rms} = \\sqrt{\\frac{2\\langle E_{kin} \\rangle}{m}}\\]\n\n\n\n\n\n\nCode\n# Constants\nkb = 1.380649e-23  # Boltzmann constant in J/K\nm_H = 1.6735575e-27  # Mass of hydrogen atom in kg\nT = 300  # Temperature in K\n\n# Velocity range for plotting\nv = np.linspace(-10000, 10000, 1000)  # m/s\nv_mag = np.linspace(0, 10000, 1000)  # m/s\n\n# Maxwell-Boltzmann distribution for x-component\ndef MB_1D(v, m, T):\n    return np.sqrt(m/(2*np.pi*kb*T)) * np.exp(-m*v**2/(2*kb*T))\n\n# Maxwell-Boltzmann distribution for velocity magnitude in 2D\ndef MB_2D_mag(v, m, T):\n    return v * m/(kb*T) * np.exp(-m*v**2/(2*kb*T))\n\n# Create figure\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=get_size(16, 8))\n\n# Plot x-component distribution\nax1.plot(v, MB_1D(v, m_H, T))\nax1.set_xlabel('Velocity [m/s]')\nax1.set_ylabel('Probability density')\n\n# Plot magnitude distribution\nax2.plot(v_mag, MB_2D_mag(v_mag, m_H, T))\nax2.set_xlabel('Velocity magnitude [m/s]')\nax2.set_ylabel('Probability density')\nax2.axvline(np.sqrt(kb*T/m_H), color='r', linestyle='--', label='Most probable velocity')\nax2.axvline(np.sqrt(2)*np.sqrt(kb*T/m_H), color='g', linestyle='--', label='Mean velocity')\n\nplt.tight_layout()\nplt.show()\n\n# Print most probable velocity\nv_mp_1D = 0  # Most probable velocity for 1D is zero\nv_mp_2D = np.sqrt(kb*T/m_H)  # Most probable velocity magnitude in 2D\nprint(f\"Most probable velocity magnitude in 2D: {v_mp_2D:.1f} m/s\")\nprint(f\"Mean velocity magnitude in 2D: {np.sqrt(2)*v_mp_2D:.1f} m/s\")\n\n\n\n\n\n\n\n\nFigure 1: Maxwell Boltzmann distribution of speeds for one component of the velocity and the magnitude of the velocity vector.\n\n\n\n\n\nMost probable velocity magnitude in 2D: 1573.2 m/s\nMean velocity magnitude in 2D: 2224.8 m/s\n\n\nThe temperature T in a 2D system is related to the kinetic energy by:\n\\[T = \\frac{2K}{N_f k_B}\\]\nwhere:\n\nK is the total kinetic energy: \\(K = \\sum_i \\frac{1}{2}m_i v_i^2\\)\n\\(N_f\\) is the number of degrees of freedom (2N in 2D, where N is number of particles)\n\\(k_B\\) is Boltzmann’s constant (often set to 1 in reduced units)\n\nTo scale to a target temperature \\(T_{target}\\), we multiply velocities by \\(\\sqrt{\\frac{T_{target}}{T_{current}}}\\)\n\n\n\nFinally, we need a class that controls the simulation. This class will contain the main loop of the simulation, where the integration algorithm is called in each time step. It will also contain the methods to calculate the forces between the atoms.\n\n\n\nBefore we implement all classes, we will first visualize the particles moving in a 2D box. We will use the matplotlib library to create an animation of the particles moving in the box. We will also implement periodic boundary conditions, so that particles that leave the box on one side re-enter on the opposite side.\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom IPython.display import clear_output\nfrom scipy.spatial.distance import cdist\n\nn_side =2\n\n1x = np.linspace(0.05, 0.95, n_side)\ny = np.linspace(0.05, 0.95, n_side)\n2xx, yy = np.meshgrid(x, y)\n3particles = np.vstack([xx.ravel(), yy.ravel()]).T\n\nvelocities = np.random.normal(scale=0.005, size=(n_side**2, 2))\n\nradius = 0.0177\nfig, ax = plt.subplots(figsize=(9,9))\n\nn_steps = 200\n\n4for _ in range(n_steps):\n5    clear_output(wait=True)\n\n    # Update particle positions based on their velocities\n    particles += velocities\n    # Apply periodic boundary conditions in x direction (wrap around at 0 and 1)\n    particles[:, 0] = particles[:, 0] % 1\n    # Apply periodic boundary conditions in y direction (wrap around at 0 and 1)\n    particles[:, 1] = particles[:, 1] % 1\n    # Calculate distances between all pairs of particles\n    distances = cdist(particles, particles)\n\n    # Calculate collisions using the upper triangle of the distance matrix\n    # distances &lt; 2*radius gives a boolean matrix where True means collision\n    # np.triu takes only the upper triangle to avoid counting collisions twice\n    collisions = np.triu(distances &lt; 2*radius, 1)\n\n    # Handle collisions between particles\n    for i, j in zip(*np.nonzero(collisions)):\n        # Exchange velocities between colliding particles (elastic collision)\n6        velocities[i], velocities[j] = velocities[j], velocities[i].copy()\n\n        # Calculate how much particles overlap\n        overlap = 2*radius - distances[i, j]\n\n        # Calculate unit vector pointing from j to i\n        direction = particles[i] - particles[j]\n        direction /= np.linalg.norm(direction)\n\n        # Move particles apart to prevent overlap\n        particles[i] += 0.5 * overlap * direction\n        particles[j] -= 0.5 * overlap * direction\n\n    ax.scatter(particles[:, 0], particles[:, 1], s=100, edgecolors='r', facecolors='none')\n\n\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    ax.axis(\"off\")\n\n    display(fig)\n    plt.pause(0.01)\n\n    # Clear the current plot to prepare for next frame\n    ax.clear()\n\n1\n\nCreate a 1D array of x and y-coordinates for the particles.\n\n2\n\nCreate a meshgrid of x and y-coordinates.\n\n3\n\nFlatten the meshgrid to get a 2D array of particle positions.\n\n4\n\nSimulation loop\n\n5\n\nClear the output to display the animation in a single cell.\n\n6\n\nHandle collisions between particles by exchanging velocities and moving particles apart to prevent overlap. The exchange of velocities in your code works because of the conservation of momentum and energy:\n\n\n\nFor two particles of equal mass m in a head-on elastic collision: Before collision:\n\nMomentum: \\(p = mv_1 + mv_2\\)\nEnergy: \\(E = \\frac{1}{2}mv_1^2 + \\frac{1}{2}mv_2^2\\)\n\nAfter collision (with velocity exchange): - Momentum: \\(p = mv_2 + mv_1\\) (same as before!) - Energy: \\(E = \\frac{1}{2}mv_2^2 + \\frac{1}{2}mv_1^2\\) (same as before!)\n\n\n\nCode\nfrom IPython.display import clear_output\nfrom scipy.spatial.distance import cdist\nfrom scipy.stats import maxwell\n\n# Increase number of particles for better statistics\nn_side = 10  # Creates 100 particles\nx = np.linspace(0.05, 0.95, n_side)\ny = np.linspace(0.05, 0.95, n_side)\nxx, yy = np.meshgrid(x, y)\nparticles = np.vstack([xx.ravel(), yy.ravel()]).T\n\n# Initialize with normal distribution\nvelocities = np.random.normal(scale=0.005, size=(n_side**2, 2))\nradius = 0.0177\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=get_size(15, 8))\nn_steps = 500\n\n# Store velocity magnitudes for analysis\nvelocity_history = []\n\nfor step in range(n_steps):\n    clear_output(wait=True)\n    # Update particle positions based on their velocities\n    particles += velocities\n\n    # Apply periodic boundary conditions\n    particles = particles % 1\n\n    # Calculate distances between all pairs of particles\n    distances = cdist(particles, particles)\n\n    # Calculate collisions using the upper triangle of the distance matrix\n    collisions = np.triu(distances &lt; 2*radius, 1)\n\n    # Handle collisions between particles\n    for i, j in zip(*np.nonzero(collisions)):\n        # Get particle positions and velocities\n        pos_i, pos_j = particles[i], particles[j]\n        vel_i, vel_j = velocities[i], velocities[j]\n\n        # Calculate relative position vector (line of centers)\n        r_ij = pos_i - pos_j\n        dist = np.linalg.norm(r_ij)\n        r_ij_normalized = r_ij / dist if dist &gt; 0 else np.array([1, 0])\n\n        # *** CORRECTED COLLISION PHYSICS ***\n        # Split velocities into components parallel and perpendicular to collision axis\n        v_i_parallel = np.dot(vel_i, r_ij_normalized) * r_ij_normalized\n        v_i_perp = vel_i - v_i_parallel\n\n        v_j_parallel = np.dot(vel_j, r_ij_normalized) * r_ij_normalized\n        v_j_perp = vel_j - v_j_parallel\n\n        # Exchange parallel components (proper elastic collision)\n        velocities[i] = v_i_perp + v_j_parallel\n        velocities[j] = v_j_perp + v_i_parallel\n\n        # Move particles apart to prevent overlap\n        overlap = 2*radius - dist\n        particles[i] += 0.5 * overlap * r_ij_normalized\n        particles[j] -= 0.5 * overlap * r_ij_normalized\n\n    # Every 5 steps, record velocity magnitudes\n    if step % 5 == 0:\n        speeds = np.sqrt(np.sum(velocities**2, axis=1))\n        velocity_history.extend(speeds)\n\n    # Every 20 steps, update the visualization\n    if step % 20 == 0:\n        # Clear the current plot to prepare for next frame\n        ax1.clear()\n        ax2.clear()\n\n        # Plot particles\n        ax1.scatter(particles[:, 0], particles[:, 1], s=100, edgecolors='r', facecolors='none')\n        ax1.set_xlim(0, 1)\n        ax1.set_ylim(0, 1)\n        ax1.set_title(f\"Step {step}\")\n        ax1.axis(\"off\")\n\n        # Plot velocity distribution\n        if velocity_history:\n            # Plot histogram of speeds\n            hist, bins = np.histogram(velocity_history, bins=30, density=True)\n            bin_centers = 0.5 * (bins[1:] + bins[:-1])\n            ax2.bar(bin_centers, hist, width=bins[1]-bins[0], alpha=0.7, label='Simulation')\n\n            # Plot Maxwell-Boltzmann distribution for comparison\n            # For 2D Maxwell-Boltzmann, use Rayleigh distribution parameters\n            scale = np.std(velocity_history) / np.sqrt(1 - 2/np.pi)\n            x = np.linspace(0, max(velocity_history), 100)\n\n            # In 2D, speed follows Rayleigh distribution\n            rayleigh_pdf = (x/scale**2) * np.exp(-x**2/(2*scale**2))\n            ax2.plot(x, rayleigh_pdf, 'r-', lw=2, label='Maxwell-Boltzmann (2D)')\n\n            ax2.set_title(\"Speed Distribution\")\n            ax2.set_xlabel(\"Speed\")\n            ax2.set_ylabel(\"Probability Density\")\n            ax2.legend()\n\n        display(fig)\n        plt.pause(0.01)\n\n# Final velocity distribution\nplt.figure(figsize=get_size(12, 8))\nhist, bins = np.histogram(velocity_history, bins=30, density=True)\nbin_centers = 0.5 * (bins[1:] + bins[:-1])\nplt.bar(bin_centers, hist, width=bins[1]-bins[0], alpha=0.7, label='Simulation')\n\n# Plot ideal Maxwell-Boltzmann distribution for 2D (Rayleigh)\nscale = np.std(velocity_history) / np.sqrt(1 - 2/np.pi)\nx = np.linspace(0, max(velocity_history), 100)\nrayleigh_pdf = (x/scale**2) * np.exp(-x**2/(2*scale**2))\nplt.plot(x, rayleigh_pdf, 'r-', lw=2, label='MB (2D)')\n\n\nplt.xlabel(r\"speed $v$\")\nplt.ylabel(\"probability density\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2: Simulated and analytica for the Maxwell-Boltzmann distribution.",
    "crumbs": [
      "Seminars",
      "Seminar03",
      "MD Simulation 2"
    ]
  },
  {
    "objectID": "seminars/seminar01/01-seminar01.html",
    "href": "seminars/seminar01/01-seminar01.html",
    "title": "Seminar 1",
    "section": "",
    "text": "Let’s test your understanding of Python data types!\n\n\nWhat is the output of the following code?\na = [1, 2, 3]\nb = (1, 2, 3)\nprint(type(a), type(b))\n\n&lt;class 'list'&gt; &lt;class 'list'&gt;\n&lt;class 'list'&gt; &lt;class 'tuple'&gt;\n&lt;class 'tuple'&gt; &lt;class 'list'&gt;\n&lt;class 'tuple'&gt; &lt;class 'tuple'&gt;\n\nWhich of the following is mutable?\n\nList\nTuple\nString\nInteger\n\nWhat will be the output of this code?\nmy_dict = {'a': 1, 'b': 2, 'c': 3}\nprint(my_dict['b'])\n\na\n2\nb\nKeyError\n\nHow do you create an empty set in Python?\n\n{}\n[]\nset()\n()\n\nWhat is the result of 3 + 4.0?\n\n7\n7.0\n‘7.0’\nTypeError\n\n\n\n\n\n\n\n\n\nClick to reveal answers\n\n\n\n\n\n\n&lt;class 'list'&gt; &lt;class 'tuple'&gt;\nList\n2\nset()\n7.0",
    "crumbs": [
      "Seminars",
      "Seminar01",
      "Seminar Quizz"
    ]
  },
  {
    "objectID": "seminars/seminar01/01-seminar01.html#quiz-data-types-in-python",
    "href": "seminars/seminar01/01-seminar01.html#quiz-data-types-in-python",
    "title": "Seminar 1",
    "section": "",
    "text": "Let’s test your understanding of Python data types!\n\n\nWhat is the output of the following code?\na = [1, 2, 3]\nb = (1, 2, 3)\nprint(type(a), type(b))\n\n&lt;class 'list'&gt; &lt;class 'list'&gt;\n&lt;class 'list'&gt; &lt;class 'tuple'&gt;\n&lt;class 'tuple'&gt; &lt;class 'list'&gt;\n&lt;class 'tuple'&gt; &lt;class 'tuple'&gt;\n\nWhich of the following is mutable?\n\nList\nTuple\nString\nInteger\n\nWhat will be the output of this code?\nmy_dict = {'a': 1, 'b': 2, 'c': 3}\nprint(my_dict['b'])\n\na\n2\nb\nKeyError\n\nHow do you create an empty set in Python?\n\n{}\n[]\nset()\n()\n\nWhat is the result of 3 + 4.0?\n\n7\n7.0\n‘7.0’\nTypeError\n\n\n\n\n\n\n\n\n\nClick to reveal answers\n\n\n\n\n\n\n&lt;class 'list'&gt; &lt;class 'tuple'&gt;\nList\n2\nset()\n7.0",
    "crumbs": [
      "Seminars",
      "Seminar01",
      "Seminar Quizz"
    ]
  },
  {
    "objectID": "seminars/seminar01/mdX.html",
    "href": "seminars/seminar01/mdX.html",
    "title": "Computer-Based Physical Modelling",
    "section": "",
    "text": "The Lennard-Jones potential describes the interaction between two atoms:\n\\[V_{LJ}(r) = 4\\epsilon \\left[\\left(\\frac{\\sigma}{r}\\right)^{12} - \\left(\\frac{\\sigma}{r}\\right)^6\\right]\\]\nThe corresponding force:\n\\[F_{LJ}(r) = -\\frac{dV_{LJ}}{dr} = 24\\epsilon \\left[2\\left(\\frac{\\sigma}{r}\\right)^{12} - \\left(\\frac{\\sigma}{r}\\right)^6\\right]\\frac{\\vec{r}}{r^2}\\]\nwhere: - \\(\\epsilon\\) is the depth of the potential well - \\(\\sigma\\) is the distance at which the potential is zero - \\(r\\) is the distance between particles\n\n\n\nimport numpy as np\n\ndef lennard_jones_force(pos1, pos2, epsilon=1.0, sigma=1.0):\n    r_vec = pos2 - pos1\n    r = np.linalg.norm(r_vec)\n\n    # Force magnitude\n    force_mag = 24 * epsilon * (2 * (sigma/r)**12 - (sigma/r)**6) / r\n\n    # Force vector\n    force_vec = force_mag * r_vec / r\n\n    return force_vec"
  },
  {
    "objectID": "seminars/seminar01/mdX.html#simple-atomic-system-with-lennard-jones-potential",
    "href": "seminars/seminar01/mdX.html#simple-atomic-system-with-lennard-jones-potential",
    "title": "Computer-Based Physical Modelling",
    "section": "",
    "text": "The Lennard-Jones potential describes the interaction between two atoms:\n\\[V_{LJ}(r) = 4\\epsilon \\left[\\left(\\frac{\\sigma}{r}\\right)^{12} - \\left(\\frac{\\sigma}{r}\\right)^6\\right]\\]\nThe corresponding force:\n\\[F_{LJ}(r) = -\\frac{dV_{LJ}}{dr} = 24\\epsilon \\left[2\\left(\\frac{\\sigma}{r}\\right)^{12} - \\left(\\frac{\\sigma}{r}\\right)^6\\right]\\frac{\\vec{r}}{r^2}\\]\nwhere: - \\(\\epsilon\\) is the depth of the potential well - \\(\\sigma\\) is the distance at which the potential is zero - \\(r\\) is the distance between particles\n\n\n\nimport numpy as np\n\ndef lennard_jones_force(pos1, pos2, epsilon=1.0, sigma=1.0):\n    r_vec = pos2 - pos1\n    r = np.linalg.norm(r_vec)\n\n    # Force magnitude\n    force_mag = 24 * epsilon * (2 * (sigma/r)**12 - (sigma/r)**6) / r\n\n    # Force vector\n    force_vec = force_mag * r_vec / r\n\n    return force_vec"
  },
  {
    "objectID": "seminars/seminar01/mdX.html#boundary-conditions",
    "href": "seminars/seminar01/mdX.html#boundary-conditions",
    "title": "Computer-Based Physical Modelling",
    "section": "2. Boundary Conditions",
    "text": "2. Boundary Conditions\n\n2.1 Periodic Boundary Conditions (PBC)\nParticles that exit one side of the box re-enter from the opposite side.\ndef apply_periodic_bc(positions, box_length):\n    return positions - box_length * np.floor(positions/box_length)\n\ndef minimum_image_distance(pos1, pos2, box_length):\n    dr = pos2 - pos1\n    dr = dr - box_length * np.round(dr/box_length)\n    return dr\n\n\n2.2 Reflective Boundaries\nParticles bounce off the walls:\ndef apply_reflective_bc(positions, velocities, box_length):\n    for i in range(len(positions)):\n        for dim in range(3):\n            if positions[i,dim] &lt; 0:\n                positions[i,dim] = -positions[i,dim]\n                velocities[i,dim] = -velocities[i,dim]\n            elif positions[i,dim] &gt; box_length:\n                positions[i,dim] = 2*box_length - positions[i,dim]\n                velocities[i,dim] = -velocities[i,dim]\n    return positions, velocities"
  },
  {
    "objectID": "seminars/seminar01/mdX.html#basic-simulation-loop",
    "href": "seminars/seminar01/mdX.html#basic-simulation-loop",
    "title": "Computer-Based Physical Modelling",
    "section": "3. Basic Simulation Loop",
    "text": "3. Basic Simulation Loop\n\n3.1 Initial Implementation with Just LJ Forces\nclass MDSimulation:\n    def __init__(self, positions, velocities, mass, box_length, dt):\n        self.positions = positions\n        self.velocities = velocities\n        self.mass = mass\n        self.box_length = box_length\n        self.dt = dt\n\n    def calculate_forces(self):\n        n_particles = len(self.positions)\n        forces = np.zeros_like(self.positions)\n\n        for i in range(n_particles):\n            for j in range(i+1, n_particles):\n                r_ij = minimum_image_distance(\n                    self.positions[i],\n                    self.positions[j],\n                    self.box_length\n                )\n                f_ij = lennard_jones_force(np.zeros(3), r_ij)\n                forces[i] += f_ij\n                forces[j] -= f_ij  # Newton's third law\n\n        return forces\n\n    def velocity_verlet_step(self):\n        # Calculate initial forces\n        forces = self.calculate_forces()\n\n        # Update positions\n        self.positions += self.velocities * self.dt + \\\n                         0.5 * forces / self.mass * self.dt**2\n\n        # Apply boundary conditions\n        self.positions = apply_periodic_bc(self.positions, self.box_length)\n\n        # Calculate new forces\n        new_forces = self.calculate_forces()\n\n        # Update velocities\n        self.velocities += 0.5 * (forces + new_forces) / self.mass * self.dt"
  },
  {
    "objectID": "seminars/seminar01/mdX.html#adding-molecular-forces",
    "href": "seminars/seminar01/mdX.html#adding-molecular-forces",
    "title": "Computer-Based Physical Modelling",
    "section": "4. Adding Molecular Forces",
    "text": "4. Adding Molecular Forces\n\n4.1 Bond Forces\nAdding harmonic bond potential:\n\\[V_{bond}(r) = \\frac{k}{2}(r - r_0)^2\\]\ndef bond_force(pos1, pos2, k_bond, r0):\n    r_vec = pos2 - pos1\n    r = np.linalg.norm(r_vec)\n    force_mag = -k_bond * (r - r0)\n    return force_mag * r_vec / r\n\n\n4.2 Angle Forces\nThree-body angle potential:\n\\[V_{angle}(\\theta) = \\frac{k_\\theta}{2}(\\theta - \\theta_0)^2\\]\ndef angle_force(pos1, pos2, pos3, k_angle, theta0):\n    # Calculate vectors\n    v1 = pos1 - pos2\n    v2 = pos3 - pos2\n\n    # Calculate angle\n    cos_theta = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n    theta = np.arccos(np.clip(cos_theta, -1.0, 1.0))\n\n    # Calculate forces (simplified version)\n    force_magnitude = -k_angle * (theta - theta0)\n\n    return force_magnitude * v1, -force_magnitude * v2\n\n\n4.3 Enhanced Simulation Class\nclass MolecularMDSimulation(MDSimulation):\n    def __init__(self, *args, bonds=None, angles=None):\n        super().__init__(*args)\n        self.bonds = bonds or []  # [(i, j, k, r0), ...]\n        self.angles = angles or []  # [(i, j, k, k_angle, theta0), ...]\n\n    def calculate_forces(self):\n        # Start with non-bonded forces\n        forces = super().calculate_forces()\n\n        # Add bond forces\n        for bond in self.bonds:\n            i, j, k_bond, r0 = bond\n            f_ij = bond_force(\n                self.positions[i],\n                self.positions[j],\n                k_bond, r0\n            )\n            forces[i] += f_ij\n            forces[j] -= f_ij\n\n        # Add angle forces\n        for angle in self.angles:\n            i, j, k, k_angle, theta0 = angle\n            f_i, f_k = angle_force(\n                self.positions[i],\n                self.positions[j],\n                self.positions[k],\n                k_angle, theta0\n            )\n            forces[i] += f_i\n            forces[k] += f_k\n            forces[j] -= (f_i + f_k)\n\n        return forces"
  },
  {
    "objectID": "seminars/seminar01/mdX.html#example-usage",
    "href": "seminars/seminar01/mdX.html#example-usage",
    "title": "Computer-Based Physical Modelling",
    "section": "5. Example Usage",
    "text": "5. Example Usage\n# Initialize system\nn_particles = 100\nbox_length = 10.0\npositions = np.random.rand(n_particles, 3) * box_length\nvelocities = np.zeros((n_particles, 3))\nmass = 1.0\ndt = 0.001\n\n# Create simulation\nsim = MolecularMDSimulation(\n    positions, velocities, mass, box_length, dt,\n    bonds=[(0, 1, 1000.0, 1.0)],  # Example bond\n    angles=[(0, 1, 2, 100.0, np.pi)]  # Example angle\n)\n\n# Run simulation\nn_steps = 1000\nfor step in range(n_steps):\n    sim.velocity_verlet_step()\nHere’s a suggested basic structure:\nclass Atom:\n    def __init__(self, atom_id, atom_type, position, velocity=None, mass=None):\n        self.id = atom_id          # unique identifier\n        self.type = atom_type      # e.g., 'H', 'C', 'O'\n        self.position = position   # numpy array [x, y, z]\n        self.velocity = velocity if velocity is not None else np.zeros(3)\n        self.mass = mass\n        self.force = np.zeros(3)   # current force on atom\n\n        # Optional attributes that might be useful later:\n        self.bonded_atoms = []     # list of atoms this atom is bonded to\n        self.charges = 0.0         # for electrostatic interactions\nThen you can later create classes for: 1. Bond (connects two Atom objects)\nclass Bond:\n    def __init__(self, atom1, atom2, k, r0):\n        self.atom1 = atom1\n        self.atom2 = atom2\n        self.k = k      # force constant\n        self.r0 = r0    # equilibrium distance\n\nAngle (three Atom objects)\n\nclass Angle:\n    def __init__(self, atom1, atom2, atom3, k, theta0):\n        self.atoms = [atom1, atom2, atom3]\n        self.k = k          # force constant\n        self.theta0 = theta0  # equilibrium angle\n\nDihedral (four Atom objects)\n\nThis modular approach makes it easier to: - Add features incrementally - Debug each interaction type separately - Keep track of connectivity - Calculate forces systematically\nclass Atom:\n    def __init__(self, atom_id, atom_type, position, velocity=None, mass=None):\n        self.id = atom_id\n        self.type = atom_type\n        self.position = position\n        self.velocity = velocity if velocity is not None else np.zeros(3)\n        self.mass = mass\n        self.force = np.zeros(3)\n\n    def add_force(self, force):\n        \"\"\"Add force contribution to total force on atom\"\"\"\n        self.force += force\n\n    def reset_force(self):\n        \"\"\"Reset force to zero at start of each step\"\"\"\n        self.force = np.zeros(3)\n\n    def update_position(self, dt):\n        \"\"\"First step of velocity Verlet: update position\"\"\"\n        self.position += self.velocity * dt + 0.5 * (self.force/self.mass) * dt**2\n\n    def update_velocity(self, dt, new_force):\n        \"\"\"Second step of velocity Verlet: update velocity using average force\"\"\"\n        self.velocity += 0.5 * (new_force + self.force)/self.mass * dt\n        self.force = new_force\n\nclass ForceField:\n    def __init__(self, sigma, epsilon):\n        self.sigma = sigma\n        self.epsilon = epsilon\n\n    def calculate_lj_force(self, atom1, atom2):\n        \"\"\"Calculate LJ force between two atoms\"\"\"\n        r = atom1.position - atom2.position\n        r_mag = np.linalg.norm(r)\n        # LJ force calculation\n        force = 24 * self.epsilon * (2 * (self.sigma/r_mag)**12\n                                   - (self.sigma/r_mag)**6) * r/r_mag**2\n        return force\n\nclass MDSimulation:\n    def __init__(self, atoms, forcefield):\n        self.atoms = atoms\n        self.forcefield = forcefield\n\n    def calculate_forces(self):\n        # Reset all forces\n        for atom in self.atoms:\n            atom.reset_force()\n\n        # Calculate forces between all pairs\n        for i, atom1 in enumerate(self.atoms):\n            for atom2 in self.atoms[i+1:]:\n                force = self.forcefield.calculate_lj_force(atom1, atom2)\n                atom1.add_force(force)\n                atom2.add_force(-force)  # Newton's third law\n\nNew lecture\nclass Atom: def init(self, atom_id, atom_type, position, velocity=None, mass=None): self.id = atom_id self.type = atom_type # e.g., ‘A’, ‘B’, etc. self.position = position self.velocity = velocity if velocity is not None else np.zeros(2) self.mass = mass self.force = np.zeros(2)\nclass ForceField: def init(self): # Dictionary to store interaction parameters between atom types self.pair_parameters = {}\ndef add_pair_parameters(self, type1, type2, epsilon, sigma):\n    \"\"\"Add interaction parameters for a pair of atom types\"\"\"\n    # Store parameters symmetrically\n    key = tuple(sorted([type1, type2]))\n    self.pair_parameters[key] = {'epsilon': epsilon, 'sigma': sigma}\n\ndef get_pair_parameters(self, type1, type2):\n    \"\"\"Get interaction parameters for a pair of atom types\"\"\"\n    key = tuple(sorted([type1, type2]))\n    if key not in self.pair_parameters:\n        raise ValueError(f\"No parameters defined for atom types {type1} and {type2}\")\n    return self.pair_parameters[key]\n\ndef calculate_lj_force(self, atom1, atom2, r_vec, r_mag):\n    \"\"\"Calculate LJ force with type-specific parameters\"\"\"\n    params = self.get_pair_parameters(atom1.type, atom2.type)\n    epsilon = params['epsilon']\n    sigma = params['sigma']\n\n    force_mag = 24.0 * epsilon * (2.0 * (sigma/r_mag)**13\n                                - (sigma/r_mag)**7)\n    return force_mag * r_vec / r_mag\n\n\n\n\n    def setup_simulation():\n        # Create force field and add parameters\n        ff = ForceField()\n\n        # Add parameters for different combinations\n        ff.add_pair_parameters('A', 'A', epsilon=1.0, sigma=1.0)  # A-A interaction\n        ff.add_pair_parameters('B', 'B', epsilon=0.5, sigma=1.2)  # B-B interaction\n        ff.add_pair_parameters('A', 'B', epsilon=0.7, sigma=1.1)  # A-B interaction\n\n        # Create atoms of different types\n        atoms = [\n            Atom(0, 'A', np.array([1.0, 1.0]), mass=1.0),\n            Atom(1, 'A', np.array([2.0, 2.0]), mass=1.0),\n            Atom(2, 'B', np.array([3.0, 3.0]), mass=1.5),\n            Atom(3, 'B', np.array([4.0, 4.0]), mass=1.5)\n        ]\n\n        return ff, atoms"
  },
  {
    "objectID": "seminars/seminar01/md1.html",
    "href": "seminars/seminar01/md1.html",
    "title": "Step-by-Step Development of a Molecular Dynamics Simulation",
    "section": "",
    "text": "Real molecular dynamics (MD) simulations are complex and computationally expensive but very cool, as they give you a glimpse into the world of atoms and molecules. Here, we will develop a simple MD simulation from scratch in Python. The goal is to understand the basic concepts and algorithms behind MD simulations and get something running which can be extended later but also what we are proud of at the end of the course.\nBefore we can start with implementing a simulation, we need to understand the basic concepts and algorithms behind MD simulations. The following sections will guide you through the development of a simple MD simulation."
  },
  {
    "objectID": "seminars/seminar01/md1.html#molecular-dynamics-simulations",
    "href": "seminars/seminar01/md1.html#molecular-dynamics-simulations",
    "title": "Step-by-Step Development of a Molecular Dynamics Simulation",
    "section": "",
    "text": "Real molecular dynamics (MD) simulations are complex and computationally expensive but very cool, as they give you a glimpse into the world of atoms and molecules. Here, we will develop a simple MD simulation from scratch in Python. The goal is to understand the basic concepts and algorithms behind MD simulations and get something running which can be extended later but also what we are proud of at the end of the course.\nBefore we can start with implementing a simulation, we need to understand the basic concepts and algorithms behind MD simulations. The following sections will guide you through the development of a simple MD simulation."
  },
  {
    "objectID": "seminars/seminar01/md1.html#basic-physical-concepts",
    "href": "seminars/seminar01/md1.html#basic-physical-concepts",
    "title": "Step-by-Step Development of a Molecular Dynamics Simulation",
    "section": "Basic Physical Concepts",
    "text": "Basic Physical Concepts\n\nNewton’s Equations of Motion\nThe motion of particles in a molecular dynamics simulation is governed by Newton’s equations of motion:\n\\[m_i \\frac{d^2\\vec{r}_i}{dt^2} = \\vec{F}_i\\]\nwhere:\n\n\\(m_i\\) is the mass of particle \\(i\\)\n\\(\\vec{r}_i\\) is the position of particle \\(i\\)\n\\(\\vec{F}_i\\) is the force acting on particle \\(i\\)\n\nThe force acting on a particle is the sum of all forces acting on it:\n\\[\\vec{F}_i = \\sum_{j \\neq i} \\vec{F}_{ij}\\]\nwhere \\(\\vec{F}_{ij}\\) is the force acting on particle \\(i\\) due to particle \\(j\\).\n\n\nPotential Energy Functions and Forces\nThe force \\(\\vec{F}_{ij}\\) is usually derived from a potential energy function and may result from a variety of interactions, such as:\n\nBonded interactions\n\nbond stretching \nbond angle bending \ntorsional interactions \n\nNon-bonded interactions\n\nelectrostatic interactions\nvan der Waals interactions\n\nExternal forces\n\nWe will implement some of them but not all of them.\n\nLennard-Jones Potential\nThe most common potential energy function used in MD simulations is the Lennard-Jones potential. It is belonging to the class of non-bonded interactions. The force and the potential energy of the Lennard-Jones potential are given by:\n\\[V_{LJ}(r) = 4\\epsilon \\left[\\left(\\frac{\\sigma}{r}\\right)^{12} - \\left(\\frac{\\sigma}{r}\\right)^6\\right]\\]\nand\n\\[F_{LJ}(r) = -\\frac{dV_{LJ}}{dr} = 24\\epsilon \\left[2\\left(\\frac{\\sigma}{r}\\right)^{12} - \\left(\\frac{\\sigma}{r}\\right)^6\\right]\\frac{\\vec{r}}{r^2}\\]\nwhere:\n\n\\(\\epsilon\\) is the depth of the potential well\n\\(\\sigma\\) is the distance at which the potential is zero\n\\(r\\) is the distance between particles\n\nThe Lenard Jones potential is good for describing the interaction of non-bonded atoms in a molecular system e.g. in a gas or a liquid and is therefore well suited if we first want to simulate a gas or a liquid.\n\n\nCode\ndef lennard_jones(r, epsilon=1, sigma=1):\n    return 4 * epsilon * ((sigma/r)**12 - (sigma/r)**6)\n\nr = np.linspace(0.8, 3, 1000)\nV = lennard_jones(r)\n\nplt.figure(figsize=get_size(8, 6),dpi=150)\nplt.plot(r, V, 'b-', linewidth=2)\nplt.grid(True)\nplt.xlabel('r/σ')\nplt.ylabel('V/ε')\nplt.title('Lennard-Jones Potential')\nplt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\nplt.ylim(-1.5, 3)\nplt.show()\n\n\n\n\n\n\n\n\n\nThe figure above shows the Lenard-Jones potential as a function of the distance between particles. The potential energy is zero at the equilibrium distance \\(r = \\sigma\\) and has a minimum at \\(r = 2^{1/6}\\sigma\\). The potential energy is positive for \\(r &lt; \\sigma\\) and negative for \\(r &gt; \\sigma\\).\n\n\n\n\n\n\nValues for atomic hydrogen\n\n\n\nFor atomic hydrogen (H), typical Lennard-Jones parameters are:\n\n\\(\\sigma \\approx 2.38\\) Å = \\(2.38 \\times 10^{-10}\\) meters\n\\(\\epsilon \\approx 0.0167\\) kcal/mol = \\(1.16 \\times 10^{-21}\\) joules\n\n\n\nLater, if we manage to advance to some more complicated systems, we may want to introduce:\n\nforce in bonds between two atoms\nforce in bond angles between three atoms\nforce in dihedral angles between four atoms\n\nBut for now, we will stick to the Lennard-Jones potential."
  },
  {
    "objectID": "seminars/seminar01/md1.html#integrating-newtons-euqation-of-motion",
    "href": "seminars/seminar01/md1.html#integrating-newtons-euqation-of-motion",
    "title": "Step-by-Step Development of a Molecular Dynamics Simulation",
    "section": "Integrating Newtons Euqation of Motion",
    "text": "Integrating Newtons Euqation of Motion\nWhen we have the forces on a particle we have in principle its acceleration. To get the velocity and the position of the particle we need to integrate the equations of motion. There are several methods to do this, but we will start with the simplest one, the Euler method.\n\nEuler Method\nTo obtain this one first needs to know about the Taylor expansion of a function in general. The Taylor expansion of a function \\(f(x)\\) around a point \\(x_0\\) is providing an approximation of the function in the vicinity of \\(x_0\\). It is given by:\n\\[f(x) = f(x_0) + f'(x_0)(x - x_0) + \\frac{1}{2}f''(x_0)(x - x_0)^2 + \\cdots\\]\nwhere \\(f'(x_0)\\) is the first derivative of \\(f(x)\\) at \\(x_0\\), \\(f''(x_0)\\) is the second derivative of \\(f(x)\\) at \\(x_0\\), and so on. We can demonstrate that by expanding a sine function around \\(x_0 = 0\\):\n\\[\\sin(x) = \\sin(0) + \\cos(0)x - \\frac{1}{2}\\sin(0)x^2 + \\cdots = x - \\frac{1}{6}x^3 + \\cdots\\]\nPlotting this yields:\n\n\nCode\nx = np.linspace(-2*np.pi, 2*np.pi, 1000)\ny = np.sin(x)\ny_taylor = x - 1/6*x**3\n\nplt.figure(figsize=get_size(8, 6),dpi=150)\nplt.plot(x, y, 'b-', label='sin(x)', linewidth=2)\nplt.plot(x, y_taylor, 'r--', label='Taylor expansion', linewidth=2)\nplt.grid(True)\nplt.xlabel('x')\nplt.ylabel('f(x)')\nplt.xlim(-2,2)\nplt.ylim(-2,2)\nplt.title('Taylor Expansion of sin(x)')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe expansion is therefore a good approximation in a region close to \\(x_0\\).\n\n\nVelocity Verlet Algorithm\nThe velocity Verlet algorithm is a second-order algorithm that is more accurate than the Euler method. It can be derived from the Taylor expansion of the position and velocity vectors\n\\[\\mathbf{r}(t + \\Delta t) = \\mathbf{r}(t) + \\mathbf{v}(t)\\Delta t + \\frac{1}{2}\\frac{\\mathbf{F}(t)}{m}\\Delta t^2+ O(\\Delta t^3)\\]\nThe higher order terms in the Taylor expansion are neglected, which results in an error of order \\(\\Delta t^3\\). As compared to that the Euler method is obtained by neglecting the higher order terms in the Taylor expansion of the velocity vector:\n\\[\\mathbf{v}(t + \\Delta t) = \\mathbf{v}(t) + \\frac{\\mathbf{F}(t)}{m}\\Delta t + O(\\Delta t^2)\\]\nand is therefore only first order accurate with an error of order \\(\\Delta t^2\\).\nThe velocity Verlet algorithm consists of three steps:\n\nUpdate positions: \\(\\mathbf{r}(t + \\Delta t) = \\mathbf{r}(t) + \\mathbf{v}(t)\\Delta t + \\frac{1}{2}\\frac{\\mathbf{F}(t)}{m}\\Delta t^2\\)\nCalculate new forces: \\(\\mathbf{F}(t + \\Delta t) = \\mathbf{F}(\\mathbf{r}(t + \\Delta t))\\)\nUpdate velocities: \\(\\mathbf{v}(t + \\Delta t) = \\mathbf{v}(t) + \\frac{1}{2}\\frac{\\mathbf{F}(t) + \\mathbf{F}(t + \\Delta t)}{m}\\Delta t\\)\n\nwhere: - \\(\\mathbf{r}\\) is the position vector - \\(\\mathbf{v}\\) is the velocity vector - \\(\\mathbf{F}\\) is the force vector - \\(m\\) is the mass - \\(\\Delta t\\) is the timestep\n\n\nSimple Integration Example: Free Fall\nLet’s start and try to integrate the equation of motion for a particle in free fall with the help of the Velocity Verlet algorithm. The only force acting on the particle is gravity. The equation of motion is:\nNewton’s equation of motion: \\(\\mathbf{F} = m\\mathbf{a}\\)\nFor gravity: \\(\\mathbf{F} = -mg\\hat{\\mathbf{y}}\\)\nTherefore: \\(\\ddot{y} = -g\\)\nThe analytical solution is:\n\nPosition: \\(y(t) = y_0 + v_0t - \\frac{1}{2}gt^2\\)\nVelocity: \\(v(t) = v_0 - gt\\)\n\n\n\nCode\n# Parameters\n\ng = 9.81  # m/s^2\ndt = 0.01  # time step\nt_max = 2.0  # total simulation time\nsteps = int(t_max/dt)\n\n# Initial conditions\ny0 = 20.0  # initial height\nv0 = 0.0   # initial velocity\n\n\n# Arrays to store results\nt = np.zeros(steps)\ny = np.zeros(steps)\nv = np.zeros(steps)\na = np.zeros(steps)\n\n# Initial values\ny[0] = y0\nv[0] = v0\na[0] = -g\n\n# Velocity Verlet integration\nfor i in range(1, steps):\n    t[i] = i * dt\n    y[i] = y[i-1] + v[i-1] * dt + 0.5 * a[i-1] * dt**2  # update position\n    a_new = -g                                          # new acceleration (assuming constant gravity)\n    v[i] = v[i-1] + 0.5 * (a[i-1] + a_new) * dt         # update velocity\n    a[i] = a_new                                        # store new acceleration\n\ny_analytical = y0 + v0*t - 0.5*g*t**2\nplt.figure(figsize=get_size(8, 6), dpi=150)\nplt.plot(t, y)\nplt.plot(t, y_analytical, 'r--')\n\nplt.xlabel('Time (s)')\nplt.ylabel('Height (m)')\nplt.title('Free Fall Motion')\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "seminars/seminar06/seminar06.html",
    "href": "seminars/seminar06/seminar06.html",
    "title": "Free coding Seminar",
    "section": "",
    "text": "Today’s seminar will be a free coding session. You’ll have 30 minutes to work on one of the following physics problems. After the coding time, we’ll discuss your approaches, solutions, and any challenges you encountered.\nOpen in New Window\n\n\n\n\n\nPhysics Background: A damped driven pendulum is a classic example of a system that can exhibit chaotic behavior under certain conditions.\nProgramming Task:\n\nImplement the equation of motion for a damped driven pendulum: \\[\\ddot{\\theta} + b\\dot{\\theta} + \\omega_0^2\\sin\\theta = F\\cos(\\Omega t)\\] where \\(b\\) is the damping coefficient, \\(\\omega_0^2 = g/L\\), \\(F\\) is the driving amplitude, and \\(\\Omega\\) is the driving frequency.\nSolve this second-order differential equation numerically using solve_ivp.\nVisualize the pendulum’s motion over time for different parameter values.\nCreate a phase-space plot (\\(\\theta\\) vs. \\(\\dot{\\theta}\\)) to visualize the system’s behavior.",
    "crumbs": [
      "Seminars",
      "Seminar06",
      "Free Coding Seminar"
    ]
  },
  {
    "objectID": "seminars/seminar06/seminar06.html#free-coding-seminar---physics-problems",
    "href": "seminars/seminar06/seminar06.html#free-coding-seminar---physics-problems",
    "title": "Free coding Seminar",
    "section": "",
    "text": "Today’s seminar will be a free coding session. You’ll have 30 minutes to work on one of the following physics problems. After the coding time, we’ll discuss your approaches, solutions, and any challenges you encountered.\nOpen in New Window\n\n\n\n\n\nPhysics Background: A damped driven pendulum is a classic example of a system that can exhibit chaotic behavior under certain conditions.\nProgramming Task:\n\nImplement the equation of motion for a damped driven pendulum: \\[\\ddot{\\theta} + b\\dot{\\theta} + \\omega_0^2\\sin\\theta = F\\cos(\\Omega t)\\] where \\(b\\) is the damping coefficient, \\(\\omega_0^2 = g/L\\), \\(F\\) is the driving amplitude, and \\(\\Omega\\) is the driving frequency.\nSolve this second-order differential equation numerically using solve_ivp.\nVisualize the pendulum’s motion over time for different parameter values.\nCreate a phase-space plot (\\(\\theta\\) vs. \\(\\dot{\\theta}\\)) to visualize the system’s behavior.",
    "crumbs": [
      "Seminars",
      "Seminar06",
      "Free Coding Seminar"
    ]
  },
  {
    "objectID": "lectures/lecture04/01-lecture04.html#introduction-to-object-oriented-programming",
    "href": "lectures/lecture04/01-lecture04.html#introduction-to-object-oriented-programming",
    "title": "Classes and Objects",
    "section": "Introduction to Object Oriented Programming",
    "text": "Introduction to Object Oriented Programming\nImagine you’re simulating a complex physical system—perhaps a collection of interacting particles or cells. Each entity in your simulation has both properties (position, velocity, size) and behaviors (move, interact, divide). How do you organize this complexity in your code?\n\nFrom Procedural to Object-Oriented Thinking\nIn previous lectures, we’ve designed programs using a procedural approach—organizing code around functions that operate on separate data structures. While this works for simpler problems, it can become unwieldy as systems grow more complex.\nObject-oriented programming (OOP) offers a more intuitive paradigm: it combines data and functionality together into self-contained units called objects. Instead of having separate variables and functions, each object maintains its own state and defines its own behaviors.\nFor computational modeling, this is particularly powerful because:\n\nObjects can directly represent the entities you’re modeling (particles, cells, molecules)\nCode organization mirrors the structure of the real-world system\nComplex systems become easier to build incrementally and modify later",
    "crumbs": [
      "Python Basics",
      "Lecture 4",
      "Classes & Objects"
    ]
  },
  {
    "objectID": "lectures/lecture04/01-lecture04.html#the-building-blocks-classes-and-objects",
    "href": "lectures/lecture04/01-lecture04.html#the-building-blocks-classes-and-objects",
    "title": "Classes and Objects",
    "section": "The Building Blocks: Classes and Objects",
    "text": "The Building Blocks: Classes and Objects\nObject-oriented programming is built upon two fundamental concepts: classes and objects.\n\n\n\n\n\n\nFigure 1: Sketch of the relation of classes and objects\n\n\n\n\nClasses: Creating BlueprintsObjects: Creating InstancesProperties: Storing Data\n\n\nA class serves as a blueprint or template that defines a new type of object. Think of it as a mold that creates objects with specific characteristics and behaviors. It specifies:\n\nWhat data the object will store (properties)\nWhat operations the object can perform (methods)\n\n\n\nAn object is a specific instance of a class—a concrete realization of that blueprint. When you create an object, you’re essentially saying “make me a new thing based on this class design.”\nObjects have two main components:\n\nProperties (also called attributes or fields): Variables that store data within the object\nMethods: Functions that define what the object can do and how it manipulates its data\n\n\n\nProperties come in two varieties:\n\nInstance variables: Unique to each object instance (each object has its own copy)\nClass variables: Shared among all instances of the class (one copy for the entire class)\n\nFor example, if you had a Colloid class for a particle simulation:\n\nInstance variables might include radius and position (unique to each particle)\nClass variables might include material_density (same for all colloids of that type)\nMethods might include move() or calculate_volume()",
    "crumbs": [
      "Python Basics",
      "Lecture 4",
      "Classes & Objects"
    ]
  },
  {
    "objectID": "lectures/lecture04/01-lecture04.html#working-with-classes-in-python",
    "href": "lectures/lecture04/01-lecture04.html#working-with-classes-in-python",
    "title": "Classes and Objects",
    "section": "Working with Classes in Python",
    "text": "Working with Classes in Python\n\nCreating a Class\nTo define a class in Python, we use this basic syntax:\nclass ClassName:\n    # Class content goes here\nThe definition starts with the class keyword, followed by the class name, and a colon. The class content is indented and contains all properties and methods of the class.\nLet’s start with a minimal example that represents a colloidal particle:\n\n\n\n\n\n\nEven this empty class is a valid class definition, though it doesn’t do anything useful yet. Let’s start adding functionality to make it more practical.\n\n\nCreating Methods\nMethods are functions that belong to a class. They define the behaviors and capabilities of your objects.\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding self in Python Classes\n\n\n\nEvery method in a Python class automatically receives a special first parameter, conventionally named self. This parameter represents the specific instance of the class that calls the method.\nKey points about self: - It’s automatically passed by Python when you call a method - It gives the method access to the instance’s properties and other methods - By convention, we name it self (though technically you could use any valid name) - You don’t include it when calling the method\nExample:\nclass Colloid:\n    def type(self):  # self is automatically provided\n        print('I am a plastic colloid')\n\n# Usage:\nparticle = Colloid()\nparticle.type()  # Notice: no argument needed for self\nIn this example, even though type() appears to take no arguments when called, Python automatically passes particle as the self parameter.\n\n\n\n\nThe Constructor Method: __init__\nThe __init__ method is a special method called when a new object is created. It lets you initialize the object’s properties with specific values.\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nPython also provides a __del__ method (destructor) that’s called when an object is deleted. This can be useful for cleanup operations or tracking object lifecycles.\n\n\n\n\nString Representation: The __str__ Method\nThe __str__ method defines how an object should be represented as a string. It’s automatically called when: - You use print(object) - You convert the object to a string using str(object)\nThis method helps make your objects more readable and informative:\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe .1f format specification means the radius will be displayed with one decimal place. This helps make your output more readable. You can customize this string representation to show whatever information about your object is most relevant.",
    "crumbs": [
      "Python Basics",
      "Lecture 4",
      "Classes & Objects"
    ]
  },
  {
    "objectID": "lectures/lecture04/01-lecture04.html#managing-data-in-classes",
    "href": "lectures/lecture04/01-lecture04.html#managing-data-in-classes",
    "title": "Classes and Objects",
    "section": "Managing Data in Classes",
    "text": "Managing Data in Classes\n\nClass Variables vs. Instance Variables\nOne of the core features of OOP is how it manages data. Python classes offer two distinct types of variables:\n\nClass Variables: Shared Among All Objects\n\nDefinition: Variables defined directly inside the class but outside any method\nBehavior: All instances of the class share the same copy of these variables\nUsage: For properties that should be the same across all instances\nAccess pattern: Typically accessed as ClassName.variable_name\n\n\n\nInstance Variables: Unique to Each Object\n\nDefinition: Variables defined within methods, typically in __init__\nBehavior: Each object has its own separate copy of these variables\nUsage: For properties that can vary between different instances\nAccess pattern: Typically accessed as self.variable_name within methods\n\nHere’s a practical example showing both types of variables in action:\n\n\n\n\n\n\n\n\n\nWhen to Use Each Type of Variable\n\nUse Class Variables When:\n\nA property should be the same for all instances (like physical constants)\nYou need to track information about the class as a whole (like counters)\nYou want to save memory by not duplicating unchanging values\n\n\n\nUse Instance Variables When:\n\nObjects need their own independent state\nProperties vary between instances (position, size, etc.)\nYou’re representing unique characteristics of individual objects\n\n\n\n\n\n\n\nWarning\n\n\n\nBe careful when modifying class variables! Since they’re shared, changes will affect all instances of the class. This can lead to unexpected behavior if not managed carefully.",
    "crumbs": [
      "Python Basics",
      "Lecture 4",
      "Classes & Objects"
    ]
  },
  {
    "objectID": "lectures/lecture03/01-lecture03.html",
    "href": "lectures/lecture03/01-lecture03.html",
    "title": "Modules",
    "section": "",
    "text": "Most of the functionality in Python is provided by modules. The Python Standard Library is a large collection of modules that provides cross-platform implementations of common facilities such as access to the operating system, file I/O, string management, network communication, math, web-scraping, text manipulation, machine learning and much more.\nTo use a module in a Python program it first has to be imported. A module can be imported using the import statement. For example, to import the module math, which contains many standard mathematical functions, we can do:\n\n\n\n\n\n\nThis includes the whole module and makes it available for use later in the program. Note that the functions of the module are accessed using the prefix math., which is the namespace for the module.\nAlternatively, we can chose to import all symbols (functions and variables) in a module so that we don’t need to use the prefix “math.” every time we use something from the math module:\n\n\n\n\n\n\nThis pattern can be very convenient, but in large programs that include many modules it is often a good idea to keep the symbols from each module in their own namespaces, by using the import math pattern. This would eliminate potentially confusing problems.\n\nNamespaces\n\n\n\n\n\n\nNamespaces\n\n\n\nA namespace is an identifier used to organize objects, e.g. the methods and variables of a module. The prefix math. we have used in the previous section is such a namespace. You may also create your own namespace for a module. This is done by using the import math as mymath pattern.\n\n\n\n\n\n\n\n\nYou may also only import specific functions of a module.\n\n\n\n\n\n\n\n\nDirectory of a module\nOnce a module is imported, we can list the symbols it provides using the dir function:\n\n\n\n\n\n\nAnd using the function help we can get a description of each function (almost .. not all functions have docstrings, as they are technically called, but the vast majority of functions are documented this way).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can also use the help function directly on modules: Try\nhelp(math)\nSome very useful modules from the Python standard library are os, sys, math, shutil, re, subprocess, multiprocessing, threading.\nA complete lists of standard modules for Python 3 is available at the python website .\n\n\nAdvanced topics\n\n\n\n\n\n\nCreate Your Own Modules\n\n\n\n\n\nCreating your own modules in Python is a great way to organize your code and make it reusable. A module is simply a file containing Python definitions and statements. Here’s how you can create and use your own module:\n\nCreating a Module\nTo create a module, you just need to save your Python code in a file with a .py extension. For example, let’s create a module named mymodule.py with the following content:\n# mymodule.py\n\ndef greet(name: str) -&gt; str:\n    return f\"Hello, {name}!\"\n\ndef add(a: int, b: int) -&gt; int:\n    return a + b\n\n\nUsing Your Module\nOnce you have created your module, you can import it into other Python scripts using the import statement. Here’s an example of how to use the mymodule we just created:\n# main.py\n\nimport mymodule\n\n# Use the functions from mymodule\nprint(mymodule.greet(\"Alice\"))\nprint(mymodule.add(5, 3))\n\n\nImporting Specific Functions\nYou can also import specific functions from a module using the from ... import ... syntax:\n# main.py\n\nfrom mymodule import greet, add\n\n# Use the imported functions directly\nprint(greet(\"Bob\"))\nprint(add(10, 7))\n\n\nModule Search Path\nWhen you import a module, Python searches for the module in the following locations: 1. The directory containing the input script (or the current directory if no script is specified). 2. The directories listed in the PYTHONPATH environment variable. 3. The default directory where Python is installed.\nYou can view the module search path by printing the sys.path variable:\nimport sys\nprint(sys.path)\n\n\nCreating Packages\nA package is a way of organizing related modules into a directory hierarchy. A package is simply a directory that contains a special file named __init__.py, which can be empty. Here’s an example of how to create a package:\nmypackage/\n    __init__.py\n    module1.py\n    module2.py\nYou can then import modules from the package using the dot notation:\n# main.py\n\nfrom mypackage import module1, module2\n\n# Use the functions from the modules\nprint(module1.some_function())\nprint(module2.another_function())\nCreating and using modules and packages in Python helps you organize your code better and makes it easier to maintain and reuse.\n\n\nNamespaces in Packages\nYou can also create sub-packages by adding more directories with __init__.py files. This allows you to create a hierarchical structure for your modules:\nmypackage/\n    __init__.py\n    subpackage/\n        __init__.py\n        submodule.py\nYou can then import submodules using the full package name:\n# main.py\n\nfrom mypackage.subpackage import submodule\n\n# Use the functions from the submodule\nprint(submodule.some_sub_function())",
    "crumbs": [
      "Python Basics",
      "Lecture 3",
      "Modules"
    ]
  },
  {
    "objectID": "lectures/04-plotting.html",
    "href": "lectures/04-plotting.html",
    "title": "Plotting",
    "section": "",
    "text": "Data visualization through plotting is a crucial tool for analyzing and interpreting scientific data and theoretical predictions. While plotting capabilities are not built into Python’s core, they are available through various external library modules. Matplotlib is widely recognized as the de facto standard for plotting in Python. However, several other powerful plotting libraries exist, including PlotLy, Seaborn, and Bokeh, each offering unique features and capabilities for data visualization.\nAs Matplotlib is an external library (actually a collection of libraries), it must be imported into any script that uses it. While Matplotlib relies heavily on NumPy, importing NumPy separately is not always necessary for basic plotting. However, for most scientific applications, you’ll likely use both. To create 2D plots, you typically start by importing Matplotlib’s pyplot module:\nThis import introduces the implicit interface of pyplot for creating figures and plots. Matplotlib offers two main interfaces:\nWe will use most of the the the pyplot interface as in the examples below. The section Additional Plotting will refer to the explicit programming of figures.\nWe can set some of the parameters for the appearance of graphs globally. In case you still want to modify a part of it, you can set individual parameters later during plotting. The command used here is the\nfunction, which takes a dictionary with the specific parameters as key."
  },
  {
    "objectID": "lectures/04-plotting.html#simple-plotting",
    "href": "lectures/04-plotting.html#simple-plotting",
    "title": "Plotting",
    "section": "Simple Plotting",
    "text": "Simple Plotting\nMatplotlib offers multiple levels of functionality for creating plots. Throughout this section, we’ll primarily focus on using commands that leverage default settings. This approach simplifies the process, as Matplotlib automatically handles much of the graph layout. These high-level commands are ideal for quickly creating effective visualizations without delving into intricate details. At the end of this section, we’ll briefly touch upon more advanced techniques that provide greater control over plot elements and layout.\n\nAnatomy of a Line Plot\nTo create a basic line plot, use the following command:\nplt.plot(x, y)\nBy default, this generates a line plot. However, you can customize the appearance by adjusting various parameters within the plot() function. For instance, you can modify it to resemble a scatter plot by changing certain arguments. The versatility of this command allows for a range of visual representations beyond simple line plots.\nLet’s create a simple line plot of the sine function over the interval [0, 4π]. We’ll use NumPy to generate the x-values and calculate the corresponding y-values. The following code snippet demonstrates this process:\n1x = np.linspace(0, 4.*np.pi, 100)\n2y = np.sin(x)\n\n3plt.figure(figsize=(4,3))\n4plt.plot(x, y)\n5plt.tight_layout()\n6plt.show()\n\n1\n\nCreate an array of 100 values between 0 and 4π.\n\n2\n\nCalculate the sine of each value in the array.\n\n3\n\ncreate a new figure\n\n4\n\nplot the data\n\n5\n\nautomatically adjust the layout\n\n6\n\nshow the figure\n\n\nHere is the code in a Python cell:\n\n\n\n\n\n\nTry to change the values of the x and y arrays and see how the plot changes.\n\n\n\n\n\n\nWhy use plt.tight_layout()\n\n\n\n\n\nplt.tight_layout() is a very useful function in Matplotlib that automatically adjusts the spacing between plot elements to prevent overlapping and ensure that all elements fit within the figure area. Here’s what it does:\n\nPadding Adjustment: It adjusts the padding between and around subplots to prevent overlapping of axis labels, titles, and other elements.\nSubplot Spacing: It optimizes the space between multiple subplots in a figure.\nText Accommodation: It ensures that all text elements (like titles, labels, and legends) fit within the figure without being cut off.\nMargin Adjustment: It adjusts the margins around the entire figure to make sure everything fits neatly.\nAutomatic Resizing: If necessary, it can slightly resize subplot areas to accommodate all elements.\nLegend Positioning: It takes into account the presence and position of legends when adjusting layouts.\n\nKey benefits of using plt.tight_layout():\n\nIt saves time in manual adjustment of plot elements.\nIt helps create more professional-looking and readable plots.\nIt’s particularly useful when creating figures with multiple subplots or when saving figures to files.\n\nYou typically call plt.tight_layout() just before plt.show() or plt.savefig(). For example:\nplt.figure()\n# ... (your plotting code here)\nplt.tight_layout()\nplt.show()\n\n\n\n\nAxis Labels\nTo enhance the clarity and interpretability of our plots, it’s crucial to provide context through proper labeling. Let’s add descriptive axis labels to our diagram, a practice that significantly improves the readability and comprehension of the data being presented.\nplt.xlabel('x-label')\nplt.ylabel('y-label')\n\n\n\n\n\n\n\n\nLegends\nplt.plot(..., label=r'$\\sin(x)$')\nplt.legend(loc='lower left')\n\n\n\n\n\n\n\n\nPlots with error bars\nWhen plotting experimental data it is customary to include error bars that indicate graphically the degree of uncertainty that exists in the measurement of each data point. The MatPlotLib function errorbar plots data with error bars attached. It can be used in a way that either replaces or augments the plot function. Both vertical and horizontal error bars can be displayed. The figure below illustrates the use of error bars.\n\n\n\n\n\n\n\n\nSaving figures\nTo save a figure to a file we can use the savefig method in the Figure class. Matplotlib can generate high-quality output in a number formats, including PNG, JPG, EPS, SVG, PGF and PDF. For scientific papers, I recommend using PDF whenever possible. (LaTeX documents compiled with pdflatex can include PDFs using the includegraphics command). In some cases, PGF can also be good alternative."
  },
  {
    "objectID": "lectures/04-plotting.html#other-plot-types",
    "href": "lectures/04-plotting.html#other-plot-types",
    "title": "Plotting",
    "section": "Other Plot Types",
    "text": "Other Plot Types\n\nScatter plot\nIf you prefer to use symbols for plotting just use the\nplt.scatter(x,y)\ncommand of pylab. Note that the scatter command requires a x and y values and you can set the marker symbol (see an overview of the marker symbols).\n\n\n\n\n\n\n\n\nHistograms\nA very useful plotting command is also the hist command. It generates a histogram of the data provided. A histogram is a graphical representation of the distribution of numerical data. It is an estimate of the probability distribution of a continuous variable. To construct a histogram, the first step is to “bin” the range of values—that is, divide the entire range of values into a series of intervals—and then count how many values fall into each interval. The bins are usually specified as consecutive, non-overlapping intervals of a variable. The bins must be adjacent, and are often (but not required to be) of equal size.\nWhen using the histogram function, you have flexibility in how the data is grouped. If you only provide the dataset, the function will automatically determine appropriate bins. However, you can also specify custom bins by passing an array of intervals using the syntax hist(data, bins=b), where b is your custom array of bin edges. To normalize the histogram so that the total area under it equals 1, you can set the density parameter to True. It’s worth noting that the histogram function doesn’t just create a visual representation; it also returns useful information such as the count of data points in each bin and the bin edges themselves.\n\n\n\n\n\n\nPhysics Interlude- Probability density for finding an oscillating particle\n\n\n\nLet’s integrate histogram plotting with a fundamental physics concept: the simple harmonic oscillator in one dimension. This system is described by a specific equation of motion:\n\\[\\begin{equation}\n\\ddot{x}(t) = -\\omega^2 x(t)\n\\end{equation}\\]\nFor an initial elongation \\(\\Delta x\\) at \\(t=0\\), the solution is:\n\\[\\begin{equation}\nx(t) = \\Delta x \\cos(\\omega t)\n\\end{equation}\\]\nTo calculate the probability of finding the spring at a certain elongation, we need to consider the time spent at different positions. The time \\(dt\\) spent in the interval [\\(x(t)\\), \\(x(t)+dx\\)] depends on the speed:\n\\[\\begin{equation}\nv(t) = \\frac{dx}{dt} = -\\omega \\Delta x \\sin(\\omega t)\n\\end{equation}\\]\nThe probability of finding the oscillator in a certain interval is the fraction of time spent in this interval, normalized by half the oscillation period \\(T/2\\):\n\\[\\begin{equation}\n\\frac{dt}{T/2} = \\frac{1}{T/2}\\frac{dx}{v(t)} = \\frac{1}{T/2}\\frac{-dx}{\\omega \\Delta x \\sin(\\omega t)}\n\\end{equation}\\]\nGiven that \\(\\omega = 2\\pi/T\\), we can derive the probability density:\n\\[\\begin{equation}\np(x)dx = \\frac{1}{\\pi \\Delta x}\\frac{dx}{\\sqrt{1-\\left(\\frac{x(t)}{\\Delta x}\\right)^2}}\n\\end{equation}\\]\nThis probability density reveals that the spring is more likely to be found at elongations where its speed is low. This principle extends to non-equilibrium physics, where entities moving with variable speed are more likely to be found in locations where they move slowly.\nWe can visualize this using the histogram function. By evaluating the position at equidistant times using the equation of motion and creating a histogram of these positions, we can represent the probability of finding the oscillator at certain positions. When properly normalized, this histogram will reflect the theoretical probability density we derived.\n\n\n\n\n\n\n\n\n\n\nSetting plotting limits and excluding data\nIf you want to zoom in to s specific region of a plot you can set the limits of the individual axes.\n\n\n\n\n\n\n\n\nMasked arrays\nSometimes you encounter situations, when you wish to mask some of the data of your plot, because they are not showing real data as the vertical lines in the plot above. For this purpose, you can mask the data arrays in various ways to not show up. The example below uses the\nnp.ma.masked_where()\nfunction of NumPy, which takes a condition as the first argument and what should be returned if that condition is fulfilled.\n\n\n\n\n\n\nIf you look at the resulting array, you will find, that the entries have not been removed but replaced by --, so the values are not existent and thefore not plotted.\n\n\n\n\n\n\n\nLogarithmic plots\n\n\n\n\n\nData sets can span many orders of magnitude from fractional quantities much smaller than unity to values much larger than unity. In such cases it is often useful to plot the data on logarithmic axes.\n\nSemi-log plots\nFor data sets that vary exponentially in the independent variable, it is often useful to use one or more logarithmic axes. Radioactive decay of unstable nuclei, for example, exhibits an exponential decrease in the number of particles emitted from the nuclei as a function of time.\nMatPlotLib provides two functions for making semi-logarithmic plots, semilogx and semilogy, for creating plots with logarithmic x and y axes, with linear y and x axes, respectively. We illustrate their use in the program below, which made the above plots.\n\n\n\n\n\n\n\n\nLog-log plots\nMatPlotLib can also make log-log or double-logarithmic plots using the function loglog. It is useful when both the \\(x\\) and \\(y\\) data span many orders of magnitude. Data that are described by a power law \\(y=Ax^b\\), where \\(A\\) and \\(b\\) are constants, appear as straight lines when plotted on a log-log plot. Again, the loglog function works just like the plot function but with logarithmic axes.\n\n\n\n\n\n\n\n\n\n\n\n\nCombined plots\nYou can combine multiple data with the same axes by stacking multiple plots.\n\n\n\n\n\n\n\n\nArranging multiple plots\nOften you want to create two or more graphs and place them next to one another, generally because they are related to each other in some way.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnimations\n\n\n\n\n\nMatplotlib can also be used to create animations. The FuncAnimation class makes it easy to create animations by repeatedly calling a function to update the plot. The following example shows a simple pendulum animation.\n\n\n\n\n\n\n\n\n\n\n\nSimple contour plot\n\n\n\n\n\n\nPhysics Interlude\n\n\n\n\n\n\nContour and Density Plots\nA contour plots are useful tools to study two dimensional data, meaning \\(Z(X,Y)\\). A contour plots the lines of constant value of the function \\(Z\\).\n\n\nUnderstanding Wave Interference\nImagine throwing two stones into a pond. Each stone creates circular waves that spread out. When these waves meet, they create interesting patterns - this is called interference. Let’s explore this using physics and Python!\n\nWhat is a Wave?\nA wave can be described mathematically. For our example, we’ll look at spherical waves (like those in the pond). Each wave has: - An amplitude (how tall the wave is) - A wavelength (distance between wave peaks) - A frequency (how fast it oscillates)\n\n\nMathematical Description\nFor a single wave source, we can write: \\[\\begin{equation}\nU(r)=e^{-i\\,k r}\n\\end{equation}\\]\nWhere: - \\(k\\) is related to the wavelength (\\(k = 2\\pi/\\lambda\\)) - \\(r\\) is the distance from the source - We’ve simplified by ignoring how the wave gets smaller as it travels (\\(1/r\\) term)\n\n\nTwo Wave Sources\nWhen we have two wave sources (like two stones dropped in the pond): 1. Each source creates its own wave 2. The waves combine where they meet 3. The total wave is the sum of both waves\n\n\n\ninterference\n\n\nMathematically: \\[\\begin{equation}\nU_{total} = e^{-i\\,k r_1} + e^{-i\\,k r_2}\n\\end{equation}\\]\nWhere \\(r_1\\) and \\(r_2\\) are the distances from each source.\n\n\nWhat We See (Intensity)\nWhat we actually see is the intensity of the combined waves:\n\\[\\begin{equation}\n\\text{Intensity} \\propto |U_{total}|^2\n\\end{equation}\\]\nThis will show us where the waves:\n\nAdd up (bright regions - constructive interference)\nCancel out (dark regions - destructive interference)\n\nLet’s create a Python program to visualize this!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColor contour plot\n\n\n\n\n\n\n\n\nImage plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdvanced Plotting - Explicit Version\n\n\n\n\n\nAdvanced Plotting - Explicit Version\nWhile we have so far largely relied on the default setting and the automatic arrangement of plots, there is also a way to precisely design your plot. Python provides the tools of object oriented programming and thus modules provide classes which can be instanced into objects. This explicit interfaces allows you to control all details without the automatisms of pyplot.\nThe figure below, which is taken from the matplotlib documentation website shows the sets of commands and the objects in the figure, the commands refer to. It is a nice reference, when creating a figure.\n\n\n\nanatomy of a figure\n\n\n\nPlots with Multiple Spines\nSometimes it is very useful to plot different quantities in the same plot with the same x-axis but with different y-axes. Here is some example, where each line plot has its own y-axis.\n\n\n\n\n\n\n\n\nInsets\nInsets are plots within plots using their own axes. We therefore need to create two axes systems, if we want to have a main plot and and inset.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpine axis\n\n\n\n\n\n\n\n\nPolar plot\n\n\n\n\n\n\n\n\nText annotation\nAnnotating text in matplotlib figures can be done using the text function. It supports LaTeX formatting just like axis label texts and titles:\n\n\n\n\n\n\n\n\n3D Plotting\nMatplotlib was initially designed with only two-dimensional plotting in mind. Around the time of the 1.0 release, some three-dimensional plotting utilities were built on top of Matplotlib’s two-dimensional display, and the result is a convenient (if somewhat limited) set of tools for three-dimensional data visualization. Three-dimensional plots are enabled by importing the mplot3d toolkit, included with the main Matplotlib installation:\n\n\n\n\n\n\nOnce this submodule is imported, a three-dimensional axes can be created by passing the keyword projection=‘3d’ to any of the normal axes creation routines:\n\nProjection Scence\n\n\n\n\n\n\nWith this three-dimensional axes enabled, we can now plot a variety of three-dimensional plot types. Three-dimensional plotting is one of the functionalities that benefits immensely from viewing figures interactively rather than statically in the notebook; recall that to use interactive figures, you can use %matplotlib notebook rather than %matplotlib inline when running this code.\n\n\nLine Plotting in 3D\nfrom sets of (x, y, z) triples. In analogy with the more common two-dimensional plots discussed earlier, these can be created using the ax.plot3D and ax.scatter3D functions. The call signature for these is nearly identical to that of their two-dimensional counterparts, so you can refer to Simple Line Plots and Simple Scatter Plots for more information on controlling the output. Here we’ll plot a trigonometric spiral, along with some points drawn randomly near the line:\n\n\n\n\n\n\nNotice that by default, the scatter points have their transparency adjusted to give a sense of depth on the page. While the three-dimensional effect is sometimes difficult to see within a static image, an interactive view can lead to some nice intuition about the layout of the points. Use the scatter3D or the plot3D method to plot a random walk in 3-dimensions in your exercise.\n\n\nSurface Plotting\nA surface plot is like a wireframe plot, but each face of the wireframe is a filled polygon. Adding a colormap to the filled polygons can aid perception of the topology of the surface being visualized:"
  },
  {
    "objectID": "lectures/04-plotting.html#contour-and-density-plots",
    "href": "lectures/04-plotting.html#contour-and-density-plots",
    "title": "Plotting",
    "section": "Contour and Density Plots",
    "text": "Contour and Density Plots\nA contour plots are useful tools to study two dimensional data, meaning \\(Z(X,Y)\\). A contour plots the lines of constant value of the function \\(Z\\)."
  },
  {
    "objectID": "lectures/04-plotting.html#understanding-wave-interference",
    "href": "lectures/04-plotting.html#understanding-wave-interference",
    "title": "Plotting",
    "section": "Understanding Wave Interference",
    "text": "Understanding Wave Interference\nImagine throwing two stones into a pond. Each stone creates circular waves that spread out. When these waves meet, they create interesting patterns - this is called interference. Let’s explore this using physics and Python!\n\nWhat is a Wave?\nA wave can be described mathematically. For our example, we’ll look at spherical waves (like those in the pond). Each wave has: - An amplitude (how tall the wave is) - A wavelength (distance between wave peaks) - A frequency (how fast it oscillates)\n\n\nMathematical Description\nFor a single wave source, we can write: \\[\\begin{equation}\nU(r)=e^{-i\\,k r}\n\\end{equation}\\]\nWhere: - \\(k\\) is related to the wavelength (\\(k = 2\\pi/\\lambda\\)) - \\(r\\) is the distance from the source - We’ve simplified by ignoring how the wave gets smaller as it travels (\\(1/r\\) term)\n\n\nTwo Wave Sources\nWhen we have two wave sources (like two stones dropped in the pond): 1. Each source creates its own wave 2. The waves combine where they meet 3. The total wave is the sum of both waves\n\n\n\ninterference\n\n\nMathematically: \\[\\begin{equation}\nU_{total} = e^{-i\\,k r_1} + e^{-i\\,k r_2}\n\\end{equation}\\]\nWhere \\(r_1\\) and \\(r_2\\) are the distances from each source.\n\n\nWhat We See (Intensity)\nWhat we actually see is the intensity of the combined waves:\n\\[\\begin{equation}\n\\text{Intensity} \\propto |U_{total}|^2\n\\end{equation}\\]\nThis will show us where the waves:\n\nAdd up (bright regions - constructive interference)\nCancel out (dark regions - destructive interference)\n\nLet’s create a Python program to visualize this!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColor contour plot\n\n\n\n\n\n\n\n\nImage plot"
  },
  {
    "objectID": "lectures/04-plotting.html#advanced-plotting---explicit-version",
    "href": "lectures/04-plotting.html#advanced-plotting---explicit-version",
    "title": "Plotting",
    "section": "Advanced Plotting - Explicit Version",
    "text": "Advanced Plotting - Explicit Version\nWhile we have so far largely relied on the default setting and the automatic arrangement of plots, there is also a way to precisely design your plot. Python provides the tools of object oriented programming and thus modules provide classes which can be instanced into objects. This explicit interfaces allows you to control all details without the automatisms of pyplot.\nThe figure below, which is taken from the matplotlib documentation website shows the sets of commands and the objects in the figure, the commands refer to. It is a nice reference, when creating a figure.\n\n\n\nanatomy of a figure\n\n\n\nPlots with Multiple Spines\nSometimes it is very useful to plot different quantities in the same plot with the same x-axis but with different y-axes. Here is some example, where each line plot has its own y-axis.\n\n\n\n\n\n\n\n\nInsets\nInsets are plots within plots using their own axes. We therefore need to create two axes systems, if we want to have a main plot and and inset.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpine axis\n\n\n\n\n\n\n\n\nPolar plot\n\n\n\n\n\n\n\n\nText annotation\nAnnotating text in matplotlib figures can be done using the text function. It supports LaTeX formatting just like axis label texts and titles:\n\n\n\n\n\n\n\n\n3D Plotting\nMatplotlib was initially designed with only two-dimensional plotting in mind. Around the time of the 1.0 release, some three-dimensional plotting utilities were built on top of Matplotlib’s two-dimensional display, and the result is a convenient (if somewhat limited) set of tools for three-dimensional data visualization. Three-dimensional plots are enabled by importing the mplot3d toolkit, included with the main Matplotlib installation:\n\n\n\n\n\n\nOnce this submodule is imported, a three-dimensional axes can be created by passing the keyword projection=‘3d’ to any of the normal axes creation routines:\n\nProjection Scence\n\n\n\n\n\n\nWith this three-dimensional axes enabled, we can now plot a variety of three-dimensional plot types. Three-dimensional plotting is one of the functionalities that benefits immensely from viewing figures interactively rather than statically in the notebook; recall that to use interactive figures, you can use %matplotlib notebook rather than %matplotlib inline when running this code.\n\n\nLine Plotting in 3D\nfrom sets of (x, y, z) triples. In analogy with the more common two-dimensional plots discussed earlier, these can be created using the ax.plot3D and ax.scatter3D functions. The call signature for these is nearly identical to that of their two-dimensional counterparts, so you can refer to Simple Line Plots and Simple Scatter Plots for more information on controlling the output. Here we’ll plot a trigonometric spiral, along with some points drawn randomly near the line:\n\n\n\n\n\n\nNotice that by default, the scatter points have their transparency adjusted to give a sense of depth on the page. While the three-dimensional effect is sometimes difficult to see within a static image, an interactive view can lead to some nice intuition about the layout of the points. Use the scatter3D or the plot3D method to plot a random walk in 3-dimensions in your exercise.\n\n\nSurface Plotting\nA surface plot is like a wireframe plot, but each face of the wireframe is a filled polygon. Adding a colormap to the filled polygons can aid perception of the topology of the surface being visualized:"
  },
  {
    "objectID": "lectures/lecture02/02-lecture02.html",
    "href": "lectures/lecture02/02-lecture02.html",
    "title": "Python Overview",
    "section": "",
    "text": "Building on our understanding of Python’s basic data types and operations, we’ll now explore how to control program flow and create reusable code blocks. These structures allow us to write more sophisticated programs that can make decisions, repeat operations, and organize code efficiently.\n\n\nFunctions are reusable blocks of code that can be executed multiple times from different parts of your program. They help in organizing code, making it more readable, and reducing redundancy. Functions can take input arguments and return output values.\n\nDefining a FunctionCalling a Function\n\n\nA function in Python is defined using the def keyword followed by the name of the function, which is usually descriptive and indicates what the function does. The parameters inside the parentheses indicate what data the function expects to receive. The -&gt; symbol is used to specify the return type of the function.\nHere’s an example:\n\n\n\n\n\n\n\n\nFunctions can be called by specifying the name of the function followed by parentheses containing the arguments. The arguments passed to the function should match the number and type of parameters defined in the function. Here’s an example:\n\n\n\n\n\n\n\n\n\n\n\n\nLoops are used to execute a block of code repeatedly. There are two main types of loops in Python: for loops and while loops.\n\nFor LoopWhile Loop\n\n\nA for loop in Python is used to iterate over a sequence (such as a list or string) and execute a block of code for each item in the sequence. Here’s an example:\n\n\n\n\n\n\n\n\nA while loop in Python is used to execute a block of code while a certain condition is met. The loop continues as long as the condition is true. Here’s an example:\n\n\n\n\n\n\n\n\n\n\n\n\nConditional statements are used to control the flow of your program based on conditions. The main conditional statements in Python are if, else, and elif.\n\nIf StatementElse StatementElif Statement\n\n\nAn if statement in Python is used to execute a block of code if a certain condition is met. Here’s an example:\n\n\n\n\n\n\n\n\nAn else statement in Python is used to execute a block of code if the condition in an if statement is not met. Here’s an example:\n\n\n\n\n\n\n\n\nAn elif statement in Python is used to execute a block of code if the condition in an if statement is not met but under an extra condition. Here’s an example:",
    "crumbs": [
      "Python Basics",
      "Lecture 2",
      "Control Structures & Functions"
    ]
  },
  {
    "objectID": "lectures/lecture02/02-lecture02.html#control-structures-and-functions",
    "href": "lectures/lecture02/02-lecture02.html#control-structures-and-functions",
    "title": "Python Overview",
    "section": "",
    "text": "Building on our understanding of Python’s basic data types and operations, we’ll now explore how to control program flow and create reusable code blocks. These structures allow us to write more sophisticated programs that can make decisions, repeat operations, and organize code efficiently.\n\n\nFunctions are reusable blocks of code that can be executed multiple times from different parts of your program. They help in organizing code, making it more readable, and reducing redundancy. Functions can take input arguments and return output values.\n\nDefining a FunctionCalling a Function\n\n\nA function in Python is defined using the def keyword followed by the name of the function, which is usually descriptive and indicates what the function does. The parameters inside the parentheses indicate what data the function expects to receive. The -&gt; symbol is used to specify the return type of the function.\nHere’s an example:\n\n\n\n\n\n\n\n\nFunctions can be called by specifying the name of the function followed by parentheses containing the arguments. The arguments passed to the function should match the number and type of parameters defined in the function. Here’s an example:\n\n\n\n\n\n\n\n\n\n\n\n\nLoops are used to execute a block of code repeatedly. There are two main types of loops in Python: for loops and while loops.\n\nFor LoopWhile Loop\n\n\nA for loop in Python is used to iterate over a sequence (such as a list or string) and execute a block of code for each item in the sequence. Here’s an example:\n\n\n\n\n\n\n\n\nA while loop in Python is used to execute a block of code while a certain condition is met. The loop continues as long as the condition is true. Here’s an example:\n\n\n\n\n\n\n\n\n\n\n\n\nConditional statements are used to control the flow of your program based on conditions. The main conditional statements in Python are if, else, and elif.\n\nIf StatementElse StatementElif Statement\n\n\nAn if statement in Python is used to execute a block of code if a certain condition is met. Here’s an example:\n\n\n\n\n\n\n\n\nAn else statement in Python is used to execute a block of code if the condition in an if statement is not met. Here’s an example:\n\n\n\n\n\n\n\n\nAn elif statement in Python is used to execute a block of code if the condition in an if statement is not met but under an extra condition. Here’s an example:",
    "crumbs": [
      "Python Basics",
      "Lecture 2",
      "Control Structures & Functions"
    ]
  },
  {
    "objectID": "lectures/lecture02/02-lecture02.html#exercises",
    "href": "lectures/lecture02/02-lecture02.html#exercises",
    "title": "Python Overview",
    "section": "Exercises",
    "text": "Exercises\nThe following exercises will help you practice using functions with conditional logic.\n\n\n\n\n\n\nExercise 1: Temperature Conversion Function\n\n\n\nCreate a function that converts temperatures between Fahrenheit and Celsius scales. This exercise demonstrates how to define and use functions with conditional logic to perform different types of conversions based on user input.\nThe conversion formulas are: - Celsius to Fahrenheit: \\(F = (C \\times 9/5) + 32\\) - Fahrenheit to Celsius: \\(C = (F - 32) \\times 5/9\\)\nTime estimate: 15-20 minutes\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nUse an if-else statement to check the scale parameter. Depending on whether it’s ‘C’ or ‘F’, apply the appropriate conversion formula. Remember to return both the converted temperature value and the new scale designation (either ‘F’ or ‘C’).\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2: Prime Number Checker\n\n\n\nCreate a function that checks whether a given number is prime. This exercise demonstrates the use of loops, conditional statements, and early return to solve a common mathematical problem.\nA prime number is a natural number greater than 1 that cannot be formed by multiplying two smaller natural numbers.\nTime estimate: 15-20 minutes\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nFirst, check if the number is less than 2 (not prime). Then, use a loop to check if the number is divisible by any integer from 2 to the square root of the number. If you find a divisor, the number is not prime. If no divisors are found, the number is prime.\n\n\n\n\n\n\n\n\n\n\n\nNote",
    "crumbs": [
      "Python Basics",
      "Lecture 2",
      "Control Structures & Functions"
    ]
  },
  {
    "objectID": "lectures/lecture05/simpson_short.html",
    "href": "lectures/lecture05/simpson_short.html",
    "title": "Simpson’s Method",
    "section": "",
    "text": "Theory and Implementation\n\n\n\nSimpson’s Method Illustration\n\n\nSimpson’s method provides higher accuracy by approximating the function with parabolic segments rather than straight lines. This approach is particularly effective for functions with curvature, which are ubiquitous in physics problems.\nThe mathematical formulation of Simpson’s rule is:\n\\[\\begin{equation}\n\\int_{a}^{b} f(x) dx \\approx \\frac{\\Delta x}{3} \\sum_{i=0}^{(N-1)/2} \\left(f(x_{2i}) + 4f(x_{2i+1}) + f(x_{2i+2})\\right)\n\\end{equation}\\]\nWhere: - \\(N\\) is the number of intervals (must be even) - \\(\\Delta x = \\frac{b-a}{N}\\) is the width of each interval\nSimpson’s rule is derived from fitting a quadratic polynomial through every three consecutive points and then integrating these polynomials.\n\n\n\n\n\n\n\n\nPhysics Application: Quantum Mechanics\nA critical application of Simpson’s method in physics is calculating probabilities in quantum mechanics. For a wavefunction \\(\\psi(x)\\), the probability of finding a particle in a region \\([a,b]\\) is:\n\\[\\begin{equation}\nP(a \\leq x \\leq b) = \\int_{a}^{b} |\\psi(x)|^2 dx\n\\end{equation}\\]\nSimpson’s method provides the accuracy needed for these calculations, particularly for oscillatory wavefunctions where simpler methods would require many more points to achieve the same precision."
  },
  {
    "objectID": "lectures/lecture05/0_input_output.html",
    "href": "lectures/lecture05/0_input_output.html",
    "title": "Computer-Based Physical Modelling",
    "section": "",
    "text": "In physics laboratory experiments, you’ll frequently encounter the need to handle data stored in text files. Whether you’re collecting measurements from a pendulum experiment, analyzing spectrometer readings, or processing particle collision data, efficiently importing, manipulating, and exporting this data is essential for your analysis. As second-semester physics students, mastering these file handling techniques will save you significant time when processing experimental results and allow you to focus on the physical interpretation rather than data management. This section covers the fundamental approaches to working with data files in Python, from basic file operations to specialized tools in NumPy that are particularly useful for the large datasets common in physics applications.\n\n\nTo input or output data to a file you can use Python’s built-in file handling, e.g. to write data:\n\n\n\n\n\n\nThis approach gives you more control over formatting and is useful when dealing with complex data structures or when you need custom formatting. Python’s built-in file handling allows you to precisely control how each line is formatted, which is particularly valuable when working with heterogeneous data or when you need to create files that conform to specific format requirements.\n\n\n\n\n\n\nNote\n\n\n\n\n\nThe Python with statement is a context manager that provides a clean and efficient way to handle resources that need setup and teardown operations, such as file handling, database connections, or network connections.\nThe basic syntax looks like this:\nwith expression as variable:\n    # code block\nThe with statement ensures that resources are properly managed by automatically handling the setup before entering the code block and the cleanup after exiting it, even if exceptions occur within the block.\nHere’s a common example with file operations:\nwith open('file.txt', 'r') as file:\n    data = file.read()\n    # Process data\n# File is automatically closed when exiting the with block\nThe key benefits of using the with statement include:\n\nAutomatic resource management - no need to explicitly call methods like close()\nException safety - resources are properly cleaned up even if exceptions occur\nCleaner, more readable code compared to try-finally blocks\n\nIn physics and electrical engineering contexts, you might use the with statement when working with measurement equipment, data acquisition, or when processing large datasets that require temporary file handling.\n\n\n\n\n\n\nNumPy provides several functions for reading and writing text data, which can be particularly useful for handling numeric data stored in text files.\n\n\n\n\nThe most common method for loading text data is np.loadtxt. This function reads data from a text file and creates a NumPy array with the values:\n\n\n\n\n\n\nYou can customize how loadtxt interprets the file using various parameters. For instance, you can specify a delimiter to handle CSV files, skip header rows that contain metadata, and select only specific columns to read:\n# Load with specific delimiter, skipping rows, and selecting columns\ndata = np.loadtxt('data.txt',\n                  delimiter=',',   # CSV file\n                  skiprows=1,      # Skip header row\n                  usecols=(0, 1, 2))  # Use only first three columns\n\n\n\nFor more flexible loading, especially with missing values, NumPy provides the genfromtxt function. This function is particularly useful when dealing with real-world data that may have inconsistencies or missing entries:\n# Handle missing values with genfromtxt\ndata = np.genfromtxt('data_with_missing.txt',\n                     delimiter=',',\n                     filling_values=-999,  # Replace missing values\n                     skip_header=1)        # Skip header row\nThe genfromtxt function allows you to specify how missing values should be handled, making it more robust for imperfect datasets where some entries might be missing or corrupted.\n\n\n\n\n\n\nYou can save NumPy arrays to text files using the savetxt function. This function allows you to convert your array data into a human-readable text format that can be easily shared or used by other programs:\n\n\n\n\n\n\nThe savetxt function offers numerous formatting options to control exactly how your data is written. You can add headers and footers to provide context, specify the numeric format of your data, and control other aspects of the output file:\n\n\n\n\n\n\nThese formatting options give you considerable control over how your numerical data is presented in the output file, which can be important for compatibility with other software or for human readability.\n\n\n\n\nHere’s a complete example of reading, processing, and writing text data that demonstrates a typical data analysis workflow using NumPy’s I/O capabilities:\n\n\n\n\n\n\nThis workflow demonstrates how NumPy can efficiently handle text-based data input and output for numerical analysis. The example reads data from a CSV file, performs statistical calculations on each row, combines the original data with the calculated statistics, and then saves the processed results to a new CSV file with appropriate headers. This type of pipeline is common in data analysis and scientific computing, where raw data is imported, transformed, and then exported in a more useful format.",
    "crumbs": [
      "Python Basics",
      "Lecture 5",
      "Input & Output"
    ]
  },
  {
    "objectID": "lectures/lecture05/0_input_output.html#dealing-with-text-files-containing-data",
    "href": "lectures/lecture05/0_input_output.html#dealing-with-text-files-containing-data",
    "title": "Computer-Based Physical Modelling",
    "section": "",
    "text": "In physics laboratory experiments, you’ll frequently encounter the need to handle data stored in text files. Whether you’re collecting measurements from a pendulum experiment, analyzing spectrometer readings, or processing particle collision data, efficiently importing, manipulating, and exporting this data is essential for your analysis. As second-semester physics students, mastering these file handling techniques will save you significant time when processing experimental results and allow you to focus on the physical interpretation rather than data management. This section covers the fundamental approaches to working with data files in Python, from basic file operations to specialized tools in NumPy that are particularly useful for the large datasets common in physics applications.\n\n\nTo input or output data to a file you can use Python’s built-in file handling, e.g. to write data:\n\n\n\n\n\n\nThis approach gives you more control over formatting and is useful when dealing with complex data structures or when you need custom formatting. Python’s built-in file handling allows you to precisely control how each line is formatted, which is particularly valuable when working with heterogeneous data or when you need to create files that conform to specific format requirements.\n\n\n\n\n\n\nNote\n\n\n\n\n\nThe Python with statement is a context manager that provides a clean and efficient way to handle resources that need setup and teardown operations, such as file handling, database connections, or network connections.\nThe basic syntax looks like this:\nwith expression as variable:\n    # code block\nThe with statement ensures that resources are properly managed by automatically handling the setup before entering the code block and the cleanup after exiting it, even if exceptions occur within the block.\nHere’s a common example with file operations:\nwith open('file.txt', 'r') as file:\n    data = file.read()\n    # Process data\n# File is automatically closed when exiting the with block\nThe key benefits of using the with statement include:\n\nAutomatic resource management - no need to explicitly call methods like close()\nException safety - resources are properly cleaned up even if exceptions occur\nCleaner, more readable code compared to try-finally blocks\n\nIn physics and electrical engineering contexts, you might use the with statement when working with measurement equipment, data acquisition, or when processing large datasets that require temporary file handling.\n\n\n\n\n\n\nNumPy provides several functions for reading and writing text data, which can be particularly useful for handling numeric data stored in text files.\n\n\n\n\nThe most common method for loading text data is np.loadtxt. This function reads data from a text file and creates a NumPy array with the values:\n\n\n\n\n\n\nYou can customize how loadtxt interprets the file using various parameters. For instance, you can specify a delimiter to handle CSV files, skip header rows that contain metadata, and select only specific columns to read:\n# Load with specific delimiter, skipping rows, and selecting columns\ndata = np.loadtxt('data.txt',\n                  delimiter=',',   # CSV file\n                  skiprows=1,      # Skip header row\n                  usecols=(0, 1, 2))  # Use only first three columns\n\n\n\nFor more flexible loading, especially with missing values, NumPy provides the genfromtxt function. This function is particularly useful when dealing with real-world data that may have inconsistencies or missing entries:\n# Handle missing values with genfromtxt\ndata = np.genfromtxt('data_with_missing.txt',\n                     delimiter=',',\n                     filling_values=-999,  # Replace missing values\n                     skip_header=1)        # Skip header row\nThe genfromtxt function allows you to specify how missing values should be handled, making it more robust for imperfect datasets where some entries might be missing or corrupted.\n\n\n\n\n\n\nYou can save NumPy arrays to text files using the savetxt function. This function allows you to convert your array data into a human-readable text format that can be easily shared or used by other programs:\n\n\n\n\n\n\nThe savetxt function offers numerous formatting options to control exactly how your data is written. You can add headers and footers to provide context, specify the numeric format of your data, and control other aspects of the output file:\n\n\n\n\n\n\nThese formatting options give you considerable control over how your numerical data is presented in the output file, which can be important for compatibility with other software or for human readability.\n\n\n\n\nHere’s a complete example of reading, processing, and writing text data that demonstrates a typical data analysis workflow using NumPy’s I/O capabilities:\n\n\n\n\n\n\nThis workflow demonstrates how NumPy can efficiently handle text-based data input and output for numerical analysis. The example reads data from a CSV file, performs statistical calculations on each row, combines the original data with the calculated statistics, and then saves the processed results to a new CSV file with appropriate headers. This type of pipeline is common in data analysis and scientific computing, where raw data is imported, transformed, and then exported in a more useful format.",
    "crumbs": [
      "Python Basics",
      "Lecture 5",
      "Input & Output"
    ]
  },
  {
    "objectID": "lectures/lecture05/01-lecture05.html",
    "href": "lectures/lecture05/01-lecture05.html",
    "title": "Brownian Motion",
    "section": "",
    "text": "A class is perfect for this physics simulation because each colloidal particle:\n\nHas specific properties\n\nSize (radius)\nCurrent position\nMovement history\nDiffusion coefficient\n\nFollows certain behaviors\n\nMoves randomly (Brownian motion)\nUpdates its position over time\nKeeps track of where it’s been\n\nCan exist alongside other particles\n\nMany particles can move independently\nEach particle keeps track of its own properties\nParticles can have different sizes\n\nNeeds to track its state over time\n\nRemember previous positions\nCalculate distances moved\nMaintain its own trajectory\n\n\nThis natural mapping between real particles and code objects makes classes an ideal choice for our simulation.\n\n\n\nWe design a Colloid class to simulate particles undergoing Brownian motion. Using object-oriented programming makes physical sense here - in the real world, each colloidal particle is an independent object with its own properties that follows the same physical laws as other particles.\n\n\nOur Colloid class will store information common to all particles:\n\nnumber: A counter tracking how many particles we’ve created\nf = 2.2×10^{-19}: The physical constant \\(k_B T/(6\\pi\\eta)\\) in m³/s\n\nThis combines Boltzmann’s constant (\\(k_B\\)), temperature (\\(T\\)), and fluid viscosity (\\(\\eta\\))\nUsing this constant simplifies our diffusion calculations\n\n\n\n\n\nThe class provides these shared behaviors:\n\nhow_many(): Returns the total count of particles created\n\nUseful for tracking how many particles exist in our simulation\n\n__str__(): Returns a human-readable description when we print a particle\n\nShows the particle’s radius and current position\n\n\n\n\n\nEach individual particle will have its own:\n\nR: Radius in meters\nx, y: Lists storing position history (starting with initial position)\nindex: Unique ID number for each particle\nD: Diffusion coefficient calculated as \\(D = f/R\\)\n\nFrom Einstein-Stokes relation: \\(D = \\frac{k_B T}{6\\pi\\eta R}\\)\nSmaller particles diffuse faster (larger D)\n\n\n\n\n\nEach particle object will have these behaviors:\n\nupdate(dt): Performs a single timestep of Brownian motion\n\nTakes a timestep dt in seconds\nAdds random displacement based on diffusion coefficient\nReturns the new position\n\nsim_trajectory(N, dt): Simulates a complete trajectory\n\nGenerates N steps with timestep dt\nCalls update() repeatedly to build the trajectory\n\nget_trajectory(): Returns the particle’s movement history as a DataFrame\n\nConvenient for analysis and plotting\n\nget_D(): Returns the particle’s diffusion coefficient\n\nUseful for calculations and verification\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that the function sim_trajectory is actually calling the function update of the same object to generate the whole trajectory at once."
  },
  {
    "objectID": "lectures/lecture05/01-lecture05.html#brownian-motion---object-oriented-implementation",
    "href": "lectures/lecture05/01-lecture05.html#brownian-motion---object-oriented-implementation",
    "title": "Brownian Motion",
    "section": "",
    "text": "A class is perfect for this physics simulation because each colloidal particle:\n\nHas specific properties\n\nSize (radius)\nCurrent position\nMovement history\nDiffusion coefficient\n\nFollows certain behaviors\n\nMoves randomly (Brownian motion)\nUpdates its position over time\nKeeps track of where it’s been\n\nCan exist alongside other particles\n\nMany particles can move independently\nEach particle keeps track of its own properties\nParticles can have different sizes\n\nNeeds to track its state over time\n\nRemember previous positions\nCalculate distances moved\nMaintain its own trajectory\n\n\nThis natural mapping between real particles and code objects makes classes an ideal choice for our simulation.\n\n\n\nWe design a Colloid class to simulate particles undergoing Brownian motion. Using object-oriented programming makes physical sense here - in the real world, each colloidal particle is an independent object with its own properties that follows the same physical laws as other particles.\n\n\nOur Colloid class will store information common to all particles:\n\nnumber: A counter tracking how many particles we’ve created\nf = 2.2×10^{-19}: The physical constant \\(k_B T/(6\\pi\\eta)\\) in m³/s\n\nThis combines Boltzmann’s constant (\\(k_B\\)), temperature (\\(T\\)), and fluid viscosity (\\(\\eta\\))\nUsing this constant simplifies our diffusion calculations\n\n\n\n\n\nThe class provides these shared behaviors:\n\nhow_many(): Returns the total count of particles created\n\nUseful for tracking how many particles exist in our simulation\n\n__str__(): Returns a human-readable description when we print a particle\n\nShows the particle’s radius and current position\n\n\n\n\n\nEach individual particle will have its own:\n\nR: Radius in meters\nx, y: Lists storing position history (starting with initial position)\nindex: Unique ID number for each particle\nD: Diffusion coefficient calculated as \\(D = f/R\\)\n\nFrom Einstein-Stokes relation: \\(D = \\frac{k_B T}{6\\pi\\eta R}\\)\nSmaller particles diffuse faster (larger D)\n\n\n\n\n\nEach particle object will have these behaviors:\n\nupdate(dt): Performs a single timestep of Brownian motion\n\nTakes a timestep dt in seconds\nAdds random displacement based on diffusion coefficient\nReturns the new position\n\nsim_trajectory(N, dt): Simulates a complete trajectory\n\nGenerates N steps with timestep dt\nCalls update() repeatedly to build the trajectory\n\nget_trajectory(): Returns the particle’s movement history as a DataFrame\n\nConvenient for analysis and plotting\n\nget_D(): Returns the particle’s diffusion coefficient\n\nUseful for calculations and verification\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that the function sim_trajectory is actually calling the function update of the same object to generate the whole trajectory at once."
  },
  {
    "objectID": "lectures/lecture05/01-lecture05.html#simulation-and-analysis",
    "href": "lectures/lecture05/01-lecture05.html#simulation-and-analysis",
    "title": "Brownian Motion",
    "section": "Simulation and Analysis",
    "text": "Simulation and Analysis\n\nSimulating\nWith the help of this Colloid class, we would like to carry out simulations of Brownian motion of multiple particles. The simulations shall\n\ntake n=200 particles\nhave N=200 trajectory points each\nstart all at 0,0\nparticle objects should be stored in a list p_list\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlotting the trajectories\nThe next step is to plot all the trajectories.\n\n\n\n\n\n\n\n\nCharacterizing the Brownian motion\nNow that we have a number of trajectories, we can analyze the motion of our Brownian particles.\n\nCalculate the particle speed\nOne way is to calculate its speed by measuring how far it traveled within a certain time \\(n\\, dt\\), where \\(dt\\) is the timestep of out simulation. We can do that as\n\\[\\begin{equation}\nv(n dt) = \\frac{&lt;\\sqrt{(x_{i+n}-x_{i})^2+(y_{i+n}-y_{i})^2}&gt;}{n\\,dt}\n\\end{equation}\\]\nThe angular brackets on the top take care of the fact that we can measure the distance traveled within a certain time \\(n\\, dt\\) several times along a trajectory.\nThese values can be used to calculate a mean speed. Note that there is not an equal amount of data pairs for all separations available. For \\(n=1\\) there are 5 distances available. For \\(n=5\\), however, only 1. This changes the statistical accuracy of the mean.\n\n\n\n\n\n\nThe result of this analysis shows, that each particle has an apparent speed which seems to increase with decreasing time of observation or which decreases with increasing time. This would mean that there is some friction at work, which slows down the particle in time, but this is apparently not true. Also an infinite speed at zero time appears to be unphysical. The correct answer is just that the speed is no good measure to characterize the motion of a Brownian particle.\n\n\nCalculate the particle mean squared displacement\nA better way to characterize the motion of a Brownian particle is the mean squared displacement, as we have already mentioned it in previous lectures. We may compare our simulation now to the theoretical prediction, which is\n\\[\\begin{equation}\n\\langle \\Delta r^{2}(t)\\rangle=2 d D t\n\\end{equation}\\]\nwhere \\(d\\) is the dimension of the random walk, which is \\(d=2\\) in our case.\n\n\n\n\n\n\nThe results show that the mean squared displacement of the individual particles follows on average the theoretical predictions of a linear growth in time. That means, we are able to read the diffusion coefficient from the slope of the MSD of the individual particles if recorded in a simulation or an experiment.\nYet, each individual MSD is deviating strongly from the theoretical prediction especially at large times. This is due to the fact mentioned earlier that our simulation (or experimental) data only has a limited number of data points, while the theoretical prediction is made for the limit of infinite data points.\n\n\n\n\n\n\nAnalysis of MSD data\n\n\n\nSingle particle tracking, either in the experiment or in numerical simulations can therefore only deliver an estimate of the diffusion coefficient and care should be taken when using the whole MSD to obtain the diffusion coefficient. One typically uses only a short fraction of the whole MSD data at short times."
  },
  {
    "objectID": "lectures/lecture05/01-lecture05.html#summary",
    "href": "lectures/lecture05/01-lecture05.html#summary",
    "title": "Brownian Motion",
    "section": "Summary",
    "text": "Summary\nIn this lecture, we have:\n\nExplored the physical principles behind Brownian motion and its mathematical description\nImplemented a computational model using object-oriented programming principles\nCreated a Colloid class with properties and methods that simulate realistic particle behavior\nGenerated and visualized multiple particle trajectories\nAnalyzed the simulation results using mean squared displacement calculations\nCompared our numerical results with theoretical predictions\n\nThis exercise demonstrates how object-oriented programming provides an elegant framework for physics simulations, where the objects in our code naturally represent physical entities in the real world."
  },
  {
    "objectID": "lectures/lecture05/01-lecture05.html#further-reading",
    "href": "lectures/lecture05/01-lecture05.html#further-reading",
    "title": "Brownian Motion",
    "section": "Further Reading",
    "text": "Further Reading\n\nEinstein, A. (1905). “On the Movement of Small Particles Suspended in Stationary Liquids Required by the Molecular-Kinetic Theory of Heat”\nBerg, H.C. (1993). “Random Walks in Biology”\nChandrasekhar, S. (1943). “Stochastic Problems in Physics and Astronomy”\nNelson, E. (2001). “Dynamical Theories of Brownian Motion”"
  },
  {
    "objectID": "lectures/lecture07/3_fourier_analysis.html",
    "href": "lectures/lecture07/3_fourier_analysis.html",
    "title": "Computer-Based Physical Modelling",
    "section": "",
    "text": "Fourier analysis, or the description of functions as a series of sine and cosine functions, serves as a powerful tool in both numerical data analysis and the solution of differential equations. In experimental physics, Fourier transforms find widespread applications. For instance, optical tweezers utilize frequency spectra to characterize positional fluctuations, while Lock-In detection employs Fourier analysis for specific frequency signals. Additionally, many optical phenomena can be understood through the lens of Fourier transforms.\nFourier analysis extends far beyond these examples, finding applications across numerous fields of physics and engineering. In this lecture, we will examine Fourier Series and Fourier transforms from a mathematical perspective. We will apply these concepts to analyze the frequency spectrum of oscillations in coupled pendula, and later revisit them when simulating the motion of a Gaussian wavepacket in quantum mechanics.\n\n\n\n\n\n\n\n\nA Fourier series represents a periodic function \\(f(t)\\) with period \\(2\\pi\\) or, more generally, any arbitrary interval \\(T\\) as a sum of sine and cosine functions:\n\\[\nf(t)=\\frac{A_{0}}{2}+\\sum_{k=1}^{\\infty}\\left ( A_{k}\\cos\\left (\\omega_k t\\right) + B_{k}\\sin\\left (\\omega_k t\\right)\\right )\n\\tag{1}\\]\nwhere \\(\\omega_k=\\frac{2\\pi k}{T}\\). Here, \\(T\\) represents the period of the cosine and sine functions, with their amplitudes defined by the coefficients \\(A_k\\) and \\(B_k\\). The term \\(A_0\\) represents a constant offset added to the oscillating functions. Equation 1 expresses an arbitrary periodic function \\(f(t)\\) on an interval T as a sum of oscillating sine and cosine functions with discrete frequencies (\\(\\omega_k\\)):\n\\[\\begin{equation*}\n\\omega_k= 0, \\frac{2\\pi}{T}, \\frac{4\\pi}{T}, \\frac{6\\pi}{T}, ... , \\frac{n\\pi}{T}\n\\end{equation*}\\]\nand varying amplitudes. We can demonstrate that the cosine and sine functions in the sum (Equation 1) are orthogonal using the trigonometric identity:\n\\[\\begin{equation}\n\\sin(\\omega_{i} t)\\sin(\\omega_{k}t )=\\frac{1}{2}\\lbrace\\cos((\\omega_{i}-\\omega_{k})t)- \\cos((\\omega_{i}+\\omega_{k})t\\rbrace\n\\end{equation}\\]\nThis leads to the integral:\n\\[\\begin{equation}\n\\int\\limits_{-\\frac{T}{2}}^{+\\frac{T}{2}}  \\sin(\\omega_{i}t)\\sin (\\omega_k t) dt\n\\end{equation}\\]\nwhich splits into two integrals over cosine functions with sum \\((\\omega_{1}+\\omega_{2})\\) and difference frequency \\((\\omega_{1}-\\omega_{2})\\). With \\(\\omega_k=k 2\\pi/T\\), \\((k \\in \\mathbb{Z}^+ )\\), this evaluates to:\n\\[\\begin{equation}\n\\int\\limits_{-\\frac{T}{2}}^{+\\frac{T}{2}}  \\sin(\\omega_{i}t)\\sin (\\omega_k t) dt  =\\begin{cases}\n0 &\\text{for }  i\\neq k, \\\\\nT/2 &\\text{for }  i=k\n\\end{cases}\n\\end{equation}\\]\nA similar result holds for cosine functions:\n\\[\\begin{equation}\n\\int\\limits_{-\\frac{T}{2}}^{+\\frac{T}{2}}  \\cos(\\omega_{i}t)\\cos (\\omega_k t) dt  =\\begin{cases}\n0 &\\text{for }  i\\neq k, \\\\\nT/2 &\\text{for }  i=k\n\\end{cases}\n\\end{equation}\\]\nThe coefficients \\(A_k\\) and \\(B_k\\) are determined by projecting the function \\(f(t)\\) onto these basis functions:\n\\[\\begin{align}\n\\int\\limits_{-\\frac{T}{2}}^{+\\frac{T}{2}} & \\cos (\\omega_k t) dt  =\\begin{cases}\n0 &\\text{for }  k\\neq0, \\\\\nT &\\text{for }  k=0\n\\end{cases} \\\\\n\\int\\limits_{-\\frac{T}{2}}^{+\\frac{T}{2}} & \\sin(\\omega_k t) dt=0  \\text{ for all }k\n\\end{align}\\]\nFor the cosine coefficients:\n\\[\\begin{equation}\\label{A_k}\nA_k=\\frac{2}{T}\\int\\limits_{-\\frac{T}{2}}^{+\\frac{T}{2}} f(t)\\cos(\\omega_k t) dt  \\text{ for } k \\neq 0\n\\end{equation}\\]\nand the constant term:\n\\[\\begin{equation}\nA_0= \\frac{1}{T}\\int\\limits_{-\\frac{T}{2}}^{+\\frac{T}{2}} f(t) dt\n\\end{equation}\\]\nFinally, for the sine coefficients:\n\\[\\begin{equation}\\label{B_k}\nB_k=\\frac{2}{T}\\int\\limits_{-\\frac{T}{2}}^{+\\frac{T}{2}} f(t) \\sin(\\omega_k t) dt,\\,  \\forall k\n\\end{equation}\\]\n\n\n\n\n\n\nPhysical interpretation of Fourier coefficients\n\n\n\nThe Fourier coefficients provide crucial physical insights about the original function:\n\n\\(A_0\\) represents the mean value or DC offset of the function over one period.\n\\(A_k\\) and \\(B_k\\) represent the strength (amplitude) of frequency components at \\(\\omega_k = \\frac{2\\pi k}{T}\\).\nThe larger the coefficient, the more that particular frequency contributes to the overall function.\nThe power of a frequency component is proportional to \\(A_k^2 + B_k^2\\).\nThe phase angle \\(\\phi_k = \\tan^{-1}(-B_k/A_k)\\) tells us the relative timing of each frequency component.\n\nThis decomposition allows us to understand complex signals as combinations of simpler oscillations. For instance, in a vibrating string, each coefficient corresponds to the amplitude of a particular harmonic. In an electrical circuit, these coefficients represent the amplitude of voltage or current at specific frequencies.\n\n\n\n\n\nLet’s consider a concrete example: a square wave function with period \\(T=2\\pi\\):\n\\[f(t) = \\begin{cases}\n1, & 0 &lt; t &lt; \\pi \\\\\n-1, & -\\pi &lt; t &lt; 0\n\\end{cases}\\]\n\n\n\n\n\n\n\n\nFigure 1: Square Wave Function\n\n\n\n\n\nThis represents, for example, a signal that alternates between two voltage levels.\nTo find the Fourier coefficients, we have to determine the coefficients according to the integrals shown above. According to that, the DC component is:\n\\[A_0 = \\frac{1}{2\\pi}\\int_{-\\pi}^{\\pi} f(t) dt = \\frac{1}{2\\pi}\\left(\\int_{-\\pi}^{0} (-1) dt + \\int_{0}^{\\pi} 1 dt\\right) = 0\\]\nAlso, the coefficients for the cosine terms are:\n\\[A_k = \\frac{1}{\\pi}\\int_{-\\pi}^{\\pi} f(t)\\cos(kt) dt = \\frac{1}{\\pi}\\left(\\int_{-\\pi}^{0} (-1)\\cos(kt) dt + \\int_{0}^{\\pi} \\cos(kt) dt\\right) = 0 \\, \\text{for all } k\\]\nzero, as the cosine function is even and the square wave function is odd. Therefore, only the sine terms will contribute to the Fourier series.\n\\[B_k = \\frac{1}{\\pi}\\int_{-\\pi}^{\\pi} f(t)\\sin(kt) dt = \\frac{1}{\\pi}\\left(\\int_{-\\pi}^{0} (-1)\\sin(kt) dt + \\int_{0}^{\\pi} \\sin(kt) dt\\right)\\]\nWorking through the integral, we find:\n\\[B_k = \\frac{2}{\\pi}\\int_{0}^{\\pi} \\sin(kt) dt = \\begin{cases}\n\\frac{4}{k\\pi}, & k \\text{ odd} \\\\\n0, & k \\text{ even}\n\\end{cases}\\]\nTherefore, the Fourier series for our square wave is: \\[f(t) = \\frac{4}{\\pi}\\left(\\sin(t) + \\frac{1}{3}\\sin(3t) + \\frac{1}{5}\\sin(5t) + \\ldots\\right) = \\frac{4}{\\pi}\\sum_{n=0}^{\\infty} \\frac{1}{2n+1}\\sin((2n+1)t)\\]\n\n\n\n\n\n\nNotice how adding more terms in the series makes the approximation closer to the true square wave. Also note that high-frequency components (larger k values) have smaller amplitudes, explaining why the approximation smoothes out the sharp transitions even with many terms.\n\n\n\n\n\n\n\n\n\nThe Fourier transform extends the concept of Fourier series by representing arbitrary non-periodic functions \\(f(t)\\) through a continuous spectrum of complex functions \\(\\exp(i\\omega t)\\). Known as the continuous Fourier transform, this approach replaces the discrete frequency sum of sine and cosine functions found in Fourier series with an integral over complex exponential functions \\(\\exp(i\\omega t)\\) spanning continuous frequency values \\(\\omega\\).\n\n\nBefore diving into the Fourier transform, let’s clarify the transition from the real representation (sines and cosines) to the complex exponential notation. Using Euler’s formula:\n\\[e^{i\\theta} = \\cos(\\theta) + i\\sin(\\theta)\\]\nWe can rewrite our Fourier series from:\n\\[f(t)=\\frac{A_{0}}{2}+\\sum_{k=1}^{\\infty}\\left ( A_{k}\\cos\\left (\\omega_k t\\right) + B_{k}\\sin\\left (\\omega_k t\\right)\\right )\\]\nto:\n\\[f(t)=\\sum_{k=-\\infty}^{\\infty} c_k e^{i\\omega_k t}\\]\nwhere \\(c_k\\) is a complex coefficient related to \\(A_k\\) and \\(B_k\\) by:\n\\[c_0 = \\frac{A_0}{2}, \\quad c_k = \\frac{A_k - iB_k}{2}, \\quad c_{-k} = \\frac{A_k + iB_k}{2}\\]\nThis complex representation often simplifies calculations and provides a more elegant mathematical formulation. The advantage becomes even more apparent when we move from discrete frequencies (Fourier series) to continuous frequencies (Fourier transform).\nThe Fourier transform of a function \\(f(t)\\) is defined as:\n\\[\nF(\\omega)=\\int\\limits_{-\\infty}^{+\\infty}f(t)e^{-i\\omega t}dt\n\\tag{2}\\]\nwhere \\(F(\\omega)\\) represents the spectrum of frequencies present in \\(f(t)\\). The inverse Fourier transform recovers the original function \\(f(t)\\) from its frequency spectrum (Equation 2):\n\\[\\begin{equation}\\label{eq:inverse_FT}\nf(t)=\\frac{1}{2\\pi}\\int\\limits_{-\\infty}^{+\\infty}F(\\omega)e^{+i\\omega t}dt\n\\end{equation}\\]\nThe Fourier transform \\(F(\\omega)\\) yields a complex number, encoding both phase and amplitude information of the oscillations. The phase of oscillation at frequency \\(\\omega\\) is given by:\n\\[\\begin{equation}\n\\phi=\\tan^{-1}\\left(\\frac{Im(F(\\omega))}{Re(F(\\omega))}\\right)\n\\end{equation}\\]\nwhile the amplitude at frequency \\(\\omega\\) is:\n\\[\\begin{equation}\nx_{0}^{\\rm theo}=|F(\\omega)|\n\\end{equation}\\]\nModern computing offers efficient algorithms for Fourier transformation, particularly the Fast Fourier Transform (FFT) implemented in libraries like NumPy. We’ll use these algorithms to identify oscillation patterns in our signals. Here’s an example of using NumPy’s FFT functions to compute the transform and generate the corresponding frequency axis:\nf=np.fft.fft(alpha)\n\n\n\n\nLet us now apply Fourier analysis to examine the data from our coupled pendula, which includes both normal modes and beat mode oscillations of the harmonic oscillator. Instead of using the numerical data from the simulation, we will use the analytical expressions for the normal modes and beat mode to demonstrate the Fourier analysis.\n\n\nFor two pendula with equal masses, coupled by a spring of constant \\(k\\), the equations of motion are:\n\\[\\ddot{\\theta}_1 + \\omega_0^2\\theta_1 + \\kappa(\\theta_1-\\theta_2) = 0\\] \\[\\ddot{\\theta}_2 + \\omega_0^2\\theta_2 + \\kappa(\\theta_2-\\theta_1) = 0\\]\nwhere \\(\\omega_0^2 = g/L\\) is the natural frequency of a single pendulum and \\(\\kappa = k/m\\) represents the coupling strength.\nThe normal mode frequencies are: \\[\\omega_1 = \\omega_0 \\quad \\text{(in-phase mode)}\\] \\[\\omega_2 = \\sqrt{\\omega_0^2 + 2\\kappa} \\quad \\text{(out-of-phase mode)}\\]\nFor the in-phase mode, both pendula move in sync: \\[\\theta_1(t) = \\theta_2(t) = A\\cos(\\omega_1 t + \\phi_1)\\]\nFor the out-of-phase mode: \\[\\theta_1(t) = -\\theta_2(t) = B\\cos(\\omega_2 t + \\phi_2)\\]\nThe beat mode results from the superposition of these normal modes: \\[\\theta_1(t) = A\\cos(\\omega_1 t) + B\\cos(\\omega_2 t)\\] \\[\\theta_2(t) = A\\cos(\\omega_1 t) - B\\cos(\\omega_2 t)\\]\nWhen \\(A = B\\), this can be rewritten using trigonometric identities as: \\[\\theta_1(t) = 2A\\cos\\left(\\frac{\\omega_2-\\omega_1}{2}t\\right)\\cos\\left(\\frac{\\omega_2+\\omega_1}{2}t\\right)\\] \\[\\theta_2(t) = 2A\\sin\\left(\\frac{\\omega_2-\\omega_1}{2}t\\right)\\sin\\left(\\frac{\\omega_2+\\omega_1}{2}t\\right)\\]\nThis shows the characteristic beat pattern with a slow modulation (\\(\\frac{\\omega_2-\\omega_1}{2}\\)) of a fast carrier frequency (\\(\\frac{\\omega_2+\\omega_1}{2}\\)).\n\n\n\n\n\n\nWe can now perform the Fourier transform of our signals and visualize their frequency spectra. For the Fourier transform we will use the numpy.fft.fft() function.\n\n\n\n\n\n\nNumPy FFT Functions\n\n\n\nnp.fft.fft(): This function computes the one-dimensional discrete Fourier transform of an input signal. It converts a time-domain signal into its frequency domain representation, revealing which frequency components are present in the signal and their respective amplitudes. The output is a complex array of the same size as the input, containing amplitude and phase information for each frequency component.\nnp.fft.fftfreq(): This complementary function generates the frequency values corresponding to the output of np.fft.fft(). It requires two arguments: the length of the signal and the sampling period (time step between consecutive points). The returned array contains both positive and negative frequencies, with zero frequency at the beginning. For physical interpretations, we typically focus on the positive frequencies only.\nTogether, these functions allow us to analyze oscillatory data by identifying its constituent frequencies - essential for understanding systems like coupled pendula, wave phenomena, or any periodic behavior.\n\n\n\n\n\n\n\n\nOur analysis reveals that the beat mode represents a superposition of the system’s two normal modes. This demonstrates a fundamental principle: any possible state of a coupled oscillator system can be constructed from a superposition of its normal modes with specific amplitudes.\n\n\n\n\nThe frequency spectra we’ve calculated provide deep insights into the physical behavior of the coupled pendula system:\n\nPeak Locations: Each peak in the frequency spectrum corresponds to a characteristic frequency of the system. For our coupled pendula, these represent the natural frequencies of the normal modes.\nPeak Heights: The amplitude of each peak indicates how strongly that particular frequency contributes to the overall motion. In our beat mode, we see roughly equal contributions from both normal mode frequencies, explaining the energy exchange between pendula.\nPeak Width: The width of a frequency peak relates to damping in the system. Sharp, narrow peaks indicate weakly damped oscillations that persist for many cycles, while broader peaks suggest stronger damping.\nAbsence of Frequencies: The absence of certain frequencies is also informative. Our analysis shows only the two normal mode frequencies, confirming that our system behaves as expected for a coupled oscillator with two degrees of freedom.\n\nWhen analyzing more complex systems, Fourier analysis becomes even more powerful. For instance, in quantum mechanics, the frequency spectrum of a wavefunction reveals its energy level structure; in structural engineering, it identifies resonant frequencies that could lead to catastrophic failure; and in signal processing, it helps distinguish meaningful signals from noise.\nBy understanding frequency domain representations alongside time domain behavior, we gain a complete picture of dynamical systems and their responses to various inputs.",
    "crumbs": [
      "Python Basics",
      "Lecture 7",
      "Fourier Analysis"
    ]
  },
  {
    "objectID": "lectures/lecture07/3_fourier_analysis.html#fourier-analysis",
    "href": "lectures/lecture07/3_fourier_analysis.html#fourier-analysis",
    "title": "Computer-Based Physical Modelling",
    "section": "",
    "text": "Fourier analysis, or the description of functions as a series of sine and cosine functions, serves as a powerful tool in both numerical data analysis and the solution of differential equations. In experimental physics, Fourier transforms find widespread applications. For instance, optical tweezers utilize frequency spectra to characterize positional fluctuations, while Lock-In detection employs Fourier analysis for specific frequency signals. Additionally, many optical phenomena can be understood through the lens of Fourier transforms.\nFourier analysis extends far beyond these examples, finding applications across numerous fields of physics and engineering. In this lecture, we will examine Fourier Series and Fourier transforms from a mathematical perspective. We will apply these concepts to analyze the frequency spectrum of oscillations in coupled pendula, and later revisit them when simulating the motion of a Gaussian wavepacket in quantum mechanics.\n\n\n\n\n\n\n\n\nA Fourier series represents a periodic function \\(f(t)\\) with period \\(2\\pi\\) or, more generally, any arbitrary interval \\(T\\) as a sum of sine and cosine functions:\n\\[\nf(t)=\\frac{A_{0}}{2}+\\sum_{k=1}^{\\infty}\\left ( A_{k}\\cos\\left (\\omega_k t\\right) + B_{k}\\sin\\left (\\omega_k t\\right)\\right )\n\\tag{1}\\]\nwhere \\(\\omega_k=\\frac{2\\pi k}{T}\\). Here, \\(T\\) represents the period of the cosine and sine functions, with their amplitudes defined by the coefficients \\(A_k\\) and \\(B_k\\). The term \\(A_0\\) represents a constant offset added to the oscillating functions. Equation 1 expresses an arbitrary periodic function \\(f(t)\\) on an interval T as a sum of oscillating sine and cosine functions with discrete frequencies (\\(\\omega_k\\)):\n\\[\\begin{equation*}\n\\omega_k= 0, \\frac{2\\pi}{T}, \\frac{4\\pi}{T}, \\frac{6\\pi}{T}, ... , \\frac{n\\pi}{T}\n\\end{equation*}\\]\nand varying amplitudes. We can demonstrate that the cosine and sine functions in the sum (Equation 1) are orthogonal using the trigonometric identity:\n\\[\\begin{equation}\n\\sin(\\omega_{i} t)\\sin(\\omega_{k}t )=\\frac{1}{2}\\lbrace\\cos((\\omega_{i}-\\omega_{k})t)- \\cos((\\omega_{i}+\\omega_{k})t\\rbrace\n\\end{equation}\\]\nThis leads to the integral:\n\\[\\begin{equation}\n\\int\\limits_{-\\frac{T}{2}}^{+\\frac{T}{2}}  \\sin(\\omega_{i}t)\\sin (\\omega_k t) dt\n\\end{equation}\\]\nwhich splits into two integrals over cosine functions with sum \\((\\omega_{1}+\\omega_{2})\\) and difference frequency \\((\\omega_{1}-\\omega_{2})\\). With \\(\\omega_k=k 2\\pi/T\\), \\((k \\in \\mathbb{Z}^+ )\\), this evaluates to:\n\\[\\begin{equation}\n\\int\\limits_{-\\frac{T}{2}}^{+\\frac{T}{2}}  \\sin(\\omega_{i}t)\\sin (\\omega_k t) dt  =\\begin{cases}\n0 &\\text{for }  i\\neq k, \\\\\nT/2 &\\text{for }  i=k\n\\end{cases}\n\\end{equation}\\]\nA similar result holds for cosine functions:\n\\[\\begin{equation}\n\\int\\limits_{-\\frac{T}{2}}^{+\\frac{T}{2}}  \\cos(\\omega_{i}t)\\cos (\\omega_k t) dt  =\\begin{cases}\n0 &\\text{for }  i\\neq k, \\\\\nT/2 &\\text{for }  i=k\n\\end{cases}\n\\end{equation}\\]\nThe coefficients \\(A_k\\) and \\(B_k\\) are determined by projecting the function \\(f(t)\\) onto these basis functions:\n\\[\\begin{align}\n\\int\\limits_{-\\frac{T}{2}}^{+\\frac{T}{2}} & \\cos (\\omega_k t) dt  =\\begin{cases}\n0 &\\text{for }  k\\neq0, \\\\\nT &\\text{for }  k=0\n\\end{cases} \\\\\n\\int\\limits_{-\\frac{T}{2}}^{+\\frac{T}{2}} & \\sin(\\omega_k t) dt=0  \\text{ for all }k\n\\end{align}\\]\nFor the cosine coefficients:\n\\[\\begin{equation}\\label{A_k}\nA_k=\\frac{2}{T}\\int\\limits_{-\\frac{T}{2}}^{+\\frac{T}{2}} f(t)\\cos(\\omega_k t) dt  \\text{ for } k \\neq 0\n\\end{equation}\\]\nand the constant term:\n\\[\\begin{equation}\nA_0= \\frac{1}{T}\\int\\limits_{-\\frac{T}{2}}^{+\\frac{T}{2}} f(t) dt\n\\end{equation}\\]\nFinally, for the sine coefficients:\n\\[\\begin{equation}\\label{B_k}\nB_k=\\frac{2}{T}\\int\\limits_{-\\frac{T}{2}}^{+\\frac{T}{2}} f(t) \\sin(\\omega_k t) dt,\\,  \\forall k\n\\end{equation}\\]\n\n\n\n\n\n\nPhysical interpretation of Fourier coefficients\n\n\n\nThe Fourier coefficients provide crucial physical insights about the original function:\n\n\\(A_0\\) represents the mean value or DC offset of the function over one period.\n\\(A_k\\) and \\(B_k\\) represent the strength (amplitude) of frequency components at \\(\\omega_k = \\frac{2\\pi k}{T}\\).\nThe larger the coefficient, the more that particular frequency contributes to the overall function.\nThe power of a frequency component is proportional to \\(A_k^2 + B_k^2\\).\nThe phase angle \\(\\phi_k = \\tan^{-1}(-B_k/A_k)\\) tells us the relative timing of each frequency component.\n\nThis decomposition allows us to understand complex signals as combinations of simpler oscillations. For instance, in a vibrating string, each coefficient corresponds to the amplitude of a particular harmonic. In an electrical circuit, these coefficients represent the amplitude of voltage or current at specific frequencies.\n\n\n\n\n\nLet’s consider a concrete example: a square wave function with period \\(T=2\\pi\\):\n\\[f(t) = \\begin{cases}\n1, & 0 &lt; t &lt; \\pi \\\\\n-1, & -\\pi &lt; t &lt; 0\n\\end{cases}\\]\n\n\n\n\n\n\n\n\nFigure 1: Square Wave Function\n\n\n\n\n\nThis represents, for example, a signal that alternates between two voltage levels.\nTo find the Fourier coefficients, we have to determine the coefficients according to the integrals shown above. According to that, the DC component is:\n\\[A_0 = \\frac{1}{2\\pi}\\int_{-\\pi}^{\\pi} f(t) dt = \\frac{1}{2\\pi}\\left(\\int_{-\\pi}^{0} (-1) dt + \\int_{0}^{\\pi} 1 dt\\right) = 0\\]\nAlso, the coefficients for the cosine terms are:\n\\[A_k = \\frac{1}{\\pi}\\int_{-\\pi}^{\\pi} f(t)\\cos(kt) dt = \\frac{1}{\\pi}\\left(\\int_{-\\pi}^{0} (-1)\\cos(kt) dt + \\int_{0}^{\\pi} \\cos(kt) dt\\right) = 0 \\, \\text{for all } k\\]\nzero, as the cosine function is even and the square wave function is odd. Therefore, only the sine terms will contribute to the Fourier series.\n\\[B_k = \\frac{1}{\\pi}\\int_{-\\pi}^{\\pi} f(t)\\sin(kt) dt = \\frac{1}{\\pi}\\left(\\int_{-\\pi}^{0} (-1)\\sin(kt) dt + \\int_{0}^{\\pi} \\sin(kt) dt\\right)\\]\nWorking through the integral, we find:\n\\[B_k = \\frac{2}{\\pi}\\int_{0}^{\\pi} \\sin(kt) dt = \\begin{cases}\n\\frac{4}{k\\pi}, & k \\text{ odd} \\\\\n0, & k \\text{ even}\n\\end{cases}\\]\nTherefore, the Fourier series for our square wave is: \\[f(t) = \\frac{4}{\\pi}\\left(\\sin(t) + \\frac{1}{3}\\sin(3t) + \\frac{1}{5}\\sin(5t) + \\ldots\\right) = \\frac{4}{\\pi}\\sum_{n=0}^{\\infty} \\frac{1}{2n+1}\\sin((2n+1)t)\\]\n\n\n\n\n\n\nNotice how adding more terms in the series makes the approximation closer to the true square wave. Also note that high-frequency components (larger k values) have smaller amplitudes, explaining why the approximation smoothes out the sharp transitions even with many terms.\n\n\n\n\n\n\n\n\n\nThe Fourier transform extends the concept of Fourier series by representing arbitrary non-periodic functions \\(f(t)\\) through a continuous spectrum of complex functions \\(\\exp(i\\omega t)\\). Known as the continuous Fourier transform, this approach replaces the discrete frequency sum of sine and cosine functions found in Fourier series with an integral over complex exponential functions \\(\\exp(i\\omega t)\\) spanning continuous frequency values \\(\\omega\\).\n\n\nBefore diving into the Fourier transform, let’s clarify the transition from the real representation (sines and cosines) to the complex exponential notation. Using Euler’s formula:\n\\[e^{i\\theta} = \\cos(\\theta) + i\\sin(\\theta)\\]\nWe can rewrite our Fourier series from:\n\\[f(t)=\\frac{A_{0}}{2}+\\sum_{k=1}^{\\infty}\\left ( A_{k}\\cos\\left (\\omega_k t\\right) + B_{k}\\sin\\left (\\omega_k t\\right)\\right )\\]\nto:\n\\[f(t)=\\sum_{k=-\\infty}^{\\infty} c_k e^{i\\omega_k t}\\]\nwhere \\(c_k\\) is a complex coefficient related to \\(A_k\\) and \\(B_k\\) by:\n\\[c_0 = \\frac{A_0}{2}, \\quad c_k = \\frac{A_k - iB_k}{2}, \\quad c_{-k} = \\frac{A_k + iB_k}{2}\\]\nThis complex representation often simplifies calculations and provides a more elegant mathematical formulation. The advantage becomes even more apparent when we move from discrete frequencies (Fourier series) to continuous frequencies (Fourier transform).\nThe Fourier transform of a function \\(f(t)\\) is defined as:\n\\[\nF(\\omega)=\\int\\limits_{-\\infty}^{+\\infty}f(t)e^{-i\\omega t}dt\n\\tag{2}\\]\nwhere \\(F(\\omega)\\) represents the spectrum of frequencies present in \\(f(t)\\). The inverse Fourier transform recovers the original function \\(f(t)\\) from its frequency spectrum (Equation 2):\n\\[\\begin{equation}\\label{eq:inverse_FT}\nf(t)=\\frac{1}{2\\pi}\\int\\limits_{-\\infty}^{+\\infty}F(\\omega)e^{+i\\omega t}dt\n\\end{equation}\\]\nThe Fourier transform \\(F(\\omega)\\) yields a complex number, encoding both phase and amplitude information of the oscillations. The phase of oscillation at frequency \\(\\omega\\) is given by:\n\\[\\begin{equation}\n\\phi=\\tan^{-1}\\left(\\frac{Im(F(\\omega))}{Re(F(\\omega))}\\right)\n\\end{equation}\\]\nwhile the amplitude at frequency \\(\\omega\\) is:\n\\[\\begin{equation}\nx_{0}^{\\rm theo}=|F(\\omega)|\n\\end{equation}\\]\nModern computing offers efficient algorithms for Fourier transformation, particularly the Fast Fourier Transform (FFT) implemented in libraries like NumPy. We’ll use these algorithms to identify oscillation patterns in our signals. Here’s an example of using NumPy’s FFT functions to compute the transform and generate the corresponding frequency axis:\nf=np.fft.fft(alpha)\n\n\n\n\nLet us now apply Fourier analysis to examine the data from our coupled pendula, which includes both normal modes and beat mode oscillations of the harmonic oscillator. Instead of using the numerical data from the simulation, we will use the analytical expressions for the normal modes and beat mode to demonstrate the Fourier analysis.\n\n\nFor two pendula with equal masses, coupled by a spring of constant \\(k\\), the equations of motion are:\n\\[\\ddot{\\theta}_1 + \\omega_0^2\\theta_1 + \\kappa(\\theta_1-\\theta_2) = 0\\] \\[\\ddot{\\theta}_2 + \\omega_0^2\\theta_2 + \\kappa(\\theta_2-\\theta_1) = 0\\]\nwhere \\(\\omega_0^2 = g/L\\) is the natural frequency of a single pendulum and \\(\\kappa = k/m\\) represents the coupling strength.\nThe normal mode frequencies are: \\[\\omega_1 = \\omega_0 \\quad \\text{(in-phase mode)}\\] \\[\\omega_2 = \\sqrt{\\omega_0^2 + 2\\kappa} \\quad \\text{(out-of-phase mode)}\\]\nFor the in-phase mode, both pendula move in sync: \\[\\theta_1(t) = \\theta_2(t) = A\\cos(\\omega_1 t + \\phi_1)\\]\nFor the out-of-phase mode: \\[\\theta_1(t) = -\\theta_2(t) = B\\cos(\\omega_2 t + \\phi_2)\\]\nThe beat mode results from the superposition of these normal modes: \\[\\theta_1(t) = A\\cos(\\omega_1 t) + B\\cos(\\omega_2 t)\\] \\[\\theta_2(t) = A\\cos(\\omega_1 t) - B\\cos(\\omega_2 t)\\]\nWhen \\(A = B\\), this can be rewritten using trigonometric identities as: \\[\\theta_1(t) = 2A\\cos\\left(\\frac{\\omega_2-\\omega_1}{2}t\\right)\\cos\\left(\\frac{\\omega_2+\\omega_1}{2}t\\right)\\] \\[\\theta_2(t) = 2A\\sin\\left(\\frac{\\omega_2-\\omega_1}{2}t\\right)\\sin\\left(\\frac{\\omega_2+\\omega_1}{2}t\\right)\\]\nThis shows the characteristic beat pattern with a slow modulation (\\(\\frac{\\omega_2-\\omega_1}{2}\\)) of a fast carrier frequency (\\(\\frac{\\omega_2+\\omega_1}{2}\\)).\n\n\n\n\n\n\nWe can now perform the Fourier transform of our signals and visualize their frequency spectra. For the Fourier transform we will use the numpy.fft.fft() function.\n\n\n\n\n\n\nNumPy FFT Functions\n\n\n\nnp.fft.fft(): This function computes the one-dimensional discrete Fourier transform of an input signal. It converts a time-domain signal into its frequency domain representation, revealing which frequency components are present in the signal and their respective amplitudes. The output is a complex array of the same size as the input, containing amplitude and phase information for each frequency component.\nnp.fft.fftfreq(): This complementary function generates the frequency values corresponding to the output of np.fft.fft(). It requires two arguments: the length of the signal and the sampling period (time step between consecutive points). The returned array contains both positive and negative frequencies, with zero frequency at the beginning. For physical interpretations, we typically focus on the positive frequencies only.\nTogether, these functions allow us to analyze oscillatory data by identifying its constituent frequencies - essential for understanding systems like coupled pendula, wave phenomena, or any periodic behavior.\n\n\n\n\n\n\n\n\nOur analysis reveals that the beat mode represents a superposition of the system’s two normal modes. This demonstrates a fundamental principle: any possible state of a coupled oscillator system can be constructed from a superposition of its normal modes with specific amplitudes.\n\n\n\n\nThe frequency spectra we’ve calculated provide deep insights into the physical behavior of the coupled pendula system:\n\nPeak Locations: Each peak in the frequency spectrum corresponds to a characteristic frequency of the system. For our coupled pendula, these represent the natural frequencies of the normal modes.\nPeak Heights: The amplitude of each peak indicates how strongly that particular frequency contributes to the overall motion. In our beat mode, we see roughly equal contributions from both normal mode frequencies, explaining the energy exchange between pendula.\nPeak Width: The width of a frequency peak relates to damping in the system. Sharp, narrow peaks indicate weakly damped oscillations that persist for many cycles, while broader peaks suggest stronger damping.\nAbsence of Frequencies: The absence of certain frequencies is also informative. Our analysis shows only the two normal mode frequencies, confirming that our system behaves as expected for a coupled oscillator with two degrees of freedom.\n\nWhen analyzing more complex systems, Fourier analysis becomes even more powerful. For instance, in quantum mechanics, the frequency spectrum of a wavefunction reveals its energy level structure; in structural engineering, it identifies resonant frequencies that could lead to catastrophic failure; and in signal processing, it helps distinguish meaningful signals from noise.\nBy understanding frequency domain representations alongside time domain behavior, we gain a complete picture of dynamical systems and their responses to various inputs.",
    "crumbs": [
      "Python Basics",
      "Lecture 7",
      "Fourier Analysis"
    ]
  },
  {
    "objectID": "lectures/lecture06/4_solving_ODEs.html",
    "href": "lectures/lecture06/4_solving_ODEs.html",
    "title": "Solving Ordinary Differential Equations (ODEs)",
    "section": "",
    "text": "This lecture covers methods for solving ordinary differential equations (ODEs), which are fundamental to many physics problems. We’ll explore different numerical approaches, from basic to more sophisticated methods."
  },
  {
    "objectID": "lectures/lecture06/4_solving_ODEs.html#introduction",
    "href": "lectures/lecture06/4_solving_ODEs.html#introduction",
    "title": "Solving Ordinary Differential Equations (ODEs)",
    "section": "",
    "text": "This lecture covers methods for solving ordinary differential equations (ODEs), which are fundamental to many physics problems. We’ll explore different numerical approaches, from basic to more sophisticated methods."
  },
  {
    "objectID": "lectures/lecture06/4_solving_ODEs.html#the-harmonic-oscillator",
    "href": "lectures/lecture06/4_solving_ODEs.html#the-harmonic-oscillator",
    "title": "Solving Ordinary Differential Equations (ODEs)",
    "section": "The Harmonic Oscillator",
    "text": "The Harmonic Oscillator\n\n\n\n\n\n\nThe Classical Harmonic Oscillator\n\n\n\nThe harmonic oscillator represents one of the most important physical systems, appearing in: - Mechanical oscillations (springs, pendulums) - Electrical circuits (LC circuits) - Quantum mechanics (quantum harmonic oscillator) - Molecular vibrations\nThe equation of motion is:\n\\[\\begin{equation}\n\\frac{d^2x}{dt^2} + \\omega^2 x = 0\n\\end{equation}\\]\nwhere: - \\(x\\) is the displacement - \\(t\\) is time - \\(\\omega = \\sqrt{k/m}\\) is the angular frequency - \\(k\\) is the spring constant - \\(m\\) is the mass\nInitial conditions required: - Initial position: \\(x(t=0) = x_0\\) - Initial velocity: \\(\\dot{x}(t=0) = v_0\\)"
  },
  {
    "objectID": "lectures/lecture06/4_solving_ODEs.html#numerical-solution-methods",
    "href": "lectures/lecture06/4_solving_ODEs.html#numerical-solution-methods",
    "title": "Solving Ordinary Differential Equations (ODEs)",
    "section": "Numerical Solution Methods",
    "text": "Numerical Solution Methods\n\n1. Implicit Solution (Crank-Nicolson Method)\nThe matrix approach transforms our second-order ODE into a system of coupled equations. This method is particularly stable for oscillatory systems.\n\nMatrix Construction\nFor \\(N\\) time points, we construct two matrices:\n\nThe second derivative matrix (\\(T\\)):\n\n\\[\\begin{equation}\nT=\\frac{1}{\\delta t^2}\n\\begin{bmatrix}\n-2 & 1  & 0 & \\cdots & 0\\\\\n1 & -2 & 1 & \\cdots & 0\\\\\n\\vdots & \\ddots & \\ddots & \\ddots & \\vdots\\\\\n0 & \\cdots & 1 & -2 & 1\\\\\n0 & \\cdots & 0 & 1 & -2\n\\end{bmatrix}\n\\end{equation}\\]\n\nThe potential term matrix (\\(V\\)):\n\n\\[\\begin{equation}\nV = \\omega^2\n\\begin{bmatrix}\n1 & 0 & \\cdots & 0\\\\\n0 & 1 & \\cdots & 0\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\n0 & 0 & \\cdots & 1\n\\end{bmatrix}\n\\end{equation}\\]"
  },
  {
    "objectID": "lectures/lecture06/4_solving_ODEs.html#explicit-solution-methods",
    "href": "lectures/lecture06/4_solving_ODEs.html#explicit-solution-methods",
    "title": "Solving Ordinary Differential Equations (ODEs)",
    "section": "Explicit Solution Methods",
    "text": "Explicit Solution Methods\n\nState-Space Representation\nTo implement explicit numerical methods effectively, we first convert our second-order ODE into a system of first-order equations. This state-space representation is crucial for numerical integration.\nFor the harmonic oscillator:\n\\[\\begin{equation}\n\\ddot{x} + \\omega^2x = 0\n\\end{equation}\\]\nWe define: - Position: \\(x\\) - Velocity: \\(v = \\dot{x}\\)\nThis gives us the system:\n\\[\\begin{equation}\n\\begin{bmatrix} \\dot{x} \\\\ \\dot{v} \\end{bmatrix} =\n\\begin{bmatrix} v \\\\ -\\omega^2x \\end{bmatrix}\n\\end{equation}\\]\nOur state vector is:\n\\[\\begin{equation}\ny = \\begin{bmatrix} x \\\\ v \\end{bmatrix}\n\\end{equation}\\]\n\n\n1. Euler Method\nThe Euler method is the simplest numerical integration technique. It comes directly from the Taylor expansion:\n\\[\\begin{equation}\ny(t + \\Delta t) = y(t) + \\dot{y}(t)\\Delta t + O(\\Delta t^2)\n\\end{equation}\\]\n\n\n\n\n\n\n\n\n2. Euler-Cromer Method\nThe Euler-Cromer method (also known as the semi-implicit Euler method) is particularly good for oscillatory systems because it conserves energy better than the standard Euler method.\nKey difference: - Uses the updated velocity to compute position - Better energy conservation for oscillatory systems\n\\[\\begin{align}\nv_{i+1} &= v_i - \\omega^2 x_i \\Delta t \\\\\nx_{i+1} &= x_i + v_{i+1} \\Delta t\n\\end{align}\\]\n\n\n\n\n\n\n\n\n3. Velocity Verlet Method\nThe Velocity Verlet method is a symplectic integrator that provides excellent energy conservation for Hamiltonian systems. It’s particularly useful for molecular dynamics simulations.\nThe algorithm: 1. Update position using current velocity and acceleration 2. Calculate new acceleration at new position 3. Update velocity using average of old and new accelerations\n\\[\\begin{align}\nx_{i+1} &= x_i + v_i\\Delta t + \\frac{1}{2}a_i\\Delta t^2 \\\\\na_{i+1} &= -\\omega^2 x_{i+1} \\\\\nv_{i+1} &= v_i + \\frac{1}{2}(a_i + a_{i+1})\\Delta t\n\\end{align}\\]\n\n\n\n\n\n\n\n\nComparison of Methods\nLet’s compare these methods for the harmonic oscillator:\n\n\n\n\n\n\n\n\n\n\n\n\nMethod Characteristics\n\n\n\n\nEuler Method:\n\nSimplest method\nFirst-order accurate (\\(O(\\Delta t)\\))\nOften unstable for oscillatory systems\nEnergy tends to increase over time\n\nEuler-Cromer Method:\n\nBetter energy conservation\nStill first-order accurate\nMore stable for oscillatory systems\nEnergy tends to decrease slightly over time\n\nVelocity Verlet Method:\n\nSecond-order accurate (\\(O(\\Delta t^2)\\))\nExcellent energy conservation\nSymplectic (preserves phase space volume)\nRecommended for long-time integration"
  },
  {
    "objectID": "lectures/lecture06/4_solving_ODEs.html#solving-odes-with-scipy",
    "href": "lectures/lecture06/4_solving_ODEs.html#solving-odes-with-scipy",
    "title": "Solving Ordinary Differential Equations (ODEs)",
    "section": "Solving ODEs with SciPy",
    "text": "Solving ODEs with SciPy\nSciPy provides sophisticated ODE solvers through scipy.integrate.odeint and scipy.integrate.solve_ivp. These implementations use advanced algorithms with automatic step size adjustment and error control.\n\nUsing scipy.integrate.odeint\nThe odeint function uses the LSODA algorithm from the FORTRAN library ODEPACK, which automatically switches between methods for stiff and non-stiff problems.\n\n\n\n\n\n\nStiff vs Non-stiff Problems\n\n\n\n\nStiff problems: Have multiple timescales with widely different magnitudes\nNon-stiff problems: Have timescales of similar magnitude\n\nLSODA uses: - Adams method for non-stiff problems - BDF method (Backward Differentiation Formula) for stiff problems\n\n\n\n\n\n\n\n\n\n\nUsing scipy.integrate.solve_ivp\nThe newer solve_ivp function provides more control over the integration process and supports multiple modern solving methods.\n\n\n\n\n\n\n\n\n\n\n\n\nAvailable Methods in solve_ivp\n\n\n\n\nRK45 (default):\n\nExplicit Runge-Kutta method of order 5(4)\nGood general-purpose method\nAdaptive step size\n\nRK23:\n\nExplicit Runge-Kutta method of order 3(2)\nUsually faster but less accurate than RK45\nGood for rough solutions\n\nDOP853:\n\nExplicit Runge-Kutta method of order 8\nHigh accuracy\nMore expensive computationally\n\nBDF:\n\nImplicit method\nGood for stiff problems\nVariable order (1 to 5)\n\nLSODA:\n\nAutomatic method switching\nAdapts between Adams and BDF\nGood all-purpose solver\n\n\n\n\n\n\nAdvantages of SciPy Methods\n\nAdaptive Step Size:\n\nAutomatically adjusts step size for efficiency\nMaintains desired accuracy\nHandles rapid changes better\n\nError Control:\n\nSpecified through relative and absolute tolerances\nEnsures solution reliability\nProvides error estimates\n\nMethod Selection:\n\nChoose method based on problem characteristics\nAutomatic stiffness detection (LSODA)\nHigher-order methods available\n\nDense Output:\n\nContinuous solution representation\nInterpolation between steps\nEfficient for plotting or further analysis"
  },
  {
    "objectID": "lectures/lecture06/4_solving_ODEs.html#damped-driven-pendulum",
    "href": "lectures/lecture06/4_solving_ODEs.html#damped-driven-pendulum",
    "title": "Solving Ordinary Differential Equations (ODEs)",
    "section": "Damped Driven Pendulum",
    "text": "Damped Driven Pendulum\nThe damped driven pendulum is an excellent example of a nonlinear system that can exhibit both regular and chaotic behavior.\n\n\n\n\n\n\nThe Damped Driven Pendulum Equation\n\n\n\nThe equation of motion is:\n\\[\\begin{equation}\n\\ddot{\\theta} + \\frac{g}{L}\\sin(\\theta) + b\\dot{\\theta} = \\beta\\cos(\\omega t)\n\\end{equation}\\]\nwhere: - \\(\\theta\\) is the angle from vertical - \\(g\\) is gravitational acceleration - \\(L\\) is pendulum length - \\(b\\) is damping coefficient - \\(\\beta\\) is driving amplitude - \\(\\omega\\) is driving frequency\n\n\n\n\n\n\n\n\n\nParameter Study: Transition to Chaos\nLet’s examine how the system behavior changes with driving amplitude:\n\n\n\n\n\n\n\n\n\n\n\n\nKey Features of the Damped Driven Pendulum\n\n\n\n\nRegular Motion:\n\nSmall driving forces lead to periodic motion\nSystem settles into a stable orbit\nPredictable long-term behavior\n\nChaotic Motion:\n\nLarger driving forces can lead to chaos\nSensitive dependence on initial conditions\nUnpredictable long-term behavior\n\nBifurcations:\n\nSystem can transition between different types of motion\nCritical points where behavior changes qualitatively\nPeriod doubling route to chaos\n\nEnergy Balance:\n\nDriving force adds energy\nDamping removes energy\nCompetition leads to rich dynamics\n\n\n\n\n\n\nEnergy Analysis\nLet’s analyze the system’s energy over time:\n\n\n\n\n\n\nThis completes our analysis of the damped driven pendulum, demonstrating its rich dynamical behavior and various analysis techniques. The system serves as an excellent example of how nonlinearity can lead to complex behavior in even seemingly simple mechanical systems."
  },
  {
    "objectID": "lectures/lecture06/3-solving_ODEs.html",
    "href": "lectures/lecture06/3-solving_ODEs.html",
    "title": "Solving Ordinary Differential Equations",
    "section": "",
    "text": "In the previous lecture on numerical differentiation, we explored how to compute derivatives numerically using finite difference methods and matrix representations. We learned that:\n\nFinite difference approximations allow us to estimate derivatives at discrete points\nDifferentiation can be represented as matrix operations\nThe accuracy of these approximations depends on the order of the method and step size\n\nBuilding on this foundation, we can now tackle one of the most important applications in computational physics: solving ordinary differential equations (ODEs). Almost all dynamical systems in physics are described by differential equations, and learning how to solve them numerically is essential for modeling physical phenomena.\nAs second-semester physics students, you’ve likely encountered differential equations in various contexts:\n\nNewton’s second law: \\(F = ma = m\\frac{d^2x}{dt^2}\\)\nSimple harmonic motion: \\(\\frac{d^2x}{dt^2} + \\omega^2x = 0\\)\nRC circuits: \\(\\frac{dQ}{dt} + \\frac{1}{RC}Q = 0\\)\nHeat diffusion: \\(\\frac{\\partial T}{\\partial t} = \\alpha \\nabla^2 T\\)\n\nWhile analytical solutions exist for some simple cases, real-world physics problems often involve complex systems where analytical solutions are either impossible or impractical to obtain. This is where numerical methods become indispensable tools for the working physicist.\n\n\n\n\n\n\n\n\n\nThere are two main approaches to solving ODEs numerically:\n\nImplicit methods: These methods solve for all time points simultaneously using matrix operations. They treat the problem as a large system of coupled algebraic equations. Just as you might solve a system of linear equations \\(Ax = b\\) in linear algebra, implicit methods set up and solve a larger matrix equation that represents the entire evolution of the system.\nExplicit methods: These methods march forward in time, computing the solution step by step. Starting from the initial conditions, they use the current state to calculate the next state, similar to how you might use the position and velocity at time \\(t\\) to predict the position and velocity at time \\(t + \\Delta t\\).\n\nWe’ll explore both approaches, highlighting their strengths and limitations. Each has its place in physics: implicit methods often handle “stiff” problems better (problems with vastly different timescales), while explicit methods are typically easier to implement and can handle nonlinear problems more naturally.\n\n\n\n\n\n\n\n\n\nPhysics Interlude: The Harmonic Oscillator\n\n\n\nThe harmonic oscillator is one of the most fundamental systems in physics, appearing in mechanics, electromagnetism, quantum mechanics, and many other fields. It serves as an excellent test case for ODE solvers due to its simplicity and known analytical solution.\nThe harmonic oscillator describes any system that experiences a restoring force proportional to displacement. Physically, this occurs in:\n\nA mass on a spring (Hooke’s law: \\(F = -kx\\))\nA pendulum for small angles (where \\(\\sin\\theta \\approx \\theta\\))\nAn LC circuit with inductors and capacitors\nMolecular vibrations in chemistry\nPhonons in solid state physics\n\n\n\n\n\n\n\nFigure 1: A mass suspended from a spring demonstrates a classic harmonic oscillator. When displaced from equilibrium, the spring exerts a restoring force proportional to the displacement, leading to oscillatory motion.\n\n\n\nThe equation of motion for a classical harmonic oscillator is:\n\\[\\begin{equation}\n\\frac{d^2x}{dt^2} + \\omega^2 x = 0\n\\end{equation}\\]\nwhere \\(\\omega = \\sqrt{k/m}\\) is the angular frequency, with \\(k\\) being the spring constant and \\(m\\) the mass. In terms of Newton’s second law, this is: \\[\\begin{equation}\nm\\frac{d^2x}{dt^2} = -kx\n\\end{equation}\\]\nThis second-order differential equation requires two initial conditions: - Initial position: \\(x(t=0) = x_0\\) - Initial velocity: \\(\\dot{x}(t=0) = v_0\\)\nThe analytical solution is: \\[\\begin{equation}\nx(t) = x_0 \\cos(\\omega t) + \\frac{v_0}{\\omega} \\sin(\\omega t)\n\\end{equation}\\]\nThis represents sinusoidal oscillation with a constant amplitude and period \\(T = 2\\pi/\\omega\\). The total energy of the system (kinetic + potential) remains constant: \\[\\begin{equation}\nE = \\frac{1}{2}mv^2 + \\frac{1}{2}kx^2\n\\end{equation}\\]\nThis energy conservation will be an important test for our numerical methods.\n\n\n\n\n\nUsing the matrix representation of the second derivative operator from our previous lecture, we can transform the ODE into a linear system that can be solved in one step. This approach treats the entire time evolution as a single mathematical problem.\n\n\nFrom calculus, we know that the second derivative represents the rate of change of the rate of change. Physically, this corresponds to acceleration in mechanics. In numerical terms, we need to approximate this using finite differences.\nRecall that we can represent the second derivative operator as a tridiagonal matrix:\n\\[D_2 = \\frac{1}{\\Delta t^2}\n\\begin{bmatrix}\n-2 & 1  & 0 & 0 & \\cdots & 0\\\\\n1 & -2 & 1 & 0 & \\cdots & 0\\\\\n0 & 1  & -2 & 1 & \\cdots & 0\\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n0 & \\cdots & 0 & 1 & -2 & 1\\\\\n0 & \\cdots & 0 & 0 & 1 & -2\\\\\n\\end{bmatrix}\\]\nThis matrix implements the central difference approximation for the second derivative: \\[\\begin{equation}\n\\frac{d^2x}{dt^2} \\approx \\frac{x_{i+1} - 2x_i + x_{i-1}}{\\Delta t^2}\n\\end{equation}\\]\nEach row in this matrix corresponds to the equation for a specific time point, linking it to its neighbors.\nThe harmonic oscillator term \\(\\omega^2 x\\) represents the restoring force. In a spring system, this is \\(F = -kx\\) divided by mass, giving \\(a = -\\frac{k}{m}x = -\\omega^2 x\\). This can be represented by a diagonal matrix:\n\\[V = \\omega^2 I = \\omega^2\n\\begin{bmatrix}\n1 & 0 & 0 & \\cdots & 0\\\\\n0 & 1 & 0 & \\cdots & 0\\\\\n0 & 0 & 1 & \\cdots & 0\\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n0 & 0 & 0 & \\cdots & 1\\\\\n\\end{bmatrix}\\]\nOur equation \\(\\frac{d^2x}{dt^2} + \\omega^2 x = 0\\) becomes the matrix equation \\((D_2 + V)x = 0\\), where \\(x\\) is the vector containing the positions at all time points \\((x_1, x_2, ..., x_n)\\). This is a large system of linear equations that determine the entire trajectory at once.\n\n\n\nTo solve this system, we need to incorporate the initial conditions by modifying the first rows of our matrix. This is a crucial step because a second-order differential equation needs two initial conditions to define a unique solution. In physical terms, we need to know both the initial position (displacement) and the initial velocity of our oscillator. We incorporate these conditions directly into our matrix equation:\n\\[M =\n\\begin{bmatrix}\n\\color{red}{1} & \\color{red}{0} & \\color{red}{0} & \\color{red}{0} & \\color{red}{\\cdots} & \\color{red}{0} \\\\\n\\color{blue}{-1} & \\color{blue}{1} & \\color{blue}{0} & \\color{blue}{0} & \\color{blue}{\\cdots} & \\color{blue}{0} \\\\\n\\frac{1}{\\Delta t^2} & -\\frac{2}{\\Delta t^2}+\\omega^2 & \\frac{1}{\\Delta t^2} & 0 & \\cdots & 0 \\\\\n0 & \\frac{1}{\\Delta t^2} & -\\frac{2}{\\Delta t^2}+\\omega^2 & \\frac{1}{\\Delta t^2} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & \\cdots & 0 & \\frac{1}{\\Delta t^2} & -\\frac{2}{\\Delta t^2}+\\omega^2 & \\frac{1}{\\Delta t^2} \\\\\n\\end{bmatrix}\\]\nThe first row (in red) enforces \\(x(0) = x_0\\) (initial position), and the second row (in blue) implements the initial velocity condition. The rest of the matrix represents the differential equation at each time step. By incorporating the initial conditions directly into the matrix, we ensure that our solution satisfies both the differential equation and the initial conditions.\n\n\n\n\n\n\n\n\n\n\n\n\nThis matrix-based approach has several advantages:\n\nIt solves for all time points simultaneously, giving the entire trajectory in one operation\nIt can be very stable for certain types of problems, particularly “stiff” equations\nIt handles boundary conditions naturally by incorporating them into the matrix structure\nIt often provides good energy conservation for oscillatory systems\n\nHowever, it also has limitations:\n\nIt requires solving a large linear system, which becomes computationally intensive for long simulations\nIt’s not suitable for nonlinear problems without modification (e.g., a pendulum with large angles where \\(\\sin\\theta \\neq \\theta\\))\nMemory requirements grow with the time span (an N×N matrix for N time steps)\nImplementing time-dependent forces is more complex than with explicit methods\n\nFrom a physics perspective, this approach is similar to finding stationary states in quantum mechanics or solving boundary value problems in electrostatics—we’re solving the entire system at once rather than stepping through time.\n\n\n\n\nAn alternative approach is to use explicit step-by-step integration methods. These methods convert the second-order ODE to a system of first-order ODEs and then advance the solution incrementally, similar to how we intuitively understand physical motion: position changes based on velocity, and velocity changes based on acceleration.\n\n\nTo convert our second-order ODE to a first-order system, we introduce a new variable \\(v = dx/dt\\) (the velocity):\n\\[\\begin{align}\n\\frac{dx}{dt} &= v \\\\\n\\frac{dv}{dt} &= -\\omega^2 x\n\\end{align}\\]\nThis transformation is common in physics. For example, in classical mechanics, we often convert Newton’s second law (a second-order ODE) into phase space equations involving position and momentum. In the case of the harmonic oscillator:\n\nThe first equation simply states that the rate of change of position is the velocity\nThe second equation comes from \\(F = ma\\) where \\(F = -kx\\), giving \\(a = \\frac{F}{m} = -\\frac{k}{m}x = -\\omega^2 x\\)\n\nWe can now represent the state of our system as a vector\n\\[\\vec{y} = \\begin{bmatrix} x \\\\ v \\end{bmatrix}\\]\nand the derivative as\n\\[\\dot{\\vec{y}} = \\begin{bmatrix} v \\\\ -\\omega^2 x \\end{bmatrix}\\]\nThis representation is called the “phase space” description, and is fundamental in classical mechanics and dynamical systems theory.\n\n\n\n\n\n\nRecipe to solve a differential equation\n\n\n\n\nState your differential equation as a system of ordinary differential equations \\[\\dot{\\vec{y}} = \\begin{bmatrix} v \\\\ -\\omega^2 x \\end{bmatrix}\\]\nWrite a function that represents you physical system\ndef SHO(time, state, omega=2):\n    g0 = state[1]               -&gt;velocity\n    g1 = -omega * state [0]       -&gt;acceleration\n    return np.array([g0, g1])\nUse a function to integrate the system of ODEs\nt_euler, y_euler = euler_method(lambda t, y: SHO(t, y, omega),\n    y0,\n    t_span,\n    dt\n)\nor\nsolution = solve_ivp( lambda t, y: SHO(t, y, omega),\n    t_span,\n    y0,\n    method='BDF',\n    t_eval=t_eval\n)\n\n\n\n\nEuler MethodEuler-Cromer MethodMidpoint Method\n\n\nThe simplest explicit integration method is the Euler method, derived from the first-order Taylor expansion:\n\\[\\begin{equation}\n\\vec{y}(t + \\Delta t) \\approx \\vec{y}(t) + \\dot{\\vec{y}}(t) \\Delta t\n\\end{equation}\\]\nThis is essentially a linear approximation—assuming the derivative stays constant over the small time step. Physically, it’s like assuming constant velocity when updating position, and constant acceleration when updating velocity over each small time interval.\nFor the harmonic oscillator, this becomes:\n\\[\\begin{align}\nx_{n+1} &= x_n + v_n \\Delta t \\\\\nv_{n+1} &= v_n - \\omega^2 x_n \\Delta t\n\\end{align}\\]\nThe physical interpretation is straightforward:\n\nThe new position equals the old position plus the displacement (velocity × time)\nThe new velocity equals the old velocity plus the acceleration (-ω²x) multiplied by the time step\n\nIn practice, the Euler method will cause the total energy of a harmonic oscillator to increase over time, which is physically incorrect. This is because the method doesn’t account for the continuous change in acceleration during the time step.\n\n\n\n\n\n\n\n\nThe Euler method often performs poorly for oscillatory systems, as it tends to artificially increase the energy over time. A simple but effective improvement is the Euler-Cromer method (also known as the symplectic Euler method), which uses the updated velocity for the position update:\n\\[\\begin{align}\nv_{n+1} &= v_n - \\omega^2 x_n \\Delta t \\\\\nx_{n+1} &= x_n + v_{n+1} \\Delta t\n\\end{align}\\]\nNotice the subtle but crucial difference: we use \\(v_{n+1}\\) (the newly calculated velocity) to update the position, rather than \\(v_n\\).\nThis small change dramatically improves energy conservation for oscillatory systems. From a physics perspective, this method better preserves the structure of Hamiltonian systems, which include the harmonic oscillator, planetary motion, and many other important physical systems.\nThe Euler-Cromer method is especially valuable in physics simulations where long-term energy conservation is important, such as:\n\nOrbital mechanics\nMolecular dynamics\nPlasma physics\nN-body simulations\n\nWhile not perfect, this simple modification makes the method much more useful for real physical systems with oscillatory behavior.\n\n\n\n\n\n\n\n\nFor higher accuracy, we can use the midpoint method (also known as the second-order Runge-Kutta method or RK2). This evaluates the derivative at the midpoint of the interval, providing a better approximation of the average derivative over the time step.\n\\[\\begin{align}\nk_1 &= f(t_n, y_n) \\\\\nk_2 &= f(t_n + \\frac{\\Delta t}{2}, y_n + \\frac{\\Delta t}{2}k_1) \\\\\ny_{n+1} &= y_n + \\Delta t \\cdot k_2\n\\end{align}\\]\nThe physical intuition behind this method is:\n\nFirst, calculate the initial derivative \\(k_1\\) (representing the initial rates of change)\nUse this derivative to estimate the state at the middle of the time step\nCalculate a new derivative \\(k_2\\) at this midpoint\nUse the midpoint derivative to advance the full step\n\nThis is analogous to finding the average velocity by looking at the velocity in the middle of a time interval, rather than just at the beginning. In physical problems with continuously varying forces, this provides a much better approximation than the Euler method.\nThe midpoint method achieves \\(O(\\Delta t^2)\\) accuracy, meaning the error decreases with the square of the step size. This makes it much more accurate than Euler’s method for the same computational cost.\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s compare these methods for the harmonic oscillator:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEuler Method: Simple but least accurate. It systematically increases the total energy of the system, causing the amplitude to grow over time.\nEuler-Cromer Method: Much better energy conservation for oscillatory systems with minimal additional computation.\nMidpoint Method: Higher accuracy and better energy conservation.\n\nThe choice of method depends on the specific requirements of your problem:\n\nUse Euler for simplicity when accuracy is not critical\nUse Euler-Cromer for oscillatory systems when computational efficiency is important\nUse Midpoint or higher-order methods when accuracy is crucial\n\n\n\n\n\nFor practical applications, SciPy provides sophisticated ODE solvers with adaptive step size control, error estimation, and specialized algorithms for different types of problems. These methods represent the state-of-the-art in numerical integration and are what working physicists typically use for research and advanced applications.\n\n\n\nAdaptive Step Size: Unlike our fixed-step methods, these solvers can automatically adjust the step size based on the local error estimate, taking smaller steps where the solution changes rapidly and larger steps where it’s smooth.\nHigher-Order Methods: These solvers use higher-order approximations (4th, 5th, or even 8th order), achieving much higher accuracy with the same computational effort.\nError Control: They can maintain the error below a specified tolerance, giving you confidence in the accuracy of your results.\nSpecialized Methods: Different methods are optimized for different types of problems (stiff vs. non-stiff, conservative vs. dissipative).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdvanced Example: Damped Driven Pendulum\n\n\n\n\n\nLet’s apply these methods to a more complex system: a damped driven pendulum. This system can exhibit chaotic behavior under certain conditions, making it a fascinating study in nonlinear dynamics.\n\n\n\n\n\n\nFigure 2: The damped driven pendulum. External driving forces and damping (friction) create a system that can exhibit complex behaviors from regular oscillations to chaos.\n\n\n\n\n\nThe damped driven pendulum represents many real physical systems\n\nA physical pendulum with friction and external driving force\nAn RLC circuit with nonlinear components\nJosephson junctions in superconductivity\nCertain types of mechanical and electrical oscillators\n\nThe equation of motion is:\n\\[\\begin{equation}\n\\frac{d^2\\theta}{dt^2} + b\\frac{d\\theta}{dt} + \\omega_0^2\\sin\\theta = F_0\\cos(\\omega_d t)\n\\end{equation}\\]\nwhere\n\n\\(\\theta\\) is the angle from vertical (radians)\n\\(b\\) is the damping coefficient (represents friction or resistance)\n\\(\\omega_0 = \\sqrt{g/L}\\) is the natural frequency (where \\(g\\) is gravity and \\(L\\) is pendulum length)\n\\(F_0\\) is the driving amplitude (strength of the external force)\n\\(\\omega_d\\) is the driving frequency (how rapidly the external force oscillates)\n\nThis equation differs from the harmonic oscillator in three crucial ways:\n\nThe \\(\\sin\\theta\\) term (rather than just \\(\\theta\\)) makes it nonlinear\nThe damping term \\(b\\frac{d\\theta}{dt}\\) causes energy dissipation\nThe driving term \\(F_0\\cos(\\omega_d t)\\) adds energy to the system\n\nThese additions make the system much more realistic and rich in behavior. Students could explore this system by varying parameters like the damping coefficient (b), driving amplitude (F0), and driving frequency (omega_d) to observe how the pendulum transitions between regular oscillations, period doubling, and chaos.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChaotic motion refers to a deterministic system whose behavior appears random and exhibits extreme sensitivity to initial conditions—the famous “butterfly effect.” In the damped driven pendulum, chaos emerges when the system’s nonlinear restoring force (\\(\\sin\\theta\\) term) interacts with sufficient driving force. For this system, chaotic behavior typically appears when the driving amplitude \\(F_0\\) exceeds a critical value (approximately 1.0 in our case) while maintaining moderate damping (\\(b \\approx 0.1\\)) and a driving frequency \\(\\omega_d\\) that is not too far from the natural frequency \\(\\omega_0\\). In the chaotic regime, the pendulum’s trajectory becomes unpredictable over long time scales, despite being governed by deterministic equations. The phase space transforms from closed orbits (regular motion) to a strange attractor with fractal structure, and the system never settles into a periodic oscillation.\n\n\n\n\n\n\nLyapunov Exponents: Quantifying Chaos\n\n\n\n\n\nThe Lyapunov exponent is a powerful mathematical tool for characterizing chaotic systems. It measures the rate at which nearby trajectories in phase space diverge over time. For a system with state vector \\(\\vec{x}\\), if two initial conditions differ by a small displacement \\(\\delta\\vec{x}_0\\), then after time \\(t\\), this separation grows approximately as:\n\\[|\\delta\\vec{x}(t)| \\approx e^{\\lambda t}|\\delta\\vec{x}_0|\\]\nwhere \\(\\lambda\\) is the Lyapunov exponent.\n\nA positive Lyapunov exponent (\\(\\lambda &gt; 0\\)) indicates chaos: nearby trajectories diverge exponentially, making long-term prediction impossible. The larger the exponent, the more chaotic the system.\nA zero Lyapunov exponent (\\(\\lambda = 0\\)) suggests a stable limit cycle or quasiperiodic behavior.\nA negative Lyapunov exponent (\\(\\lambda &lt; 0\\)) indicates a stable fixed point, where trajectories converge.\n\nFor our damped driven pendulum, we can numerically estimate the Lyapunov exponent by comparing how two slightly different initial conditions evolve over time. For instance, we might begin with two pendulums at nearly the same angle—perhaps \\(\\theta_1(0) = 0.1\\) and \\(\\theta_2(0) = 0.1001\\)—while keeping their initial angular velocities identical at \\(\\omega_1(0) = \\omega_2(0) = 0\\). As we simulate both systems, we can track the separation between their trajectories in phase space. This separation represents the difference vector \\(\\delta\\vec{x}(t) = [\\theta_2(t) - \\theta_1(t), \\omega_2(t) - \\omega_1(t)]\\). Initially, this difference is very small, but in a chaotic system, it grows exponentially with time. By measuring this growth rate, we can compute the Lyapunov exponent as \\(\\lambda \\approx \\frac{1}{t}\\ln\\left(\\frac{|\\delta\\vec{x}(t)|}{|\\delta\\vec{x}(0)|}\\right)\\). When examining our pendulum system, which has two dimensions (\\(\\theta\\) and \\(\\omega\\)), we actually have two Lyapunov exponents forming a spectrum. If either of these exponents is positive, we can conclusively determine that our system exhibits chaotic behavior, indicating fundamental unpredictability despite its deterministic nature.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this lecture, we’ve built upon our knowledge of numerical differentiation to solve ordinary differential equations. We’ve explored:\n\nImplicit Matrix Methods: Using matrices to solve the entire system at once\nExplicit Integration Methods: Step-by-step methods like Euler, Euler-Cromer, and Midpoint\nAdvanced SciPy Methods: Leveraging powerful adaptive solvers for complex problems\n\nEach approach has its strengths and is suited to different types of problems. The matrix method is excellent for linear systems, while explicit methods are more versatile for nonlinear problems. SciPy’s solvers combine accuracy, stability, and efficiency for practical applications.\n\n\nThe methods we’ve learned are essential tools for computational physics because they: 1. Allow us to study systems with no analytical solutions 2. Provide insights into nonlinear dynamics and chaos 3. Enable the modeling of realistic systems with friction, driving forces, and complex interactions 4. Connect mathematical formulations with observable physical phenomena\nAs a second-semester physics student, these numerical tools complement your analytical understanding of mechanics, electromagnetism, and other core physics subjects. When analytical methods reach their limits, these numerical approaches allow you to continue exploring and modeling physical reality.\n\n\n\n\nHere are two simple exercises with solutions where you can train your programming skills. In these examples you should use the odeint method from SciPy’s integrate module.\n\n\n\n\n\n\nSelf-Exercise 1: Simple Harmonic Oscillator\n\n\n\nWrite a program to solve the equation of motion for a simple harmonic oscillator. This example demonstrates how to solve a second-order differential equation using scipy’s odeint.\nThe equation of motion is: \\(\\frac{d^2x}{dt^2} + \\omega^2x = 0\\)\nThis represents an idealized spring-mass system or pendulum with small oscillations.\n\n\n\n\n\n\n\nUse odeint(oscillator, initial_state, t, args=(omega,)) to solve the system. The solution will have two columns: position ([:,0]) and velocity ([:,1]). Create a plot showing position vs time using matplotlib. Remember to label your axes and add a title.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSelf-Exercise 2: Damped Driven Harmonic Oscillator using Matrix Method\n\n\n\nModel a damped harmonic oscillator with an external driving force using the implicit matrix method. This exercise demonstrates how to solve a second-order differential equation by constructing and solving a matrix system that represents the entire time evolution.\nThe equation of motion is: \\(m\\frac{d^2x}{dt^2} + b\\frac{dx}{dt} + kx = F_0 \\cos(\\omega t)\\)\nWhere \\(m\\) is mass, \\(b\\) is damping coefficient, \\(k\\) is spring constant, \\(F_0\\) is driving amplitude, and \\(\\omega\\) is driving frequency.\n\n\n\n\n\n\n\nFirst, construct a tridiagonal matrix for the second derivative operator \\(D_2\\) with elements \\(\\frac{1}{\\Delta t^2}[1, -2, 1]\\) along the diagonals.\nFor the first derivative (needed for damping), use a central difference approximation: \\(\\frac{d x}{dt} \\approx \\frac{x_{i+1} - x_{i-1}}{2\\Delta t}\\), which gives a matrix \\(D_1\\) with elements \\(\\frac{1}{2\\Delta t}[-1, 0, 1]\\) along the diagonals.\nThe full matrix equation is \\((D_2 + \\frac{b}{m}D_1 + \\frac{k}{m}I)x = \\frac{F_0}{m}\\cos(\\omega t)\\)\nTo incorporate initial conditions, modify the first two rows of your matrix and the corresponding entries in your right-hand side vector: - First row: \\(x_0 = x(0)\\) (initial position) - Second row: Approximation for initial velocity using forward difference\nAfter solving, compare your solution with the analytical solution for the steady-state response of a driven harmonic oscillator.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n“Numerical Recipes: The Art of Scientific Computing” by Press, Teukolsky, Vetterling, and Flannery - The standard reference for numerical methods in scientific computing\n“Computational Physics” by Mark Newman - Excellent introduction with Python examples\nSciPy documentation: scipy.integrate.solve_ivp\n\n\n\n\n\n“Computational Physics: Problem Solving with Python” by Landau, Páez, and Bordeianu - Connects computational methods with physics problems\n“An Introduction to Computer Simulation Methods” by Gould, Tobochnik, and Christian - Comprehensive coverage of simulation techniques in physics\n\n\n\n\n\n“Differential Equations, Dynamical Systems, and an Introduction to Chaos” by Hirsch, Smale, and Devaney\n“Nonlinear Dynamics And Chaos” by Steven Strogatz - Accessible introduction to nonlinear dynamics\n“Chaos: Making a New Science” by James Gleick - Popular science book on the history and significance of chaos theory\n\n\n\n\n\nScipy Lecture Notes - Tutorials on scientific computing with Python\nMatplotlib Gallery - Examples of scientific visualization",
    "crumbs": [
      "Python Basics",
      "Lecture 6",
      "Solving Differential Equations"
    ]
  },
  {
    "objectID": "lectures/lecture06/3-solving_ODEs.html#introduction",
    "href": "lectures/lecture06/3-solving_ODEs.html#introduction",
    "title": "Solving Ordinary Differential Equations",
    "section": "",
    "text": "In the previous lecture on numerical differentiation, we explored how to compute derivatives numerically using finite difference methods and matrix representations. We learned that:\n\nFinite difference approximations allow us to estimate derivatives at discrete points\nDifferentiation can be represented as matrix operations\nThe accuracy of these approximations depends on the order of the method and step size\n\nBuilding on this foundation, we can now tackle one of the most important applications in computational physics: solving ordinary differential equations (ODEs). Almost all dynamical systems in physics are described by differential equations, and learning how to solve them numerically is essential for modeling physical phenomena.\nAs second-semester physics students, you’ve likely encountered differential equations in various contexts:\n\nNewton’s second law: \\(F = ma = m\\frac{d^2x}{dt^2}\\)\nSimple harmonic motion: \\(\\frac{d^2x}{dt^2} + \\omega^2x = 0\\)\nRC circuits: \\(\\frac{dQ}{dt} + \\frac{1}{RC}Q = 0\\)\nHeat diffusion: \\(\\frac{\\partial T}{\\partial t} = \\alpha \\nabla^2 T\\)\n\nWhile analytical solutions exist for some simple cases, real-world physics problems often involve complex systems where analytical solutions are either impossible or impractical to obtain. This is where numerical methods become indispensable tools for the working physicist.",
    "crumbs": [
      "Python Basics",
      "Lecture 6",
      "Solving Differential Equations"
    ]
  },
  {
    "objectID": "lectures/lecture06/3-solving_ODEs.html#types-of-ode-solution-methods",
    "href": "lectures/lecture06/3-solving_ODEs.html#types-of-ode-solution-methods",
    "title": "Solving Ordinary Differential Equations",
    "section": "",
    "text": "There are two main approaches to solving ODEs numerically:\n\nImplicit methods: These methods solve for all time points simultaneously using matrix operations. They treat the problem as a large system of coupled algebraic equations. Just as you might solve a system of linear equations \\(Ax = b\\) in linear algebra, implicit methods set up and solve a larger matrix equation that represents the entire evolution of the system.\nExplicit methods: These methods march forward in time, computing the solution step by step. Starting from the initial conditions, they use the current state to calculate the next state, similar to how you might use the position and velocity at time \\(t\\) to predict the position and velocity at time \\(t + \\Delta t\\).\n\nWe’ll explore both approaches, highlighting their strengths and limitations. Each has its place in physics: implicit methods often handle “stiff” problems better (problems with vastly different timescales), while explicit methods are typically easier to implement and can handle nonlinear problems more naturally.",
    "crumbs": [
      "Python Basics",
      "Lecture 6",
      "Solving Differential Equations"
    ]
  },
  {
    "objectID": "lectures/lecture06/3-solving_ODEs.html#the-harmonic-oscillator-a-prototypical-ode",
    "href": "lectures/lecture06/3-solving_ODEs.html#the-harmonic-oscillator-a-prototypical-ode",
    "title": "Solving Ordinary Differential Equations",
    "section": "",
    "text": "Physics Interlude: The Harmonic Oscillator\n\n\n\nThe harmonic oscillator is one of the most fundamental systems in physics, appearing in mechanics, electromagnetism, quantum mechanics, and many other fields. It serves as an excellent test case for ODE solvers due to its simplicity and known analytical solution.\nThe harmonic oscillator describes any system that experiences a restoring force proportional to displacement. Physically, this occurs in:\n\nA mass on a spring (Hooke’s law: \\(F = -kx\\))\nA pendulum for small angles (where \\(\\sin\\theta \\approx \\theta\\))\nAn LC circuit with inductors and capacitors\nMolecular vibrations in chemistry\nPhonons in solid state physics\n\n\n\n\n\n\n\nFigure 1: A mass suspended from a spring demonstrates a classic harmonic oscillator. When displaced from equilibrium, the spring exerts a restoring force proportional to the displacement, leading to oscillatory motion.\n\n\n\nThe equation of motion for a classical harmonic oscillator is:\n\\[\\begin{equation}\n\\frac{d^2x}{dt^2} + \\omega^2 x = 0\n\\end{equation}\\]\nwhere \\(\\omega = \\sqrt{k/m}\\) is the angular frequency, with \\(k\\) being the spring constant and \\(m\\) the mass. In terms of Newton’s second law, this is: \\[\\begin{equation}\nm\\frac{d^2x}{dt^2} = -kx\n\\end{equation}\\]\nThis second-order differential equation requires two initial conditions: - Initial position: \\(x(t=0) = x_0\\) - Initial velocity: \\(\\dot{x}(t=0) = v_0\\)\nThe analytical solution is: \\[\\begin{equation}\nx(t) = x_0 \\cos(\\omega t) + \\frac{v_0}{\\omega} \\sin(\\omega t)\n\\end{equation}\\]\nThis represents sinusoidal oscillation with a constant amplitude and period \\(T = 2\\pi/\\omega\\). The total energy of the system (kinetic + potential) remains constant: \\[\\begin{equation}\nE = \\frac{1}{2}mv^2 + \\frac{1}{2}kx^2\n\\end{equation}\\]\nThis energy conservation will be an important test for our numerical methods.",
    "crumbs": [
      "Python Basics",
      "Lecture 6",
      "Solving Differential Equations"
    ]
  },
  {
    "objectID": "lectures/lecture06/3-solving_ODEs.html#implicit-matrix-solution",
    "href": "lectures/lecture06/3-solving_ODEs.html#implicit-matrix-solution",
    "title": "Solving Ordinary Differential Equations",
    "section": "",
    "text": "Using the matrix representation of the second derivative operator from our previous lecture, we can transform the ODE into a linear system that can be solved in one step. This approach treats the entire time evolution as a single mathematical problem.\n\n\nFrom calculus, we know that the second derivative represents the rate of change of the rate of change. Physically, this corresponds to acceleration in mechanics. In numerical terms, we need to approximate this using finite differences.\nRecall that we can represent the second derivative operator as a tridiagonal matrix:\n\\[D_2 = \\frac{1}{\\Delta t^2}\n\\begin{bmatrix}\n-2 & 1  & 0 & 0 & \\cdots & 0\\\\\n1 & -2 & 1 & 0 & \\cdots & 0\\\\\n0 & 1  & -2 & 1 & \\cdots & 0\\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n0 & \\cdots & 0 & 1 & -2 & 1\\\\\n0 & \\cdots & 0 & 0 & 1 & -2\\\\\n\\end{bmatrix}\\]\nThis matrix implements the central difference approximation for the second derivative: \\[\\begin{equation}\n\\frac{d^2x}{dt^2} \\approx \\frac{x_{i+1} - 2x_i + x_{i-1}}{\\Delta t^2}\n\\end{equation}\\]\nEach row in this matrix corresponds to the equation for a specific time point, linking it to its neighbors.\nThe harmonic oscillator term \\(\\omega^2 x\\) represents the restoring force. In a spring system, this is \\(F = -kx\\) divided by mass, giving \\(a = -\\frac{k}{m}x = -\\omega^2 x\\). This can be represented by a diagonal matrix:\n\\[V = \\omega^2 I = \\omega^2\n\\begin{bmatrix}\n1 & 0 & 0 & \\cdots & 0\\\\\n0 & 1 & 0 & \\cdots & 0\\\\\n0 & 0 & 1 & \\cdots & 0\\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n0 & 0 & 0 & \\cdots & 1\\\\\n\\end{bmatrix}\\]\nOur equation \\(\\frac{d^2x}{dt^2} + \\omega^2 x = 0\\) becomes the matrix equation \\((D_2 + V)x = 0\\), where \\(x\\) is the vector containing the positions at all time points \\((x_1, x_2, ..., x_n)\\). This is a large system of linear equations that determine the entire trajectory at once.\n\n\n\nTo solve this system, we need to incorporate the initial conditions by modifying the first rows of our matrix. This is a crucial step because a second-order differential equation needs two initial conditions to define a unique solution. In physical terms, we need to know both the initial position (displacement) and the initial velocity of our oscillator. We incorporate these conditions directly into our matrix equation:\n\\[M =\n\\begin{bmatrix}\n\\color{red}{1} & \\color{red}{0} & \\color{red}{0} & \\color{red}{0} & \\color{red}{\\cdots} & \\color{red}{0} \\\\\n\\color{blue}{-1} & \\color{blue}{1} & \\color{blue}{0} & \\color{blue}{0} & \\color{blue}{\\cdots} & \\color{blue}{0} \\\\\n\\frac{1}{\\Delta t^2} & -\\frac{2}{\\Delta t^2}+\\omega^2 & \\frac{1}{\\Delta t^2} & 0 & \\cdots & 0 \\\\\n0 & \\frac{1}{\\Delta t^2} & -\\frac{2}{\\Delta t^2}+\\omega^2 & \\frac{1}{\\Delta t^2} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & \\cdots & 0 & \\frac{1}{\\Delta t^2} & -\\frac{2}{\\Delta t^2}+\\omega^2 & \\frac{1}{\\Delta t^2} \\\\\n\\end{bmatrix}\\]\nThe first row (in red) enforces \\(x(0) = x_0\\) (initial position), and the second row (in blue) implements the initial velocity condition. The rest of the matrix represents the differential equation at each time step. By incorporating the initial conditions directly into the matrix, we ensure that our solution satisfies both the differential equation and the initial conditions.\n\n\n\n\n\n\n\n\n\n\n\n\nThis matrix-based approach has several advantages:\n\nIt solves for all time points simultaneously, giving the entire trajectory in one operation\nIt can be very stable for certain types of problems, particularly “stiff” equations\nIt handles boundary conditions naturally by incorporating them into the matrix structure\nIt often provides good energy conservation for oscillatory systems\n\nHowever, it also has limitations:\n\nIt requires solving a large linear system, which becomes computationally intensive for long simulations\nIt’s not suitable for nonlinear problems without modification (e.g., a pendulum with large angles where \\(\\sin\\theta \\neq \\theta\\))\nMemory requirements grow with the time span (an N×N matrix for N time steps)\nImplementing time-dependent forces is more complex than with explicit methods\n\nFrom a physics perspective, this approach is similar to finding stationary states in quantum mechanics or solving boundary value problems in electrostatics—we’re solving the entire system at once rather than stepping through time.",
    "crumbs": [
      "Python Basics",
      "Lecture 6",
      "Solving Differential Equations"
    ]
  },
  {
    "objectID": "lectures/lecture06/3-solving_ODEs.html#explicit-numerical-integration-methods",
    "href": "lectures/lecture06/3-solving_ODEs.html#explicit-numerical-integration-methods",
    "title": "Solving Ordinary Differential Equations",
    "section": "",
    "text": "An alternative approach is to use explicit step-by-step integration methods. These methods convert the second-order ODE to a system of first-order ODEs and then advance the solution incrementally, similar to how we intuitively understand physical motion: position changes based on velocity, and velocity changes based on acceleration.\n\n\nTo convert our second-order ODE to a first-order system, we introduce a new variable \\(v = dx/dt\\) (the velocity):\n\\[\\begin{align}\n\\frac{dx}{dt} &= v \\\\\n\\frac{dv}{dt} &= -\\omega^2 x\n\\end{align}\\]\nThis transformation is common in physics. For example, in classical mechanics, we often convert Newton’s second law (a second-order ODE) into phase space equations involving position and momentum. In the case of the harmonic oscillator:\n\nThe first equation simply states that the rate of change of position is the velocity\nThe second equation comes from \\(F = ma\\) where \\(F = -kx\\), giving \\(a = \\frac{F}{m} = -\\frac{k}{m}x = -\\omega^2 x\\)\n\nWe can now represent the state of our system as a vector\n\\[\\vec{y} = \\begin{bmatrix} x \\\\ v \\end{bmatrix}\\]\nand the derivative as\n\\[\\dot{\\vec{y}} = \\begin{bmatrix} v \\\\ -\\omega^2 x \\end{bmatrix}\\]\nThis representation is called the “phase space” description, and is fundamental in classical mechanics and dynamical systems theory.\n\n\n\n\n\n\nRecipe to solve a differential equation\n\n\n\n\nState your differential equation as a system of ordinary differential equations \\[\\dot{\\vec{y}} = \\begin{bmatrix} v \\\\ -\\omega^2 x \\end{bmatrix}\\]\nWrite a function that represents you physical system\ndef SHO(time, state, omega=2):\n    g0 = state[1]               -&gt;velocity\n    g1 = -omega * state [0]       -&gt;acceleration\n    return np.array([g0, g1])\nUse a function to integrate the system of ODEs\nt_euler, y_euler = euler_method(lambda t, y: SHO(t, y, omega),\n    y0,\n    t_span,\n    dt\n)\nor\nsolution = solve_ivp( lambda t, y: SHO(t, y, omega),\n    t_span,\n    y0,\n    method='BDF',\n    t_eval=t_eval\n)\n\n\n\n\nEuler MethodEuler-Cromer MethodMidpoint Method\n\n\nThe simplest explicit integration method is the Euler method, derived from the first-order Taylor expansion:\n\\[\\begin{equation}\n\\vec{y}(t + \\Delta t) \\approx \\vec{y}(t) + \\dot{\\vec{y}}(t) \\Delta t\n\\end{equation}\\]\nThis is essentially a linear approximation—assuming the derivative stays constant over the small time step. Physically, it’s like assuming constant velocity when updating position, and constant acceleration when updating velocity over each small time interval.\nFor the harmonic oscillator, this becomes:\n\\[\\begin{align}\nx_{n+1} &= x_n + v_n \\Delta t \\\\\nv_{n+1} &= v_n - \\omega^2 x_n \\Delta t\n\\end{align}\\]\nThe physical interpretation is straightforward:\n\nThe new position equals the old position plus the displacement (velocity × time)\nThe new velocity equals the old velocity plus the acceleration (-ω²x) multiplied by the time step\n\nIn practice, the Euler method will cause the total energy of a harmonic oscillator to increase over time, which is physically incorrect. This is because the method doesn’t account for the continuous change in acceleration during the time step.\n\n\n\n\n\n\n\n\nThe Euler method often performs poorly for oscillatory systems, as it tends to artificially increase the energy over time. A simple but effective improvement is the Euler-Cromer method (also known as the symplectic Euler method), which uses the updated velocity for the position update:\n\\[\\begin{align}\nv_{n+1} &= v_n - \\omega^2 x_n \\Delta t \\\\\nx_{n+1} &= x_n + v_{n+1} \\Delta t\n\\end{align}\\]\nNotice the subtle but crucial difference: we use \\(v_{n+1}\\) (the newly calculated velocity) to update the position, rather than \\(v_n\\).\nThis small change dramatically improves energy conservation for oscillatory systems. From a physics perspective, this method better preserves the structure of Hamiltonian systems, which include the harmonic oscillator, planetary motion, and many other important physical systems.\nThe Euler-Cromer method is especially valuable in physics simulations where long-term energy conservation is important, such as:\n\nOrbital mechanics\nMolecular dynamics\nPlasma physics\nN-body simulations\n\nWhile not perfect, this simple modification makes the method much more useful for real physical systems with oscillatory behavior.\n\n\n\n\n\n\n\n\nFor higher accuracy, we can use the midpoint method (also known as the second-order Runge-Kutta method or RK2). This evaluates the derivative at the midpoint of the interval, providing a better approximation of the average derivative over the time step.\n\\[\\begin{align}\nk_1 &= f(t_n, y_n) \\\\\nk_2 &= f(t_n + \\frac{\\Delta t}{2}, y_n + \\frac{\\Delta t}{2}k_1) \\\\\ny_{n+1} &= y_n + \\Delta t \\cdot k_2\n\\end{align}\\]\nThe physical intuition behind this method is:\n\nFirst, calculate the initial derivative \\(k_1\\) (representing the initial rates of change)\nUse this derivative to estimate the state at the middle of the time step\nCalculate a new derivative \\(k_2\\) at this midpoint\nUse the midpoint derivative to advance the full step\n\nThis is analogous to finding the average velocity by looking at the velocity in the middle of a time interval, rather than just at the beginning. In physical problems with continuously varying forces, this provides a much better approximation than the Euler method.\nThe midpoint method achieves \\(O(\\Delta t^2)\\) accuracy, meaning the error decreases with the square of the step size. This makes it much more accurate than Euler’s method for the same computational cost.\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s compare these methods for the harmonic oscillator:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEuler Method: Simple but least accurate. It systematically increases the total energy of the system, causing the amplitude to grow over time.\nEuler-Cromer Method: Much better energy conservation for oscillatory systems with minimal additional computation.\nMidpoint Method: Higher accuracy and better energy conservation.\n\nThe choice of method depends on the specific requirements of your problem:\n\nUse Euler for simplicity when accuracy is not critical\nUse Euler-Cromer for oscillatory systems when computational efficiency is important\nUse Midpoint or higher-order methods when accuracy is crucial",
    "crumbs": [
      "Python Basics",
      "Lecture 6",
      "Solving Differential Equations"
    ]
  },
  {
    "objectID": "lectures/lecture06/3-solving_ODEs.html#advanced-methods-with-scipy",
    "href": "lectures/lecture06/3-solving_ODEs.html#advanced-methods-with-scipy",
    "title": "Solving Ordinary Differential Equations",
    "section": "",
    "text": "For practical applications, SciPy provides sophisticated ODE solvers with adaptive step size control, error estimation, and specialized algorithms for different types of problems. These methods represent the state-of-the-art in numerical integration and are what working physicists typically use for research and advanced applications.\n\n\n\nAdaptive Step Size: Unlike our fixed-step methods, these solvers can automatically adjust the step size based on the local error estimate, taking smaller steps where the solution changes rapidly and larger steps where it’s smooth.\nHigher-Order Methods: These solvers use higher-order approximations (4th, 5th, or even 8th order), achieving much higher accuracy with the same computational effort.\nError Control: They can maintain the error below a specified tolerance, giving you confidence in the accuracy of your results.\nSpecialized Methods: Different methods are optimized for different types of problems (stiff vs. non-stiff, conservative vs. dissipative).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdvanced Example: Damped Driven Pendulum\n\n\n\n\n\nLet’s apply these methods to a more complex system: a damped driven pendulum. This system can exhibit chaotic behavior under certain conditions, making it a fascinating study in nonlinear dynamics.\n\n\n\n\n\n\nFigure 2: The damped driven pendulum. External driving forces and damping (friction) create a system that can exhibit complex behaviors from regular oscillations to chaos.\n\n\n\n\n\nThe damped driven pendulum represents many real physical systems\n\nA physical pendulum with friction and external driving force\nAn RLC circuit with nonlinear components\nJosephson junctions in superconductivity\nCertain types of mechanical and electrical oscillators\n\nThe equation of motion is:\n\\[\\begin{equation}\n\\frac{d^2\\theta}{dt^2} + b\\frac{d\\theta}{dt} + \\omega_0^2\\sin\\theta = F_0\\cos(\\omega_d t)\n\\end{equation}\\]\nwhere\n\n\\(\\theta\\) is the angle from vertical (radians)\n\\(b\\) is the damping coefficient (represents friction or resistance)\n\\(\\omega_0 = \\sqrt{g/L}\\) is the natural frequency (where \\(g\\) is gravity and \\(L\\) is pendulum length)\n\\(F_0\\) is the driving amplitude (strength of the external force)\n\\(\\omega_d\\) is the driving frequency (how rapidly the external force oscillates)\n\nThis equation differs from the harmonic oscillator in three crucial ways:\n\nThe \\(\\sin\\theta\\) term (rather than just \\(\\theta\\)) makes it nonlinear\nThe damping term \\(b\\frac{d\\theta}{dt}\\) causes energy dissipation\nThe driving term \\(F_0\\cos(\\omega_d t)\\) adds energy to the system\n\nThese additions make the system much more realistic and rich in behavior. Students could explore this system by varying parameters like the damping coefficient (b), driving amplitude (F0), and driving frequency (omega_d) to observe how the pendulum transitions between regular oscillations, period doubling, and chaos.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChaotic motion refers to a deterministic system whose behavior appears random and exhibits extreme sensitivity to initial conditions—the famous “butterfly effect.” In the damped driven pendulum, chaos emerges when the system’s nonlinear restoring force (\\(\\sin\\theta\\) term) interacts with sufficient driving force. For this system, chaotic behavior typically appears when the driving amplitude \\(F_0\\) exceeds a critical value (approximately 1.0 in our case) while maintaining moderate damping (\\(b \\approx 0.1\\)) and a driving frequency \\(\\omega_d\\) that is not too far from the natural frequency \\(\\omega_0\\). In the chaotic regime, the pendulum’s trajectory becomes unpredictable over long time scales, despite being governed by deterministic equations. The phase space transforms from closed orbits (regular motion) to a strange attractor with fractal structure, and the system never settles into a periodic oscillation.\n\n\n\n\n\n\nLyapunov Exponents: Quantifying Chaos\n\n\n\n\n\nThe Lyapunov exponent is a powerful mathematical tool for characterizing chaotic systems. It measures the rate at which nearby trajectories in phase space diverge over time. For a system with state vector \\(\\vec{x}\\), if two initial conditions differ by a small displacement \\(\\delta\\vec{x}_0\\), then after time \\(t\\), this separation grows approximately as:\n\\[|\\delta\\vec{x}(t)| \\approx e^{\\lambda t}|\\delta\\vec{x}_0|\\]\nwhere \\(\\lambda\\) is the Lyapunov exponent.\n\nA positive Lyapunov exponent (\\(\\lambda &gt; 0\\)) indicates chaos: nearby trajectories diverge exponentially, making long-term prediction impossible. The larger the exponent, the more chaotic the system.\nA zero Lyapunov exponent (\\(\\lambda = 0\\)) suggests a stable limit cycle or quasiperiodic behavior.\nA negative Lyapunov exponent (\\(\\lambda &lt; 0\\)) indicates a stable fixed point, where trajectories converge.\n\nFor our damped driven pendulum, we can numerically estimate the Lyapunov exponent by comparing how two slightly different initial conditions evolve over time. For instance, we might begin with two pendulums at nearly the same angle—perhaps \\(\\theta_1(0) = 0.1\\) and \\(\\theta_2(0) = 0.1001\\)—while keeping their initial angular velocities identical at \\(\\omega_1(0) = \\omega_2(0) = 0\\). As we simulate both systems, we can track the separation between their trajectories in phase space. This separation represents the difference vector \\(\\delta\\vec{x}(t) = [\\theta_2(t) - \\theta_1(t), \\omega_2(t) - \\omega_1(t)]\\). Initially, this difference is very small, but in a chaotic system, it grows exponentially with time. By measuring this growth rate, we can compute the Lyapunov exponent as \\(\\lambda \\approx \\frac{1}{t}\\ln\\left(\\frac{|\\delta\\vec{x}(t)|}{|\\delta\\vec{x}(0)|}\\right)\\). When examining our pendulum system, which has two dimensions (\\(\\theta\\) and \\(\\omega\\)), we actually have two Lyapunov exponents forming a spectrum. If either of these exponents is positive, we can conclusively determine that our system exhibits chaotic behavior, indicating fundamental unpredictability despite its deterministic nature.",
    "crumbs": [
      "Python Basics",
      "Lecture 6",
      "Solving Differential Equations"
    ]
  },
  {
    "objectID": "lectures/lecture06/3-solving_ODEs.html#conclusion",
    "href": "lectures/lecture06/3-solving_ODEs.html#conclusion",
    "title": "Solving Ordinary Differential Equations",
    "section": "",
    "text": "In this lecture, we’ve built upon our knowledge of numerical differentiation to solve ordinary differential equations. We’ve explored:\n\nImplicit Matrix Methods: Using matrices to solve the entire system at once\nExplicit Integration Methods: Step-by-step methods like Euler, Euler-Cromer, and Midpoint\nAdvanced SciPy Methods: Leveraging powerful adaptive solvers for complex problems\n\nEach approach has its strengths and is suited to different types of problems. The matrix method is excellent for linear systems, while explicit methods are more versatile for nonlinear problems. SciPy’s solvers combine accuracy, stability, and efficiency for practical applications.\n\n\nThe methods we’ve learned are essential tools for computational physics because they: 1. Allow us to study systems with no analytical solutions 2. Provide insights into nonlinear dynamics and chaos 3. Enable the modeling of realistic systems with friction, driving forces, and complex interactions 4. Connect mathematical formulations with observable physical phenomena\nAs a second-semester physics student, these numerical tools complement your analytical understanding of mechanics, electromagnetism, and other core physics subjects. When analytical methods reach their limits, these numerical approaches allow you to continue exploring and modeling physical reality.",
    "crumbs": [
      "Python Basics",
      "Lecture 6",
      "Solving Differential Equations"
    ]
  },
  {
    "objectID": "lectures/lecture06/3-solving_ODEs.html#exercises",
    "href": "lectures/lecture06/3-solving_ODEs.html#exercises",
    "title": "Solving Ordinary Differential Equations",
    "section": "",
    "text": "Here are two simple exercises with solutions where you can train your programming skills. In these examples you should use the odeint method from SciPy’s integrate module.\n\n\n\n\n\n\nSelf-Exercise 1: Simple Harmonic Oscillator\n\n\n\nWrite a program to solve the equation of motion for a simple harmonic oscillator. This example demonstrates how to solve a second-order differential equation using scipy’s odeint.\nThe equation of motion is: \\(\\frac{d^2x}{dt^2} + \\omega^2x = 0\\)\nThis represents an idealized spring-mass system or pendulum with small oscillations.\n\n\n\n\n\n\n\nUse odeint(oscillator, initial_state, t, args=(omega,)) to solve the system. The solution will have two columns: position ([:,0]) and velocity ([:,1]). Create a plot showing position vs time using matplotlib. Remember to label your axes and add a title.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSelf-Exercise 2: Damped Driven Harmonic Oscillator using Matrix Method\n\n\n\nModel a damped harmonic oscillator with an external driving force using the implicit matrix method. This exercise demonstrates how to solve a second-order differential equation by constructing and solving a matrix system that represents the entire time evolution.\nThe equation of motion is: \\(m\\frac{d^2x}{dt^2} + b\\frac{dx}{dt} + kx = F_0 \\cos(\\omega t)\\)\nWhere \\(m\\) is mass, \\(b\\) is damping coefficient, \\(k\\) is spring constant, \\(F_0\\) is driving amplitude, and \\(\\omega\\) is driving frequency.\n\n\n\n\n\n\n\nFirst, construct a tridiagonal matrix for the second derivative operator \\(D_2\\) with elements \\(\\frac{1}{\\Delta t^2}[1, -2, 1]\\) along the diagonals.\nFor the first derivative (needed for damping), use a central difference approximation: \\(\\frac{d x}{dt} \\approx \\frac{x_{i+1} - x_{i-1}}{2\\Delta t}\\), which gives a matrix \\(D_1\\) with elements \\(\\frac{1}{2\\Delta t}[-1, 0, 1]\\) along the diagonals.\nThe full matrix equation is \\((D_2 + \\frac{b}{m}D_1 + \\frac{k}{m}I)x = \\frac{F_0}{m}\\cos(\\omega t)\\)\nTo incorporate initial conditions, modify the first two rows of your matrix and the corresponding entries in your right-hand side vector: - First row: \\(x_0 = x(0)\\) (initial position) - Second row: Approximation for initial velocity using forward difference\nAfter solving, compare your solution with the analytical solution for the steady-state response of a driven harmonic oscillator.\n\n\n\n\n\n\n\n\nSolution",
    "crumbs": [
      "Python Basics",
      "Lecture 6",
      "Solving Differential Equations"
    ]
  },
  {
    "objectID": "lectures/lecture06/3-solving_ODEs.html#further-reading",
    "href": "lectures/lecture06/3-solving_ODEs.html#further-reading",
    "title": "Solving Ordinary Differential Equations",
    "section": "",
    "text": "“Numerical Recipes: The Art of Scientific Computing” by Press, Teukolsky, Vetterling, and Flannery - The standard reference for numerical methods in scientific computing\n“Computational Physics” by Mark Newman - Excellent introduction with Python examples\nSciPy documentation: scipy.integrate.solve_ivp\n\n\n\n\n\n“Computational Physics: Problem Solving with Python” by Landau, Páez, and Bordeianu - Connects computational methods with physics problems\n“An Introduction to Computer Simulation Methods” by Gould, Tobochnik, and Christian - Comprehensive coverage of simulation techniques in physics\n\n\n\n\n\n“Differential Equations, Dynamical Systems, and an Introduction to Chaos” by Hirsch, Smale, and Devaney\n“Nonlinear Dynamics And Chaos” by Steven Strogatz - Accessible introduction to nonlinear dynamics\n“Chaos: Making a New Science” by James Gleick - Popular science book on the history and significance of chaos theory\n\n\n\n\n\nScipy Lecture Notes - Tutorials on scientific computing with Python\nMatplotlib Gallery - Examples of scientific visualization",
    "crumbs": [
      "Python Basics",
      "Lecture 6",
      "Solving Differential Equations"
    ]
  },
  {
    "objectID": "lectures/lecture01/00-lecture01.html",
    "href": "lectures/lecture01/00-lecture01.html",
    "title": "Programming Background Questionnaire",
    "section": "",
    "text": "Please complete this short questionnaire to help tailor the course to your needs. Your responses are anonymous and will be used only to adapt the teaching to your level of experience.\n\n  Loading…",
    "crumbs": [
      "Python Basics",
      "Lecture 1",
      "Initial Questions"
    ]
  },
  {
    "objectID": "lectures/lecture01/02-summary01.html",
    "href": "lectures/lecture01/02-summary01.html",
    "title": "Computer-Based Physical Modelling",
    "section": "",
    "text": "Click to expand Python Basics Cheat Sheet\n\n\n\n\n\n\n\n\n\nx = 1.0  # Assigns 1.0 to variable x\nmy_variable = \"Hello\"  # Assigns string \"Hello\"\n\n\n\nUse a-z, A-Z, 0-9, and _\nStart with letter or underscore\nCase-sensitive\nAvoid reserved keywords\n\n\n\n\n\n\n\n\nType\nExample\nDescription\n\n\n\n\nint\n5\nWhole numbers\n\n\nfloat\n3.14\nDecimal numbers\n\n\ncomplex\n2 + 3j\nReal + imaginary\n\n\nbool\nTrue\nLogical values\n\n\n\n\n\n\nint_num = int(3.14)    # 3\nfloat_num = float(5)   # 5.0\nstr_num = str(42)      # \"42\"\n\n\n\n\na, b = 10, 3\nsum_result = a + b   # Addition\ndiff_result = a - b  # Subtraction\nprod_result = a * b  # Multiplication\ndiv_result = a / b   # Division (float)\nint_div_result = a // b  # Integer division\nmod_result = a % b   # Modulus\npower_result = a ** b  # Exponentiation\n\n\n\nc = 2 + 4j\nreal_part = c.real     # 2.0\nimag_part = c.imag     # 4.0\nconjugate = c.conjugate()  # 2 - 4j\n\n\n\ntype(variable)  # Returns type\nisinstance(variable, type)  # Checks type\n\n\n\nimport math\n\nsqrt_result = math.sqrt(16)\nlog_result = math.log(100, 10)\nsin_result = math.sin(math.pi/2)"
  },
  {
    "objectID": "lectures/lecture01/02-summary01.html#python-basics-cheat-sheet",
    "href": "lectures/lecture01/02-summary01.html#python-basics-cheat-sheet",
    "title": "Computer-Based Physical Modelling",
    "section": "",
    "text": "Click to expand Python Basics Cheat Sheet\n\n\n\n\n\n\n\n\n\nx = 1.0  # Assigns 1.0 to variable x\nmy_variable = \"Hello\"  # Assigns string \"Hello\"\n\n\n\nUse a-z, A-Z, 0-9, and _\nStart with letter or underscore\nCase-sensitive\nAvoid reserved keywords\n\n\n\n\n\n\n\n\nType\nExample\nDescription\n\n\n\n\nint\n5\nWhole numbers\n\n\nfloat\n3.14\nDecimal numbers\n\n\ncomplex\n2 + 3j\nReal + imaginary\n\n\nbool\nTrue\nLogical values\n\n\n\n\n\n\nint_num = int(3.14)    # 3\nfloat_num = float(5)   # 5.0\nstr_num = str(42)      # \"42\"\n\n\n\n\na, b = 10, 3\nsum_result = a + b   # Addition\ndiff_result = a - b  # Subtraction\nprod_result = a * b  # Multiplication\ndiv_result = a / b   # Division (float)\nint_div_result = a // b  # Integer division\nmod_result = a % b   # Modulus\npower_result = a ** b  # Exponentiation\n\n\n\nc = 2 + 4j\nreal_part = c.real     # 2.0\nimag_part = c.imag     # 4.0\nconjugate = c.conjugate()  # 2 - 4j\n\n\n\ntype(variable)  # Returns type\nisinstance(variable, type)  # Checks type\n\n\n\nimport math\n\nsqrt_result = math.sqrt(16)\nlog_result = math.log(100, 10)\nsin_result = math.sin(math.pi/2)"
  },
  {
    "objectID": "lectures/lecture01/02-lecture01_reveal.html#variables-in-python",
    "href": "lectures/lecture01/02-lecture01_reveal.html#variables-in-python",
    "title": "Variables & Numbers",
    "section": "Variables in Python",
    "text": "Variables in Python\nSymbol Names\nVariable names in Python can include alphanumerical characters a-z, A-Z, 0-9, and the special character _. Normal variable names must start with a letter or an underscore. By convention, variable names typically start with a lower-case letter, while Class names start with a capital letter and internal variables start with an underscore.\n\n\n\n\n\n\n\nReserved Keywords\n\n\nPython has keywords that cannot be used as variable names. The most common ones you’ll encounter in physics programming are:\nif, else, for, while, return, and, or, lambda\nNote that lambda is particularly relevant as it could naturally appear in physics code, but since it’s reserved for anonymous functions in Python, it cannot be used as a variable name.\n\n\n\n\nVariable Assignment\nThe assignment operator in Python is =. Python is a dynamically typed language, so we do not need to specify the type of a variable when we create one.\nAssigning a value to a new variable creates the variable:\n#| autorun: false\n# variable assignments\nx = 1.0\nmy_favorite_variable = 12.2\nx\nAlthough not explicitly specified, a variable does have a type associated with it (e.g., integer, float, string). The type is derived from the value that was assigned to it. To determine the type of a variable, we can use the type function.\n#| autorun: false\ntype(x)\nIf we assign a new value to a variable, its type can change.\n#| autorun: false\nx = 1\n#| autorun: false\ntype(x)\nIf we try to use a variable that has not yet been defined, we get a NameError error.\n#| autorun: false\n#print(g)"
  },
  {
    "objectID": "lectures/lecture01/02-lecture01_reveal.html#number-types",
    "href": "lectures/lecture01/02-lecture01_reveal.html#number-types",
    "title": "Variables & Numbers",
    "section": "Number Types",
    "text": "Number Types\nPython supports various number types, including integers, floating-point numbers, and complex numbers. These are some of the basic building blocks of doing arithmetic in any programming language. We will discuss each of these types in more detail.\nComparison of Number Types\n\n\n\n\n\n\n\n\n\n\nType\nExample\nDescription\nLimits\nUse Cases\n\n\n\n\nint\n42\nWhole numbers\nUnlimited precision (bounded by available memory)\nCounting, indexing\n\n\nfloat\n3.14159\nDecimal numbers\nTypically ±1.8e308 with 15-17 digits of precision (64-bit)\nScientific calculations, prices\n\n\ncomplex\n2 + 3j\nNumbers with real and imaginary parts\nSame as float for both real and imaginary parts\nSignal processing, electrical engineering\n\n\nbool\nTrue / False\nLogical values\nOnly two values: True (1) and False (0)\nConditional operations, flags\n\n\n\n\n\n\n\n\n\n\nExamples for Number Types\n\n\nIntegers\nInteger Representation: Integers are whole numbers without a decimal point.\n#| autorun: false\nx = 1\ntype(x)\nBinary, Octal, and Hexadecimal: Integers can be represented in different bases:\n#| autorun: false\n0b1010111110  # Binary\n0x0F          # Hexadecimal\nFloating Point Numbers\nFloating Point Representation: Numbers with a decimal point are treated as floating-point values.\n#| autorun: false\nx = 3.141\ntype(x)\nMaximum Float Value: Python handles large floats, converting them to infinity if they exceed the maximum representable value.\n#| autorun: false\n1.7976931348623157e+308 * 2  # Output: inf\nComplex Numbers\nComplex Number Representation: Complex numbers have a real and an imaginary part.\n#| autorun: false\nc = 2 + 4j\ntype(c)\n\nAccessors for Complex Numbers:\n\nc.real: Real part of the complex number.\nc.imag: Imaginary part of the complex number.\n\n\n#| autorun: false\nprint(c.real)\nprint(c.imag)\nComplex Conjugate: Use the .conjugate() method to get the complex conjugate.\n#| autorun: false\nc = c.conjugate()\nprint(c)"
  },
  {
    "objectID": "lectures/lecture01/02-lecture01_reveal.html#operators",
    "href": "lectures/lecture01/02-lecture01_reveal.html#operators",
    "title": "Variables & Numbers",
    "section": "Operators",
    "text": "Operators\nPython provides a variety of operators for performing operations on variables and values. Here we’ll cover the most common operators used in scientific programming.\nArithmetic Operators\nThese operators perform basic mathematical operations:\n\n\n\nOperator\nName\nExample\nResult\n\n\n\n\n+\nAddition\n5 + 3\n8\n\n\n-\nSubtraction\n5 - 3\n2\n\n\n*\nMultiplication\n5 * 3\n15\n\n\n/\nDivision\n5 / 3\n1.6666…\n\n\n//\nFloor Division\n5 // 3\n1\n\n\n%\nModulus (remainder)\n5 % 3\n2\n\n\n**\nExponentiation\n5 ** 3\n125\n\n\n\n#| autorun: false\n# Examples of arithmetic operators\nprint(f\"Addition: 5 + 3 = {5 + 3}\")\nprint(f\"Division: 5 / 3 = {5 / 3}\")\nprint(f\"Floor Division: 5 // 3 = {5 // 3}\")\nprint(f\"Exponentiation: 5 ** 3 = {5 ** 3}\")\nComparison Operators\nThese operators are used to compare values:\n\n\n\nOperator\nDescription\nExample\n\n\n\n\n==\nEqual to\nx == y\n\n\n!=\nNot equal to\nx != y\n\n\n&gt;\nGreater than\nx &gt; y\n\n\n&lt;\nLess than\nx &lt; y\n\n\n&gt;=\nGreater than or equal to\nx &gt;= y\n\n\n&lt;=\nLess than or equal to\nx &lt;= y\n\n\n\n#| autorun: false\n# Examples of comparison operators\nx, y = 5, 3\nprint(f\"x = {x}, y = {y}\")\nprint(f\"x == y: {x == y}\")\nprint(f\"x &gt; y: {x &gt; y}\")\nprint(f\"x &lt;= y: {x &lt;= y}\")\nLogical Operators\nUsed to combine conditional statements:\n\n\n\n\n\n\n\n\nOperator\nDescription\nExample\n\n\n\n\nand\nReturns True if both statements are true\nx &gt; 0 and x &lt; 10\n\n\nor\nReturns True if one of the statements is true\nx &lt; 0 or x &gt; 10\n\n\nnot\nReverses the result, returns False if the result is true\nnot(x &gt; 0 and x &lt; 10)\n\n\n\n#| autorun: false\n# Examples of logical operators\nx = 7\nprint(f\"x = {x}\")\nprint(f\"x &gt; 0 and x &lt; 10: {x &gt; 0 and x &lt; 10}\")\nprint(f\"x &lt; 0 or x &gt; 10: {x &lt; 0 or x &gt; 10}\")\nprint(f\"not(x &gt; 0): {not(x &gt; 0)}\")\nAssignment Operators\nPython provides shorthand operators for updating variables:\n\n\n\nOperator\nExample\nEquivalent to\n\n\n\n\n=\nx = 5\nx = 5\n\n\n+=\nx += 3\nx = x + 3\n\n\n-=\nx -= 3\nx = x - 3\n\n\n*=\nx *= 3\nx = x * 3\n\n\n/=\nx /= 3\nx = x / 3\n\n\n//=\nx //= 3\nx = x // 3\n\n\n%=\nx %= 3\nx = x % 3\n\n\n**=\nx **= 3\nx = x ** 3\n\n\n\n#| autorun: false\n# Examples of assignment operators\nx = 10\nprint(f\"Initial x: {x}\")\n\nx += 5\nprint(f\"After x += 5: {x}\")\n\nx *= 2\nprint(f\"After x *= 2: {x}\")\n\n\n\n\n\n\n\nOperator Precedence\n\n\nPython follows the standard mathematical order of operations (PEMDAS):\n\nParentheses\nExponentiation (**)\nMultiplication and Division (*, /, //, %)\nAddition and Subtraction (+, -)\n\nWhen operators have the same precedence, they are evaluated from left to right.\n#| autorun: false\n# Operator precedence example\nresult = 2 + 3 * 4 ** 2\nprint(f\"2 + 3 * 4 ** 2 = {result}\")  # 2 + 3 * 16 = 2 + 48 = 50\n\n# Using parentheses to change precedence\nresult = (2 + 3) * 4 ** 2\nprint(f\"(2 + 3) * 4 ** 2 = {result}\")  # 5 * 16 = 80"
  }
]